"""
PEFT + DeepSpeed í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë“¤
"""

import os
import json
import torch
import pickle
import logging
from typing import Dict, Any, Tuple, List, Optional
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model, TaskType
import deepspeed
import datasets
from datasets import Dataset as HuggingFaceDataset
import time
from omegaconf import OmegaConf, DictConfig
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from pathlib import Path
import mlflow


# ==================== ë©”íŠ¸ë¦­ ê´€ë¦¬ í´ë˜ìŠ¤ ====================

class MetricTracker:
    """ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹… í†µí•©"""
    def __init__(self, local_rank=0):
        self.local_rank = local_rank
        self.metrics_history = []
    
    def calculate_and_log(self, predictions, labels, prefix="", step=None):
        """í•œ ë²ˆì— ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…"""
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        if torch.is_tensor(predictions):
            predictions = predictions.cpu().numpy()
        if torch.is_tensor(labels):
            labels = labels.cpu().numpy()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            f"{prefix}accuracy": accuracy_score(labels, predictions),
            f"{prefix}f1_weighted": f1_score(labels, predictions, average='weighted', zero_division=0),
            f"{prefix}f1_macro": f1_score(labels, predictions, average='macro', zero_division=0),
            f"{prefix}precision": precision_score(labels, predictions, average='weighted', zero_division=0),
            f"{prefix}recall": recall_score(labels, predictions, average='weighted', zero_division=0)
        }
        
        # MLflow ë¡œê¹… (rank 0ì—ì„œë§Œ)
        if self.local_rank == 0 and mlflow.active_run():
            try:
                mlflow.log_metrics(metrics, step=step)
            except Exception as e:
                print(f"âš ï¸ MLflow ë¡œê¹… ì‹¤íŒ¨: {e}")
        
        # ê¸°ë¡ ì €ì¥
        self.metrics_history.append(metrics)
        
        return metrics
    
    def get_summary(self):
        """ì „ì²´ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ìš”ì•½"""
        if not self.metrics_history:
            return {}
        
        summary = {}
        for key in self.metrics_history[0].keys():
            values = [m[key] for m in self.metrics_history]
            summary[f"{key}_mean"] = np.mean(values)
            summary[f"{key}_std"] = np.std(values)
            summary[f"{key}_max"] = np.max(values)
        
        return summary


# ==================== GPU ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤ ====================

class GPUMonitor:
    """GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í†µí•©"""
    def __init__(self, local_rank=0):
        self.local_rank = local_rank
        self.peak_memory = 0
    
    def get_memory_stats(self):
        """í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            max_allocated = torch.cuda.max_memory_allocated() / 1024**3
            
            # í”¼í¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
            self.peak_memory = max(self.peak_memory, allocated)
            
            return {
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'max_allocated_gb': max_allocated,
                'peak_memory_gb': self.peak_memory
            }
        return {}
    
    def log_memory(self, prefix=""):
        """ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê·¸"""
        stats = self.get_memory_stats()
        if self.local_rank == 0 and stats:
            print(f"[GPU Memory {prefix}] "
                  f"Allocated: {stats['allocated_gb']:.2f}GB, "
                  f"Reserved: {stats['reserved_gb']:.2f}GB, "
                  f"Peak: {stats['peak_memory_gb']:.2f}GB")
        return stats
    
    def reset_peak_memory(self):
        """í”¼í¬ ë©”ëª¨ë¦¬ í†µê³„ ë¦¬ì…‹"""
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
        self.peak_memory = 0


# ==================== ë¡œê¹… í´ë˜ìŠ¤ ====================

class RankAwareLogger:
    """Rank-aware ë¡œê¹…ì„ ìœ„í•œ ë˜í¼"""
    def __init__(self, rank=0):
        self.rank = rank
    
    def info(self, message):
        if self.rank == 0:
            print(f"[INFO] {message}")
    
    def warning(self, message):
        if self.rank == 0:
            print(f"[WARNING] {message}")
    
    def error(self, message):
        if self.rank == 0:
            print(f"[ERROR] {message}")
    
    def debug(self, message):
        if self.rank == 0:
            print(f"[DEBUG] {message}")


# ==================== ê²½ë¡œ ê´€ë¦¬ í´ë˜ìŠ¤ ====================

class PathManager:
    """ëª¨ë“  ê²½ë¡œë¥¼ ì¤‘ì•™ì—ì„œ ê´€ë¦¬"""
    
    def __init__(self, config, local_rank=0):
        self.config = config
        self.local_rank = local_rank
        self.base_dir = Path.cwd()
        
        # ëª¨ë“  ê²½ë¡œë¥¼ ì´ˆê¸°í™” ì‹œì ì— ì •ì˜
        self.paths = {
            'data': self._resolve(config.data.dataset_all_path),
            'cache': self._resolve(config.cache_dir),
            'arrow_cache': self._resolve(config.cache_dir) / "hf_arrow",
            'model_output': self._resolve(config.model.save_path),
            'checkpoints': self._resolve(config.model.save_path) / "checkpoints",
            'best_model': self._resolve(config.model.save_path) / "best_model",
            'tensorboard': self._resolve("./tensorboard_logs"),
            'csv_logs': self._resolve("./csv_logs"),
            'mlflow': self._resolve("./mlruns")
        }
        
        # rank 0ì—ì„œë§Œ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.local_rank == 0:
            self._create_all_directories()
            self._log_all_paths()
    
    def _resolve(self, path):
        """ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        p = Path(path)
        return p if p.is_absolute() else self.base_dir / p
    
    def _create_all_directories(self):
        """í•„ìš”í•œ ëª¨ë“  ë””ë ‰í† ë¦¬ ìƒì„±"""
        for name, path in self.paths.items():
            if name != 'data':  # ë°ì´í„° íŒŒì¼ì€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹˜
                path.mkdir(parents=True, exist_ok=True)
    
    def _log_all_paths(self):
        """ëª¨ë“  ê²½ë¡œ ë¡œê¹…"""
        print("=" * 50)
        print("ğŸ“ ê²½ë¡œ ì„¤ì •:")
        for name, path in self.paths.items():
            print(f"  {name}: {path}")
        print("=" * 50)
    
    def get(self, key):
        """ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        if key not in self.paths:
            raise KeyError(f"Unknown path key: {key}")
        return self.paths[key]
    
    def validate_data_path(self):
        """ë°ì´í„° íŒŒì¼ ê²½ë¡œ ê²€ì¦"""
        data_path = self.get('data')
        if not data_path.exists():
            raise FileNotFoundError(f"Data file not found: {data_path}")
        if self.local_rank == 0:
            print(f"âœ… Data file found: {data_path}")
        return True


# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ====================

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê¹… ë ˆë²¨ ì¡°ì •
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)
    logging.getLogger("deepspeed").setLevel(logging.INFO)
    
    return logging.getLogger(__name__)


def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def save_json(data: Dict[str, Any], filepath: str):
    """JSON íŒŒì¼ ì €ì¥"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def load_json(filepath: str) -> Dict[str, Any]:
    """JSON íŒŒì¼ ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def format_time(seconds: float) -> str:
    """ì´ˆë¥¼ ì‹œ:ë¶„:ì´ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"


def calculate_total_parameters(model) -> Dict[str, int]:
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        "total": total_params,
        "trainable": trainable_params,
        "trainable_percentage": (trainable_params / total_params) * 100 if total_params > 0 else 0
    }


def get_learning_rate(optimizer):
    """í˜„ì¬ í•™ìŠµë¥  ê°€ì ¸ì˜¤ê¸°"""
    for param_group in optimizer.param_groups:
        return param_group['lr']
    return 0.0


def create_optimizer_grouped_parameters(model, weight_decay: float = 0.01):
    """ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ê·¸ë£¹ ìƒì„±"""
    no_decay = ["bias", "LayerNorm.weight", "layer_norm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    return optimizer_grouped_parameters


def print_rank_0(message: str, rank: int = 0):
    """Rank 0ì—ì„œë§Œ ë©”ì‹œì§€ ì¶œë ¥"""
    if rank == 0:
        print(message)


def get_gpu_memory_info():
    """GPU ë©”ëª¨ë¦¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        return f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB"
    return "GPU not available"