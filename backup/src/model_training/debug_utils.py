#!/usr/bin/env python3
"""
Debugging Utilities for Training Interruption Analysis
pj_peft_deepspeed2_moniter_v2

íŒ¨ë”© ê²€ì¦ ë° í† í¬ë‚˜ì´ì € ë””ë²„ê¹… í•¨ìˆ˜ë“¤ í¬í•¨
"""

import os
import sys
import time
import torch
import psutil
import logging
import traceback
import signal
import gc
from datetime import datetime
from typing import Dict, Any, Optional, List
import json


class TrainingDebugger:
    """í•™ìŠµ ì¤‘ë‹¨ ì›ì¸ ë¶„ì„ì„ ìœ„í•œ ë””ë²„ê¹… í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any], logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.debug_config = config.get('debug', {})
        self.interruption_config = self.debug_config.get('interruption_analysis', {})
        
        self.enabled = self.debug_config.get('enabled', False)
        self.memory_tracking = self.interruption_config.get('memory_tracking', True)
        self.detailed_logging = self.interruption_config.get('detailed_logging', True)
        self.exception_handling = self.interruption_config.get('exception_handling', True)
        
        # ë©”ëª¨ë¦¬ ë° ì‹œìŠ¤í…œ ì¶”ì 
        self.memory_history = []
        self.system_metrics = []
        self.step_timings = []
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì •
        if self.exception_handling:
            self.setup_signal_handlers()
            
        self.logger.info(f"ğŸ› ë””ë²„ê¹… ëª¨ë“œ: {'í™œì„±í™”' if self.enabled else 'ë¹„í™œì„±í™”'}")
        if self.enabled:
            self.logger.info(f"   ğŸ“Š ë©”ëª¨ë¦¬ ì¶”ì : {self.memory_tracking}")
            self.logger.info(f"   ğŸ“ ìƒì„¸ ë¡œê¹…: {self.detailed_logging}")
            self.logger.info(f"   ğŸ›¡ï¸ ì˜ˆì™¸ ì²˜ë¦¬: {self.exception_handling}")
    
    def setup_signal_handlers(self):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì • (SIGTERM, SIGINT ë“±)"""
        def signal_handler(signum, frame):
            self.logger.error(f"ğŸš¨ ì‹œê·¸ë„ {signum} ìˆ˜ì‹ ë¨ - í›ˆë ¨ ì¤‘ë‹¨")
            self.save_debug_info(interruption_type="signal", signal_num=signum)
            sys.exit(1)
        
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
    
    def track_memory_usage(self, step: int, phase: str = "training"):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì """
        if not self.enabled or not self.memory_tracking:
            return
        
        # GPU ë©”ëª¨ë¦¬
        gpu_memory = {}
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_memory[f'gpu_{i}'] = {
                    'allocated': torch.cuda.memory_allocated(i) / 1024**3,
                    'reserved': torch.cuda.memory_reserved(i) / 1024**3,
                    'max_allocated': torch.cuda.max_memory_allocated(i) / 1024**3
                }
        
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        
        memory_info = {
            'step': step,
            'phase': phase,
            'timestamp': datetime.now().isoformat(),
            'gpu_memory': gpu_memory,
            'cpu_memory': {
                'used_gb': cpu_memory.used / 1024**3,
                'available_gb': cpu_memory.available / 1024**3,
                'percent': cpu_memory.percent
            }
        }
        
        self.memory_history.append(memory_info)
        
        # ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ 
        if cpu_memory.percent > 90:
            self.logger.warning(f"âš ï¸ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {cpu_memory.percent}%")
        
        for gpu_id, gpu_info in gpu_memory.items():
            if gpu_info['allocated'] > 20:  # 20GB ì´ìƒ
                self.logger.warning(f"âš ï¸ {gpu_id} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {gpu_info['allocated']:.2f}GB")
    
    def track_system_metrics(self, step: int):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì¶”ì """
        if not self.enabled:
            return
        
        # CPU ì‚¬ìš©ë¥ 
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
        disk_usage = psutil.disk_usage('/')
        
        # ë„¤íŠ¸ì›Œí¬ í†µê³„ (ë©€í‹° GPU í†µì‹  ì¶”ì )
        net_io = psutil.net_io_counters()
        
        system_info = {
            'step': step,
            'timestamp': datetime.now().isoformat(),
            'cpu_percent': cpu_percent,
            'disk_usage_percent': (disk_usage.used / disk_usage.total) * 100,
            'network': {
                'bytes_sent': net_io.bytes_sent,
                'bytes_recv': net_io.bytes_recv
            },
            'process_count': len(psutil.pids())
        }
        
        self.system_metrics.append(system_info)
        
        # ì‹œìŠ¤í…œ ë¶€í•˜ ê²½ê³ 
        if cpu_percent > 95:
            self.logger.warning(f"âš ï¸ CPU ì‚¬ìš©ë¥  ê³¼ë¶€í•˜: {cpu_percent}%")
    
    def track_step_timing(self, step: int, duration: float, phase: str = "training"):
        """ìŠ¤í… ì‹œê°„ ì¶”ì """
        if not self.enabled:
            return
        
        timing_info = {
            'step': step,
            'phase': phase,
            'duration': duration,
            'timestamp': datetime.now().isoformat()
        }
        
        self.step_timings.append(timing_info)
        
        # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ìŠ¤í… ê²½ê³ 
        if duration > 60:  # 60ì´ˆ ì´ìƒ
            self.logger.warning(f"âš ï¸ ìŠ¤í… {step} ì‹¤í–‰ ì‹œê°„ ê³¼ë„: {duration:.2f}ì´ˆ")
    
    def log_exception(self, exception: Exception, context: str = ""):
        """ì˜ˆì™¸ ìƒí™© ë¡œê¹…"""
        if not self.enabled:
            return
        
        self.logger.error(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ - {context}")
        self.logger.error(f"ì˜ˆì™¸ íƒ€ì…: {type(exception).__name__}")
        self.logger.error(f"ì˜ˆì™¸ ë©”ì‹œì§€: {str(exception)}")
        self.logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        
        # ë””ë²„ê·¸ ì •ë³´ ì €ì¥
        self.save_debug_info(
            interruption_type="exception",
            exception_type=type(exception).__name__,
            exception_message=str(exception),
            context=context
        )
    
    def check_environment_issues(self) -> Dict[str, Any]:
        """í™˜ê²½ ë¬¸ì œ ì²´í¬"""
        issues = {}
        
        # CUDA ê´€ë ¨ ì²´í¬
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                device_props = torch.cuda.get_device_properties(i)
                if device_props.total_memory < 8 * 1024**3:  # 8GB ë¯¸ë§Œ
                    issues[f'gpu_{i}_memory'] = f"GPU {i} ë©”ëª¨ë¦¬ ë¶€ì¡±: {device_props.total_memory / 1024**3:.1f}GB"
        
        # ë””ìŠ¤í¬ ê³µê°„ ì²´í¬
        disk_usage = psutil.disk_usage('/')
        if disk_usage.free < 10 * 1024**3:  # 10GB ë¯¸ë§Œ
            issues['disk_space'] = f"ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {disk_usage.free / 1024**3:.1f}GB ë‚¨ìŒ"
        
        # ë©”ëª¨ë¦¬ ì²´í¬
        memory = psutil.virtual_memory()
        if memory.available < 4 * 1024**3:  # 4GB ë¯¸ë§Œ
            issues['system_memory'] = f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡±: {memory.available / 1024**3:.1f}GB ì‚¬ìš©ê°€ëŠ¥"
        
        return issues
    
    def save_debug_info(self, interruption_type: str = "unknown", **kwargs):
        """ë””ë²„ê·¸ ì •ë³´ ì €ì¥"""
        if not self.enabled:
            return
        
        debug_info = {
            'interruption_type': interruption_type,
            'timestamp': datetime.now().isoformat(),
            'environment_issues': self.check_environment_issues(),
            'memory_history': self.memory_history[-10:],  # ìµœê·¼ 10ê°œ
            'system_metrics': self.system_metrics[-10:],   # ìµœê·¼ 10ê°œ
            'step_timings': self.step_timings[-20:],       # ìµœê·¼ 20ê°œ
            'additional_info': kwargs
        }
        
        # ë””ë²„ê·¸ íŒŒì¼ ì €ì¥
        debug_file = f"debug_interruption_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(debug_file, 'w') as f:
                json.dump(debug_info, f, indent=2, ensure_ascii=False)
            self.logger.info(f"ğŸ› ë””ë²„ê·¸ ì •ë³´ ì €ì¥ë¨: {debug_file}")
        except Exception as e:
            self.logger.error(f"ë””ë²„ê·¸ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë””ë²„ê¹… ì„¸ì…˜ ì •ë¦¬"""
        if not self.enabled:
            return
        
        self.logger.info("ğŸ› ë””ë²„ê¹… ì„¸ì…˜ ì •ë¦¬ ì¤‘...")
        
        # ìµœì¢… ë””ë²„ê·¸ ì •ë³´ ì €ì¥
        self.save_debug_info(interruption_type="cleanup")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self.memory_history.clear()
        self.system_metrics.clear()
        self.step_timings.clear()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        self.logger.info("ğŸ› ë””ë²„ê¹… ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")


def create_debugger(config: Dict[str, Any], logger: logging.Logger) -> TrainingDebugger:
    """ë””ë²„ê±° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return TrainingDebugger(config, logger) 


# =============================================================================
# íŒ¨ë”© ê²€ì¦ ë° í† í¬ë‚˜ì´ì € ë””ë²„ê¹… í•¨ìˆ˜ë“¤
# =============================================================================

def validate_tokenizer_settings(tokenizer, model, phase: str = "train", local_rank: int = 0) -> bool:
    """
    í† í¬ë‚˜ì´ì € ì„¤ì •ì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        model: ëª¨ë¸ ê°ì²´
        phase: ë‹¨ê³„ ("train" ë˜ëŠ” "inference")
        local_rank: ë¡œì»¬ ë­í¬ (ë¶„ì‚° í•™ìŠµìš©)
        
    Returns:
        bool: ê²€ì¦ ì„±ê³µ ì—¬ë¶€
    """
    if local_rank == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì¶œë ¥
        print(f"ğŸ” í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦ ({phase})")
        print(f"   - pad_token: {tokenizer.pad_token}")
        print(f"   - pad_token_id: {tokenizer.pad_token_id}")
        print(f"   - eos_token: {tokenizer.eos_token}")
        print(f"   - eos_token_id: {tokenizer.eos_token_id}")
        print(f"   - padding_side: {tokenizer.padding_side}")
        print(f"   - model.config.pad_token_id: {model.config.pad_token_id}")
    
    # ê²€ì¦ ë¡œì§
    validation_errors = []
    
    # 1. íŒ¨ë“œ í† í° ID ì¼ê´€ì„± ê²€ì¦ (Qwen3: pad_token_id == eos_token_id ì—¬ì•¼ í•¨)
    if tokenizer.pad_token_id != tokenizer.eos_token_id:
        validation_errors.append("Qwen3ì—ì„œëŠ” íŒ¨ë“œ í† í° IDì™€ EOS í† í° IDê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤")
    
    # 2. ëª¨ë¸-í† í¬ë‚˜ì´ì € ì„¤ì • ì¼ê´€ì„± ê²€ì¦
    if model.config.pad_token_id != tokenizer.pad_token_id:
        validation_errors.append("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ì˜ íŒ¨ë“œ í† í° IDê°€ ë‹¤ë¦…ë‹ˆë‹¤")
    
    # 3. ë‹¨ê³„ë³„ íŒ¨ë”© ë°©í–¥ ê²€ì¦
    if phase == "train" and tokenizer.padding_side != "right":
        validation_errors.append("í›ˆë ¨ ì‹œì—ëŠ” ì˜¤ë¥¸ìª½ íŒ¨ë”©ì´ ê¶Œì¥ë©ë‹ˆë‹¤")
    elif phase == "inference" and tokenizer.padding_side != "left":
        validation_errors.append("ì¶”ë¡  ì‹œì—ëŠ” ì™¼ìª½ íŒ¨ë”©ì´ í•„ìˆ˜ì…ë‹ˆë‹¤")
    
    # 4. íŒ¨ë“œ í† í° ì¡´ì¬ì„± ê²€ì¦
    if tokenizer.pad_token is None:
        validation_errors.append("íŒ¨ë“œ í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # 5. í† í° ID ìœ íš¨ì„± ê²€ì¦
    if tokenizer.pad_token_id is None or tokenizer.pad_token_id < 0:
        validation_errors.append("ìœ íš¨í•˜ì§€ ì•Šì€ íŒ¨ë“œ í† í° IDì…ë‹ˆë‹¤")
    
    # ê²€ì¦ ê²°ê³¼ ì¶œë ¥
    if validation_errors:
        if local_rank == 0:
            print("âŒ í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦ ì‹¤íŒ¨:")
            for error in validation_errors:
                print(f"   - {error}")
        return False
    else:
        if local_rank == 0:
            print("âœ… í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦ ì„±ê³µ")
        return True


def verify_batch_padding(batch, tokenizer, max_samples: int = 3, local_rank: int = 0):
    """
    ë°°ì¹˜ì˜ íŒ¨ë”© ìƒíƒœë¥¼ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        batch: í† í¬ë‚˜ì´ì €ë¡œ ì²˜ë¦¬ëœ ë°°ì¹˜
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        max_samples: ê²€ì¦í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
        local_rank: ë¡œì»¬ ë­í¬
    """
    if local_rank != 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì‹¤í–‰
        return
    
    print("ğŸ” ë°°ì¹˜ íŒ¨ë”© ìƒíƒœ ê²€ì¦:")
    
    # ë°°ì¹˜ ì •ë³´ ì¶œë ¥
    if 'input_ids' in batch:
        batch_size = batch['input_ids'].shape[0]
        seq_length = batch['input_ids'].shape[1]
        print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_length}")
        
        # ìƒ˜í”Œë³„ íŒ¨ë”© ìƒíƒœ í™•ì¸
        for i in range(min(max_samples, batch_size)):
            input_ids = batch['input_ids'][i]
            attention_mask = batch['attention_mask'][i] if 'attention_mask' in batch else None
            
            # íŒ¨ë“œ í† í° ìœ„ì¹˜ ì°¾ê¸°
            pad_positions = (input_ids == tokenizer.pad_token_id).nonzero(as_tuple=True)[0]
            non_pad_positions = (input_ids != tokenizer.pad_token_id).nonzero(as_tuple=True)[0]
            
            print(f"   - ìƒ˜í”Œ {i+1}:")
            print(f"     * íŒ¨ë“œ í† í° ìˆ˜: {len(pad_positions)}")
            print(f"     * ì‹¤ì œ í† í° ìˆ˜: {len(non_pad_positions)}")
            
            if len(pad_positions) > 0:
                # íŒ¨ë”© ë°©í–¥ í™•ì¸
                if pad_positions[0] < non_pad_positions[0]:
                    padding_side = "left"
                else:
                    padding_side = "right"
                print(f"     * íŒ¨ë”© ë°©í–¥: {padding_side}")
                
                # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ì¼ì¹˜ì„± í™•ì¸
                if attention_mask is not None:
                    pad_mask_zeros = (attention_mask == 0).sum().item()
                    if pad_mask_zeros == len(pad_positions):
                        print(f"     * ì–´í…ì…˜ ë§ˆìŠ¤í¬: âœ… ì¼ì¹˜")
                    else:
                        print(f"     * ì–´í…ì…˜ ë§ˆìŠ¤í¬: âŒ ë¶ˆì¼ì¹˜ ({pad_mask_zeros} vs {len(pad_positions)})")


def test_tokenizer_padding(tokenizer, test_texts: List[str], phase: str = "train", local_rank: int = 0):
    """
    í† í¬ë‚˜ì´ì € íŒ¨ë”© ë™ì‘ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        tokenizer: í† í¬ë‚˜ì´ì € ê°ì²´
        test_texts: í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        phase: ë‹¨ê³„ ("train" ë˜ëŠ” "inference")
        local_rank: ë¡œì»¬ ë­í¬
    """
    if local_rank != 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì‹¤í–‰
        return
    
    print(f"ğŸ§ª í† í¬ë‚˜ì´ì € íŒ¨ë”© í…ŒìŠ¤íŠ¸ ({phase}):")
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
    encoded = tokenizer(
        test_texts,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt",
        return_attention_mask=True
    )
    
    # ê²°ê³¼ ë¶„ì„
    print(f"   - ì…ë ¥ í…ìŠ¤íŠ¸ ìˆ˜: {len(test_texts)}")
    print(f"   - ì¶œë ¥ ë°°ì¹˜ í¬ê¸°: {encoded['input_ids'].shape}")
    
    # íŒ¨ë”© ìƒíƒœ í™•ì¸
    verify_batch_padding(encoded, tokenizer, max_samples=min(3, len(test_texts)), local_rank=0)
    
    # ë””ì½”ë”© í…ŒìŠ¤íŠ¸
    print("   - ë””ì½”ë”© í…ŒìŠ¤íŠ¸:")
    for i, text in enumerate(test_texts[:3]):
        decoded = tokenizer.decode(encoded['input_ids'][i], skip_special_tokens=False)
        print(f"     * ì›ë³¸ {i+1}: {text[:50]}...")
        print(f"     * ë””ì½”ë”©: {decoded[:100]}...") 