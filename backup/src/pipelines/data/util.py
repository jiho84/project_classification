#!/usr/bin/env python3
"""
ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í†µí•©
ëª¨ë“  ìŠ¤í…Œì´ì§€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ í•œ ê³³ì— ëª¨ìŒ
"""

import pandas as pd
import numpy as np
import json
import os
import time
from typing import Dict, Any, Tuple, List, Optional, Union
import logging
from datetime import datetime
import datetime as dt
from omegaconf import DictConfig
import holidays

logger = logging.getLogger(__name__)


# =============================================================================
# Config ìœ í‹¸ë¦¬í‹°
# =============================================================================

def get_stage_config(parameters: DictConfig, stage_name: str) -> Dict[str, Any]:
    """
    ìŠ¤í…Œì´ì§€ë³„ ì„¤ì • ì¶”ì¶œ
    
    Args:
        parameters: ì „ì²´ íŒŒë¼ë¯¸í„°
        stage_name: ìŠ¤í…Œì´ì§€ ì´ë¦„ (ì˜ˆ: "stage_0")
        
    Returns:
        ìŠ¤í…Œì´ì§€ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    stage_config = parameters.get(stage_name, {})
    
    return {
        "stage_config": stage_config,
        "mlflow_enabled": parameters.get("mlflow", {}).get("enabled", True),
        "local_base_path": parameters.get("paths", {}).get("local_base", "data/pipeline_metadata"),
        "artifact_paths": {
            "data": parameters.get("paths", {}).get("mlflow_artifacts", {}).get("data", "data"),
            "metadata": parameters.get("paths", {}).get("mlflow_artifacts", {}).get("metadata", "metadata")
        },
        "save_options": stage_config.get("save_options", {})
    }


# =============================================================================
# Data ìœ í‹¸ë¦¬í‹°
# =============================================================================

def validate_required_columns(df: pd.DataFrame, required_columns: List[str]) -> List[str]:
    """
    í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
    
    Args:
        df: ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
        required_columns: í•„ìˆ˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ëˆ„ë½ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    """
    missing_columns = [col for col in required_columns if col not in df.columns]
    return missing_columns


def get_column_statistics(df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
    """
    ì»¬ëŸ¼ë³„ í†µê³„ ì •ë³´ ì¶”ì¶œ
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        
    Returns:
        ì»¬ëŸ¼ë³„ í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    stats = {}
    
    for col in df.columns:
        col_stats = {
            "dtype": str(df[col].dtype),
            "count": int(df[col].count()),
            "null_count": int(df[col].isnull().sum()),
            "null_ratio": float(df[col].isnull().mean()),
            "unique_count": int(df[col].nunique())
        }
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ì— ëŒ€í•œ ì¶”ê°€ í†µê³„
        if pd.api.types.is_numeric_dtype(df[col]):
            non_null_values = df[col].dropna()
            if len(non_null_values) > 0:
                col_stats.update({
                    "mean": float(non_null_values.mean()),
                    "std": float(non_null_values.std()),
                    "min": float(non_null_values.min()),
                    "max": float(non_null_values.max()),
                    "median": float(non_null_values.median()),
                    "q1": float(non_null_values.quantile(0.25)),
                    "q3": float(non_null_values.quantile(0.75))
                })
        
        stats[col] = col_stats
    
    return stats


def apply_row_filters(df: pd.DataFrame, filter_config: Dict[str, Any]) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    í–‰ í•„í„°ë§ ì ìš©
    
    Args:
        df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
        filter_config: í•„í„°ë§ ì„¤ì •
        
    Returns:
        (í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„, í•„í„°ë§ í†µê³„)
    """
    initial_rows = len(df)
    filtered_df = df.copy()
    filter_stats = {
        "initial_rows": initial_rows,
        "filters_applied": []
    }
    
    # í–‰ í•„í„° ì ìš©
    row_filters = filter_config.get("row_filters", [])
    for filter_rule in row_filters:
        column = filter_rule.get("column")
        operator = filter_rule.get("operator", "==")
        value = filter_rule.get("value")
        
        if column in filtered_df.columns:
            before_rows = len(filtered_df)
            
            if operator == "==":
                filtered_df = filtered_df[filtered_df[column] == value]
            elif operator == "!=":
                filtered_df = filtered_df[filtered_df[column] != value]
            elif operator == ">":
                filtered_df = filtered_df[filtered_df[column] > value]
            elif operator == "<":
                filtered_df = filtered_df[filtered_df[column] < value]
            elif operator == ">=":
                filtered_df = filtered_df[filtered_df[column] >= value]
            elif operator == "<=":
                filtered_df = filtered_df[filtered_df[column] <= value]
            elif operator == "isin":
                filtered_df = filtered_df[filtered_df[column].isin(value)]
            elif operator == "not_null":
                filtered_df = filtered_df[filtered_df[column].notna()]
            elif operator == "is_null":
                filtered_df = filtered_df[filtered_df[column].isna()]
            
            removed_rows = before_rows - len(filtered_df)
            filter_stats["filters_applied"].append({
                "column": column,
                "operator": operator,
                "value": value,
                "rows_removed": removed_rows
            })
            
            logger.info(f"í•„í„° ì ìš©: {column} {operator} {value} â†’ {removed_rows}ê°œ í–‰ ì œê±°")
    
    filter_stats["final_rows"] = len(filtered_df)
    filter_stats["total_removed"] = initial_rows - len(filtered_df)
    filter_stats["removal_ratio"] = (initial_rows - len(filtered_df)) / initial_rows if initial_rows > 0 else 0
    
    return filtered_df, filter_stats


def calculate_class_distribution(df: pd.DataFrame, target_column: str) -> Dict[str, Any]:
    """
    í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        target_column: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        
    Returns:
        í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´
    """
    if target_column not in df.columns:
        return {"error": f"Target column '{target_column}' not found"}
    
    value_counts = df[target_column].value_counts()
    total_samples = len(df)
    
    distribution = {
        "num_classes": len(value_counts),
        "total_samples": total_samples,
        "class_counts": value_counts.to_dict(),
        "class_ratios": (value_counts / total_samples).to_dict(),
        "min_class_samples": int(value_counts.min()),
        "max_class_samples": int(value_counts.max()),
        "class_imbalance_ratio": float(value_counts.max() / value_counts.min()) if value_counts.min() > 0 else float('inf')
    }
    
    # ìƒìœ„ 10ê°œ í´ë˜ìŠ¤
    distribution["top_10_classes"] = value_counts.head(10).to_dict()
    
    # í¬ì†Œ í´ë˜ìŠ¤ (ì „ì²´ì˜ 1% ë¯¸ë§Œ)
    rare_threshold = total_samples * 0.01
    rare_classes = value_counts[value_counts < rare_threshold]
    distribution["rare_classes"] = {
        "count": len(rare_classes),
        "samples": int(rare_classes.sum()),
        "ratio": float(rare_classes.sum() / total_samples) if total_samples > 0 else 0
    }
    
    return distribution


# =============================================================================
# Label Mapping ìœ í‹¸ë¦¬í‹°
# =============================================================================

def create_label_mapping(
    data: pd.DataFrame,
    target_column: str,
    total_num_classes: int = 300,
    dummy_prefix: str = "DUMMY_",
    mode: str = "auto",
    existing_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    ë¼ë²¨ ë§¤í•‘ ìƒì„± (ë”ë¯¸ í´ë˜ìŠ¤ í¬í•¨)
    
    Args:
        data: ì…ë ¥ ë°ì´í„°
        target_column: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        total_num_classes: ì´ í´ë˜ìŠ¤ ìˆ˜
        dummy_prefix: ë”ë¯¸ í´ë˜ìŠ¤ ì ‘ë‘ì‚¬
        mode: ìƒì„± ëª¨ë“œ ("auto", "create_new", "update")
        existing_path: ê¸°ì¡´ ë§¤í•‘ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ë¼ë²¨ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    """
    if target_column not in data.columns:
        logger.warning(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        return {
            "text2id": {},
            "id2text": {},
            "vocabulary": [],
            "num_classes": 0,
            "num_base_classes": 0,
            "num_dummy_classes": 0,
            "target_column": target_column,
            "creation_timestamp": datetime.now().isoformat(),
            "mode": mode
        }
    
    # í˜„ì¬ ë°ì´í„°ì˜ ê³ ìœ  í´ë˜ìŠ¤
    unique_labels = sorted([str(label) for label in data[target_column].unique()])
    current_class_count = len(unique_labels)
    logger.info(f"ğŸ“Š í˜„ì¬ ë°ì´í„°ì—ì„œ {current_class_count}ê°œì˜ ê³ ìœ  í´ë˜ìŠ¤ ë°œê²¬")
    
    # modeê°€ autoì¸ ê²½ìš° existing_path ì¡´ì¬ ì—¬ë¶€ë¡œ ê²°ì •
    if mode == "auto":
        mode = "update" if existing_path and os.path.exists(existing_path) else "create_new"
    
    if mode == "update" and existing_path and os.path.exists(existing_path):
        # ê¸°ì¡´ ë§¤í•‘ ë¡œë“œ ë° ì—…ë°ì´íŠ¸
        logger.info(f"ğŸ”„ ê¸°ì¡´ ë§¤í•‘ íŒŒì¼ ë¡œë“œ ë° ì—…ë°ì´íŠ¸: {existing_path}")
        
        with open(existing_path, 'r', encoding='utf-8') as f:
            existing_mapping = json.load(f)
        
        # ê¸°ì¡´ ì‹¤ì œ í´ë˜ìŠ¤ ì¶”ì¶œ (ID ìˆœì„œëŒ€ë¡œ)
        existing_real_labels = []
        for label, id_val in sorted(existing_mapping['text2id'].items(), key=lambda x: x[1]):
            if not label.startswith(dummy_prefix):
                existing_real_labels.append(label)
        
        # ì‹ ê·œ í´ë˜ìŠ¤ ì°¾ê¸°
        new_labels = [label for label in unique_labels if label not in existing_real_labels]
        if new_labels:
            logger.info(f"ğŸ†• ì‹ ê·œ í´ë˜ìŠ¤ ë°œê²¬: {new_labels}")
        
        # ì „ì²´ ì‹¤ì œ í´ë˜ìŠ¤ = ê¸°ì¡´ + ì‹ ê·œ (ìˆœì„œ ìœ ì§€)
        # ì¤‘ìš”: ê¸°ì¡´ í´ë˜ìŠ¤ì˜ IDëŠ” ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•ŠìŒ
        all_real_labels = existing_real_labels + sorted(new_labels)
        base_class_count = len(all_real_labels)
        
        # ë”ë¯¸ í´ë˜ìŠ¤ ì¬ìƒì„±
        num_dummy = max(0, total_num_classes - base_class_count)
        dummy_classes = [f"{dummy_prefix}{i:03d}" for i in range(num_dummy)]
        
        # ì „ì²´ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        all_labels = all_real_labels + dummy_classes
        
        # ë¼ë²¨ ë§¤í•‘ ì¬ìƒì„±
        label_mapping = {
            "text2id": {str(label): int(idx) for idx, label in enumerate(all_labels)},
            "id2text": {int(idx): str(label) for idx, label in enumerate(all_labels)},
            "vocabulary": all_labels,
            "num_classes": int(len(all_labels)),
            "num_base_classes": int(base_class_count),
            "num_dummy_classes": int(num_dummy),
            "target_column": target_column,
            "creation_timestamp": existing_mapping.get("creation_timestamp", datetime.now().isoformat()),
            "update_timestamp": datetime.now().isoformat(),
            "mode": "updated",
            "new_classes_added": new_labels,
            "existing_classes_count": len(existing_real_labels)
        }
        
        logger.info(f"âœ… ë¼ë²¨ ë§¤í•‘ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì´ {base_class_count}ê°œ ì‹¤ì œ í´ë˜ìŠ¤ ({len(new_labels)}ê°œ ì‹ ê·œ)")
        logger.info(f"ğŸ“‹ ê¸°ì¡´ í´ë˜ìŠ¤ ID ìœ ì§€, ì‹ ê·œ í´ë˜ìŠ¤ë§Œ ì¶”ê°€")
        
    else:
        # ìƒˆë¡œ ìƒì„±
        logger.info("ğŸ†• ìƒˆë¡œìš´ ë¼ë²¨ ë§¤í•‘ ìƒì„±")
        base_class_count = current_class_count
        
        # ë”ë¯¸ í´ë˜ìŠ¤ ìƒì„±
        num_dummy = max(0, total_num_classes - base_class_count)
        dummy_classes = [f"{dummy_prefix}{i:03d}" for i in range(num_dummy)]
        all_labels = unique_labels + dummy_classes
        
        # ë¼ë²¨ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
        label_mapping = {
            "text2id": {str(label): int(idx) for idx, label in enumerate(all_labels)},
            "id2text": {int(idx): str(label) for idx, label in enumerate(all_labels)},
            "vocabulary": all_labels,
            "num_classes": int(len(all_labels)),
            "num_base_classes": int(base_class_count),
            "num_dummy_classes": int(num_dummy),
            "target_column": target_column,
            "creation_timestamp": datetime.now().isoformat(),
            "mode": "created"
        }
    
    return label_mapping


# =============================================================================
# Feature Engineering ìœ í‹¸ë¦¬í‹°
# =============================================================================

def apply_robust_standardization(
    data: pd.DataFrame,
    config: Dict[str, Any]
) -> Tuple[pd.DataFrame, Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """
    Robust standardization ì ìš©
    
    Args:
        data: ì…ë ¥ ë°ì´í„°
        config: robust_standardization ì„¤ì •
        
    Returns:
        (ì²˜ë¦¬ëœ ë°ì´í„°, íšŒì‚¬ë³„ í†µê³„, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìš”ì•½, ì´ìƒì¹˜ í†µê³„)
    """
    engineered_data = data.copy()
    company_stats = {}
    outlier_stats = {}
    feature_engineering_summary = {
        "features_added": 0,
        "new_features": [],
        "total_features": len(engineered_data.columns)
    }
    
    if config.get("enable", False):
        group_col = config.get("group_column", "co_vat_id")
        target_col = config.get("target_column", "pur_total")
        
        if group_col in engineered_data.columns and target_col in engineered_data.columns:
            # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
            new_feature, company_stats = optimized_robust_standardization_with_minmax(
                df=engineered_data,
                group_col=group_col,
                target_col=target_col,
                outlier_method=config.get("outlier_bounds", {}).get("method", "iqr"),
                iqr_multiplier=config.get("outlier_bounds", {}).get("iqr_multiplier", 1.5),
                z_score_threshold=config.get("outlier_bounds", {}).get("z_score_threshold", 3.0),
                percentile_range=config.get("outlier_bounds", {}).get("percentile_range", [5, 95]),
                min_group_size=config.get("min_group_size", 3),
                min_normal_size=config.get("min_normal_size", 2),
                scaling_range=config.get("scaling_range", [1, 100])
            )
            
            # ìƒˆ ì¹¼ëŸ¼ ì¶”ê°€
            output_suffix = config.get("output_column_suffix", "_scaled")
            new_column_name = f"{target_col}{output_suffix}"
            engineered_data[new_column_name] = new_feature
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            feature_engineering_summary["features_added"] = 1
            feature_engineering_summary["new_features"] = [new_column_name]
            feature_engineering_summary["total_features"] = len(engineered_data.columns)
            
            # ì´ìƒì¹˜ í†µê³„
            total_outliers = sum(stat.get("outlier_removed", 0) for stat in company_stats.values())
            total_samples = sum(stat.get("sample_count", 0) for stat in company_stats.values())
            outlier_stats = {
                "total_outliers_removed": total_outliers,
                "total_samples_processed": total_samples,
                "outlier_ratio": total_outliers / total_samples if total_samples > 0 else 0,
                "companies_processed": len(company_stats)
            }
    
    return engineered_data, company_stats, feature_engineering_summary, outlier_stats


def optimized_robust_standardization_with_minmax(
    df: pd.DataFrame, 
    group_col: str, 
    target_col: str,
    outlier_method: str = "iqr",
    iqr_multiplier: float = 1.5,
    z_score_threshold: float = 3.0,
    percentile_range: List[float] = [5, 95],
    min_group_size: int = 3,
    min_normal_size: int = 2,
    scaling_range: List[float] = [1, 100]
) -> Tuple[pd.Series, Dict[str, Any]]:
    """
    ìµœì í™”ëœ ê·¸ë£¹ë³„ ì´ìƒì¹˜ ê°•ê±´ í‘œì¤€í™” + MinMax ë³€í™˜
    """
    logger.info(f"ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘: {group_col} â†’ {target_col}")
    start_time = time.time()
    
    # ê²°ê³¼ ì‹œë¦¬ì¦ˆ ì´ˆê¸°í™”
    result = pd.Series(index=df.index, dtype='float64')
    
    # íšŒì‚¬ë³„ í†µê³„ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
    company_stats = {}
    
    # ê·¸ë£¹ë³„ ì²˜ë¦¬
    processed_groups = 0
    processed_rows = 0
    
    for group_name, group_data in df.groupby(group_col):
        group_size = len(group_data)
        
        if group_size < min_group_size:
            continue
            
        series = group_data[target_col]
        
        # ì ˆëŒ€ê°’ ë³€í™˜ (ì´ìƒì¹˜ íƒì§€ìš©)
        series_abs = series.abs()
        
        # ì´ìƒì¹˜ íƒì§€
        if outlier_method == "iqr":
            Q1 = series_abs.quantile(0.25)
            Q3 = series_abs.quantile(0.75)
            IQR = Q3 - Q1
            
            if IQR == 0:
                result.loc[group_data.index] = (scaling_range[0] + scaling_range[1]) / 2
                processed_groups += 1
                processed_rows += group_size
                continue
            
            normal_mask = (series_abs >= Q1 - iqr_multiplier * IQR) & (series_abs <= Q3 + iqr_multiplier * IQR)
            
        elif outlier_method == "z_score":
            mean_val = series_abs.mean()
            std_val = series_abs.std()
            
            if std_val == 0:
                result.loc[group_data.index] = (scaling_range[0] + scaling_range[1]) / 2
                processed_groups += 1
                processed_rows += group_size
                continue
                
            z_scores = np.abs((series_abs - mean_val) / std_val)
            normal_mask = z_scores <= z_score_threshold
            
        elif outlier_method == "percentile":
            lower_bound = series_abs.quantile(percentile_range[0] / 100)
            upper_bound = series_abs.quantile(percentile_range[1] / 100)
            normal_mask = (series_abs >= lower_bound) & (series_abs <= upper_bound)
        
        else:
            raise ValueError(f"Unknown outlier method: {outlier_method}")
        
        normal_values = series_abs[normal_mask]
        
        if len(normal_values) < min_normal_size:
            continue
            
        # ì •ìƒ ë°ì´í„°ë¡œ í‘œì¤€í™” íŒŒë¼ë¯¸í„° í•™ìŠµ
        mean_val = normal_values.mean()
        std_val = normal_values.std()
        
        if std_val == 0:
            result.loc[group_data.index] = (scaling_range[0] + scaling_range[1]) / 2
            processed_groups += 1
            processed_rows += group_size
            continue
        
        # íšŒì‚¬ë³„ í†µê³„ ì €ì¥
        company_stats[str(group_name)] = {
            'mean': float(mean_val),
            'std': float(std_val),
            'sample_count': int(len(normal_values)),
            'outlier_method': outlier_method,
            'outlier_removed': int(group_size - len(normal_values))
        }
        
        # í‘œì¤€í™” ì ìš© (ì›ë³¸ ë°ì´í„°ì˜ ì ˆëŒ€ê°’ì— ëŒ€í•´)
        standardized = (series_abs - mean_val) / std_val
        result.loc[group_data.index] = standardized.astype(np.float64)
        
        processed_groups += 1
        processed_rows += group_size
    
    # ì„ í˜•ë³€í™˜ ì ìš© (1-100)
    valid_mask = result.notna()
    if valid_mask.sum() > 0:
        valid_z_scores = result[valid_mask]
        
        # Z-scoreë¥¼ 1-100 ë²”ìœ„ë¡œ ì—°ì†ì ìœ¼ë¡œ ë§¤í•‘
        # ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì— ë§ì¶˜ ë²”ìœ„: z-score -1 â†’ 1, z-score 5 â†’ 100
        # ì ˆëŒ€ê°’ ë³€í™˜ í›„ z-score ë¶„í¬ì— ì í•©í•œ ë²”ìœ„
        linear_scaled = 1 + (valid_z_scores - (-1)) * (100 - 1) / (5 - (-1))
        
        # 1-100 ë²”ìœ„ë¡œ í´ë¦¬í•‘ (ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ëŠ” ê°’ë“¤ì€ 1 ë˜ëŠ” 100ìœ¼ë¡œ ì²˜ë¦¬)
        linear_scaled = np.clip(linear_scaled, scaling_range[0], scaling_range[1])
        
        # ì •ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì†Œìˆ˜ì  ì œê±°
        result.loc[valid_mask] = np.round(linear_scaled).astype(int)
    
    elapsed_time = time.time() - start_time
    
    logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {processed_groups} ê·¸ë£¹, {processed_rows} í–‰ ({elapsed_time:.2f}ì´ˆ)")
    
    return result, company_stats


# =============================================================================
# Memory Optimization ìœ í‹¸ë¦¬í‹°
# =============================================================================

def optimize_dataframe_dtypes(df: pd.DataFrame, 
                            categorical_threshold: float = 0.5,
                            exclude_columns: Optional[List[str]] = None) -> pd.DataFrame:
    """
    DataFrameì˜ ë°ì´í„° íƒ€ì…ì„ ìµœì í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
    
    Args:
        df: ìµœì í™”í•  DataFrame
        categorical_threshold: ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜í•  unique ratio ì„ê³„ê°’
        exclude_columns: ìµœì í™”ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ìµœì í™”ëœ DataFrame
    """
    exclude_columns = exclude_columns or []
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
    logger.info(f"ğŸ“Š ìµœì í™” ì „ ë©”ëª¨ë¦¬: {initial_memory:.2f} MB")
    
    # 1. ì •ìˆ˜í˜• ë‹¤ìš´ìºìŠ¤íŒ…
    int_columns = df.select_dtypes(include=['int64']).columns
    for col in int_columns:
        if col not in exclude_columns:
            col_min = df[col].min()
            col_max = df[col].max()
            
            # int8ë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
            if col_min >= -128 and col_max <= 127:
                df[col] = df[col].astype(np.int8)
            # int16ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
            elif col_min >= -32768 and col_max <= 32767:
                df[col] = df[col].astype(np.int16)
            # int32ë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
            elif col_min >= -2147483648 and col_max <= 2147483647:
                df[col] = df[col].astype(np.int32)
    
    # 2. ì‹¤ìˆ˜í˜• ë‹¤ìš´ìºìŠ¤íŒ…
    float_columns = df.select_dtypes(include=['float64']).columns
    for col in float_columns:
        if col not in exclude_columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
    
    # 3. ë¬¸ìì—´ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜
    object_columns = df.select_dtypes(include=['object']).columns
    for col in object_columns:
        if col not in exclude_columns:
            unique_count = df[col].nunique()
            total_count = len(df[col])
            unique_ratio = unique_count / total_count
            
            # unique ë¹„ìœ¨ì´ ì„ê³„ê°’ ì´í•˜ë©´ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜
            if unique_ratio < categorical_threshold:
                df[col] = df[col].astype('category')
                logger.debug(f"  {col}: object â†’ category (unique: {unique_count:,})")
    
    # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
    reduction_pct = (initial_memory - final_memory) / initial_memory * 100
    
    logger.info(f"ğŸ“Š ìµœì í™” í›„ ë©”ëª¨ë¦¬: {final_memory:.2f} MB (ê°ì†Œìœ¨: {reduction_pct:.1f}%)")
    
    return df


def reduce_metadata_size(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë©”íƒ€ë°ì´í„° í¬ê¸° ì¤„ì´ê¸°
    
    Args:
        metadata: ì›ë³¸ ë©”íƒ€ë°ì´í„°
        
    Returns:
        ì¶•ì†Œëœ ë©”íƒ€ë°ì´í„°
    """
    reduced_metadata = {}
    
    for key, value in metadata.items():
        # í° ë¦¬ìŠ¤íŠ¸ëŠ” ìš”ì•½ ì •ë³´ë§Œ ì €ì¥
        if isinstance(value, list) and len(value) > 100:
            reduced_metadata[key] = {
                "type": "list",
                "length": len(value),
                "sample": value[:5] if len(value) > 5 else value
            }
        # DataFrameì€ shape ì •ë³´ë§Œ ì €ì¥
        elif isinstance(value, pd.DataFrame):
            reduced_metadata[key] = {
                "type": "DataFrame",
                "shape": value.shape,
                "columns": list(value.columns)[:10]  # ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ë§Œ
            }
        # ë”•ì…”ë„ˆë¦¬ëŠ” ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
        elif isinstance(value, dict):
            reduced_metadata[key] = reduce_metadata_size(value)
        else:
            reduced_metadata[key] = value
    
    return reduced_metadata


def get_dataframe_memory_info(df: pd.DataFrame) -> Dict[str, Any]:
    """
    DataFrameì˜ ë©”ëª¨ë¦¬ ì‚¬ìš© ì •ë³´ ë°˜í™˜
    
    Args:
        df: ë¶„ì„í•  DataFrame
        
    Returns:
        ë©”ëª¨ë¦¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    memory_usage = df.memory_usage(deep=True)
    total_memory = memory_usage.sum() / 1024 / 1024
    
    # ì»¬ëŸ¼ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ìƒìœ„ 10ê°œ)
    column_memory = memory_usage.drop('Index').sort_values(ascending=False).head(10)
    column_memory_mb = column_memory / 1024 / 1024
    
    # ë°ì´í„° íƒ€ì…ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    dtype_memory = {}
    for dtype in df.dtypes.unique():
        cols = df.select_dtypes(include=[dtype]).columns
        dtype_memory[str(dtype)] = df[cols].memory_usage(deep=True).sum() / 1024 / 1024
    
    return {
        "total_memory_mb": total_memory,
        "shape": df.shape,
        "top_memory_columns": column_memory_mb.to_dict(),
        "memory_by_dtype": dtype_memory,
        "dtype_counts": df.dtypes.value_counts().to_dict()
    }


# =============================================================================
# Text Encoding ìœ í‹¸ë¦¬í‹°
# =============================================================================

def prepare_split_data(
    df: pd.DataFrame, 
    split_name: str,
    target_column: str,
    feature_columns: List[str],
    label_mapping: Dict[str, Any],
    text_encoding_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ë¶„í•  ë°ì´í„° ì¤€ë¹„ ë° í…ìŠ¤íŠ¸ ì¸ì½”ë”©
    
    Args:
        df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
        split_name: ë¶„í•  ì´ë¦„ (train/validation/test)
        target_column: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
        feature_columns: í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        label_mapping: ë¼ë²¨ ë§¤í•‘ ì •ë³´
        text_encoding_config: í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„¤ì •
        
    Returns:
        ì¤€ë¹„ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    if len(df) == 0:
        return {'text': [], 'labels': [], 'original_labels': []}
    
    # í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„¤ì • ì¶”ì¶œ
    encoding_method = text_encoding_config.get("encoding_method", "sequential")
    
    if encoding_method == "sequential":
        sequential_config = text_encoding_config.get("sequential", {})
        separator = sequential_config.get("separator", "^ ")
        column_value_separator = sequential_config.get("column_value_separator", ":")
        include_column_names = sequential_config.get("include_column_names", True)
        null_replacement = sequential_config.get("null_replacement", "None")
        # ê²°ì¸¡ê°’ íŒ¨í„´ë“¤ ê°€ì ¸ì˜¤ê¸°
        missing_patterns = text_encoding_config.get("missing_value_patterns", [
            "N/A", "NULL", "nan", "NaN", "null", "None", "none", 
            "#N/A", "#DIV/0!", "<NA>", "NaT", ""
        ])
    else:
        # ê¸°ë³¸ê°’
        separator = "^ "
        column_value_separator = ":"
        include_column_names = True
        null_replacement = "None"
        missing_patterns = [
            "N/A", "NULL", "nan", "NaN", "null", "None", "none", 
            "#N/A", "#DIV/0!", "<NA>", "NaT", ""
        ]
    
    # í”¼ì²˜ ì»¬ëŸ¼ ê²°ì •
    if feature_columns:
        # feature_columnsì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        feat_cols = [col for col in feature_columns if col in df.columns]
        if not feat_cols:
            # feature_columnsê°€ ëª¨ë‘ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ íƒ€ê²Ÿ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©
            logger.warning(f"âš ï¸ feature_columnsê°€ ë°ì´í„°ì™€ ë§¤ì¹­ë˜ì§€ ì•ŠìŒ. ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©.")
            feat_cols = [col for col in df.columns if col != target_column]
    else:
        feat_cols = [col for col in df.columns if col != target_column]
    
    # í…ìŠ¤íŠ¸ ë³€í™˜ (ë²¡í„°í™”ëœ ë°©ì‹)
    texts = create_text_representation_vectorized(
        df, feat_cols, separator, include_column_names, null_replacement, missing_patterns, column_value_separator
    )
    
    # ë¼ë²¨ ë³€í™˜
    if target_column in df.columns:
        original_labels = df[target_column].astype(str).tolist()
        text2id = label_mapping.get('text2id', {})
        labels = [text2id.get(str(label), 0) for label in original_labels]
    else:
        # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ë¼ë²¨ ì‚¬ìš©
        logger.warning(f"âš ï¸ íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ ë°ì´í„°ì— ì—†ìŒ. ë”ë¯¸ ë¼ë²¨ ì‚¬ìš©.")
        original_labels = ['UNKNOWN'] * len(df)
        labels = [0] * len(df)
    
    logger.info(f"   {split_name}: {len(texts):,} ìƒ˜í”Œ")
    
    return {
        'text': texts,
        'labels': labels,
        'original_labels': original_labels
    }


def create_text_representation(
    row: pd.Series, 
    feature_columns: List[str],
    separator: str = "^ ",
    include_column_names: bool = True,
    null_replacement: str = "None",
    column_value_separator: str = ":"
) -> str:
    """
    í”¼ì²˜ ê°’ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    
    Args:
        row: ë°ì´í„° í–‰
        feature_columns: í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        separator: êµ¬ë¶„ì
        include_column_names: ì»¬ëŸ¼ëª… í¬í•¨ ì—¬ë¶€
        null_replacement: NULL ê°’ ëŒ€ì²´ ë¬¸ìì—´
        column_value_separator: ì¹¼ëŸ¼ëª…ê³¼ ê°’ ì‚¬ì´ì˜ êµ¬ë¶„ì
        
    Returns:
        í…ìŠ¤íŠ¸ í‘œí˜„
    """
    text_parts = []
    
    for col in feature_columns:
        value = row[col]
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        if pd.isna(value) or value == '' or value is None:
            value_str = null_replacement
        else:
            # float íƒ€ì…ì´ê³  ì •ìˆ˜ê°’ì¸ ê²½ìš° ì •ìˆ˜ë¡œ ë³€í™˜
            if isinstance(value, float) and pd.notna(value) and value.is_integer():
                value_str = str(int(value))
            else:
                value_str = str(value)
        
        # ì»¬ëŸ¼ëª… í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
        if include_column_names:
            text_parts.append(f"{col}{column_value_separator}{value_str}")
        else:
            text_parts.append(value_str)
    
    return separator.join(text_parts)


def perform_tokenizer_analysis(
    texts: List[str],
    labels: List[int],
    tokenizer_analysis_config: Dict[str, Any],
    label_mapping: Dict[str, Any]
) -> Dict[str, Any]:
    """
    í† í¬ë‚˜ì´ì € ë¶„ì„ ìˆ˜í–‰ (ì„ íƒì )
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
        tokenizer_analysis_config: í† í¬ë‚˜ì´ì € ë¶„ì„ ì„¤ì •
        label_mapping: ë¼ë²¨ ë§¤í•‘ ì •ë³´
        
    Returns:
        ë¶„ì„ ê²°ê³¼
    """
    tokenizer_analysis = {}
    
    if not tokenizer_analysis_config.get("enabled", False):
        return tokenizer_analysis
    
    try:
        # í† í¬ë‚˜ì´ì € ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        tokenizer_name = tokenizer_analysis_config.get("tokenizer_name", "Qwen/Qwen3-4B")
        sample_size = tokenizer_analysis_config.get("sample_size", 1000)
        
        # ìƒ˜í”Œë§
        if sample_size > 0 and sample_size < len(texts):
            indices = np.random.choice(len(texts), sample_size, replace=False)
            sample_texts = [texts[i] for i in indices]
            sample_labels = [labels[i] for i in indices]
        else:
            sample_texts = texts
            sample_labels = labels
        
        # ì‹¤ì œ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì—¬ë¶€ ì²´í¬
        use_actual_tokenizer = tokenizer_analysis_config.get("use_actual_tokenizer", False)
        
        if use_actual_tokenizer:
            try:
                # ì‹¤ì œ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì„ íƒì )
                from transformers import AutoTokenizer
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
                
                # í† í¬ë‚˜ì´ì§• ë° ë¶„ì„
                tokenized_texts = []
                token_lengths = []
                
                for text in sample_texts[:10]:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ë¶„ì„
                    tokens = tokenizer.tokenize(text)
                    tokenized_texts.append({
                        'original': text,  # ì „ì²´ í…ìŠ¤íŠ¸ í‘œì‹œ
                        'tokens': tokens,  # ëª¨ë“  í† í° í‘œì‹œ
                        'token_count': len(tokens)
                    })
                
                # ì „ì²´ ìƒ˜í”Œì— ëŒ€í•œ í† í° ê¸¸ì´ ê³„ì‚°
                for text in sample_texts:
                    tokens = tokenizer.tokenize(text)
                    token_lengths.append(len(tokens))
                
                tokenizer_analysis['tokenizer_name'] = tokenizer_name
                tokenizer_analysis['vocab_size'] = tokenizer.vocab_size
                tokenizer_analysis['tokenization_samples'] = tokenized_texts
                
            except Exception as e:
                logger.warning(f"ì‹¤ì œ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨, ë‹¨ìˆœ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´: {e}")
                use_actual_tokenizer = False
        
        if not use_actual_tokenizer:
            # ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ í† í° ë¶„ì„
            token_lengths = [len(text.split()) for text in sample_texts]
            
            # ìƒ˜í”Œ í† í¬ë‚˜ì´ì§• (ê³µë°± ê¸°ë°˜)
            tokenization_samples = []
            for i, text in enumerate(sample_texts[:5]):  # ì²˜ìŒ 5ê°œë§Œ
                words = text.split()
                tokenization_samples.append({
                    'index': i,
                    'original': text,  # ì „ì²´ í…ìŠ¤íŠ¸ í‘œì‹œ
                    'word_tokens': words,  # ëª¨ë“  ë‹¨ì–´ í‘œì‹œ
                    'word_count': len(words),
                    'label': label_mapping.get('id2text', {}).get(str(sample_labels[i]), str(sample_labels[i]))
                })
            
            tokenizer_analysis['tokenizer_name'] = 'whitespace_tokenizer'
            tokenizer_analysis['tokenization_samples'] = tokenization_samples
        
        # í† í° ê¸¸ì´ í†µê³„
        percentiles = tokenizer_analysis_config.get("token_length_stats", {}).get("percentiles", [25, 50, 75, 90, 95, 99])
        
        tokenizer_analysis['token_length_stats'] = {
            'sample_size': len(sample_texts),
            'mean': float(np.mean(token_lengths)),
            'std': float(np.std(token_lengths)),
            'min': int(np.min(token_lengths)),
            'max': int(np.max(token_lengths)),
            'percentiles': {f'p{p}': int(np.percentile(token_lengths, p)) for p in percentiles}
        }
        
        # í† í° ê¸¸ì´ ë¶„í¬
        tokenizer_analysis['token_length_distribution'] = {
            '0-50': sum(1 for l in token_lengths if l <= 50),
            '51-100': sum(1 for l in token_lengths if 50 < l <= 100),
            '101-200': sum(1 for l in token_lengths if 100 < l <= 200),
            '201-300': sum(1 for l in token_lengths if 200 < l <= 300),
            '301-400': sum(1 for l in token_lengths if 300 < l <= 400),
            '401-500': sum(1 for l in token_lengths if 400 < l <= 500),
            '500+': sum(1 for l in token_lengths if l > 500)
        }
        
        logger.info(f"ğŸ”¤ í† í¬ë‚˜ì´ì € ë¶„ì„ ì™„ë£Œ (ìƒ˜í”Œ í¬ê¸°: {len(sample_texts)})")
        
    except Exception as e:
        logger.warning(f"í† í¬ë‚˜ì´ì € ë¶„ì„ ì‹¤íŒ¨: {e}")
        tokenizer_analysis = {'error': str(e)}
    
    return tokenizer_analysis


def create_text_representation_vectorized(
    df: pd.DataFrame, 
    feature_columns: List[str],
    separator: str = "^ ",
    include_column_names: bool = True,
    null_replacement: str = "None",
    missing_patterns: List[str] = None,
    column_value_separator: str = ":"
) -> List[str]:
    """
    ë²¡í„°í™”ëœ í…ìŠ¤íŠ¸ í‘œí˜„ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        feature_columns: í”¼ì²˜ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        separator: êµ¬ë¶„ì
        include_column_names: ì»¬ëŸ¼ëª… í¬í•¨ ì—¬ë¶€
        null_replacement: NULL ê°’ ëŒ€ì²´ ë¬¸ìì—´
        missing_patterns: ê²°ì¸¡ê°’ìœ¼ë¡œ ì¸ì‹í•  íŒ¨í„´ë“¤
        
    Returns:
        í…ìŠ¤íŠ¸ í‘œí˜„ ë¦¬ìŠ¤íŠ¸
    """
    # 1. ê²°ì¸¡ê°’ê³¼ íƒ€ì… ì²˜ë¦¬ë¥¼ ë²¡í„°í™”ë¡œ ìˆ˜í–‰
    df_processed = df[feature_columns].copy()
    
    # 2. ê²°ì¸¡ê°’ íŒ¨í„´ ì„¤ì •
    if missing_patterns is None:
        missing_patterns = [
            "N/A", "NULL", "nan", "NaN", "null", "None", "none", 
            "#N/A", "#DIV/0!", "<NA>", "NaT", ""
        ]
    
    # 3. ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ë©´ì„œ ê²°ì¸¡ê°’ ì²˜ë¦¬
    
    for col in feature_columns:
        series = df_processed[col]
        
        # ë¨¼ì € ë¬¸ìì—´ë¡œ ë³€í™˜
        series_str = series.astype(str)
        
        # ëª¨ë“  ê²°ì¸¡ê°’ íŒ¨í„´ë“¤ì„ null_replacement('None')ë¡œ ëŒ€ì²´ - ë²¡í„°í™”ëœ ë°©ì‹
        # replaceë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ ê°œì„ 
        series_str = series_str.replace(missing_patterns, null_replacement)
        
        # float íƒ€ì…ì˜ ì •ìˆ˜ê°’ ì²˜ë¦¬
        if series.dtype in ['float64', 'float32']:
            # ì›ë³¸ì—ì„œ ê²°ì¸¡ê°’ì´ ì•„ë‹Œ ê²ƒë“¤ë§Œ ì²˜ë¦¬
            mask = series.notna()
            if mask.any():
                # ì •ìˆ˜ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê°’ë“¤ ì²˜ë¦¬
                float_values = series[mask]
                int_mask = (float_values == float_values.astype(int))
                series_str.loc[mask & int_mask] = float_values[int_mask].astype(int).astype(str)
        
        df_processed[col] = series_str
    
    # 4. í…ìŠ¤íŠ¸ ê²°í•© (ë²¡í„°í™”)
    if include_column_names:
        # ì»¬ëŸ¼ëª…:ê°’ í˜•íƒœë¡œ ë³€í™˜ í›„ ê²°í•©
        text_columns = []
        for col in feature_columns:
            text_columns.append(col + column_value_separator + df_processed[col])
        
        # ëª¨ë“  ì»¬ëŸ¼ì„ separatorë¡œ ê²°í•©
        result = pd.Series([separator.join(row) for row in zip(*text_columns)])
    else:
        # ê°’ë§Œ ê²°í•©
        result = df_processed.apply(lambda row: separator.join(row), axis=1)
    
    return result.tolist()


# =============================================================================
# JSON ì§ë ¬í™” ìœ í‹¸ë¦¬í‹°
# =============================================================================

def convert_to_json_serializable(obj: Any) -> Any:
    """
    ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜
    
    Args:
        obj: ë³€í™˜í•  ê°ì²´
        
    Returns:
        JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°ì²´
    """
    # ê¸°ë³¸ íƒ€ì…ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    
    # numpy ì •ìˆ˜
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    
    # numpy ì‹¤ìˆ˜
    if isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    
    # numpy ë°°ì—´
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    
    # datetime
    if isinstance(obj, (dt.datetime, dt.date)):
        return obj.isoformat()
    
    # pandas Timestamp
    if hasattr(obj, 'isoformat') and callable(obj.isoformat):
        return obj.isoformat()
    
    # set, tuple
    if isinstance(obj, (set, tuple)):
        return list(convert_to_json_serializable(item) for item in obj)
    
    # list
    if isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    
    # dict
    if isinstance(obj, dict):
        return {
            str(key): convert_to_json_serializable(value) 
            for key, value in obj.items()
        }
    
    # ê¸°íƒ€ ê°ì²´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
    return str(obj)


def prepare_metadata_for_json(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ì €ì¥ìš©ìœ¼ë¡œ ì¤€ë¹„
    
    Args:
        metadata: ì›ë³¸ ë©”íƒ€ë°ì´í„°
        
    Returns:
        JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë©”íƒ€ë°ì´í„°
    """
    return convert_to_json_serializable(metadata)


def extract_stage_metadata_delta(
    current_metadata: Dict[str, Any], 
    previous_metadata: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    í˜„ì¬ ìŠ¤í…Œì´ì§€ì—ì„œ ì¶”ê°€ëœ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ
    
    Args:
        current_metadata: í˜„ì¬ ìŠ¤í…Œì´ì§€ì˜ ì „ì²´ ë©”íƒ€ë°ì´í„°
        previous_metadata: ì´ì „ ìŠ¤í…Œì´ì§€ì˜ ë©”íƒ€ë°ì´í„° (ì—†ìœ¼ë©´ None)
        
    Returns:
        í˜„ì¬ ìŠ¤í…Œì´ì§€ì—ì„œ ì¶”ê°€/ë³€ê²½ëœ ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    if previous_metadata is None:
        # Stage 0ì˜ ê²½ìš° ëª¨ë“  ë©”íƒ€ë°ì´í„°ê°€ ìƒˆë¡œìš´ ê²ƒ
        return current_metadata
    
    delta = {}
    
    for key, value in current_metadata.items():
        # ìƒˆë¡œìš´ í‚¤ì´ê±°ë‚˜ ê°’ì´ ë³€ê²½ëœ ê²½ìš°
        if key not in previous_metadata:
            delta[key] = value
        elif previous_metadata[key] != value:
            # ê°’ì´ ë³€ê²½ëœ ê²½ìš°
            if isinstance(value, dict) and isinstance(previous_metadata[key], dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ ë³€ê²½ì‚¬í•­ ì¶”ì¶œ
                nested_delta = {}
                for nested_key, nested_value in value.items():
                    if nested_key not in previous_metadata[key] or previous_metadata[key][nested_key] != nested_value:
                        nested_delta[nested_key] = nested_value
                if nested_delta:
                    delta[key] = nested_delta
            else:
                delta[key] = value
    
    return delta


def json_encoder(o: Any) -> Any:
    """
    numpy íƒ€ì…ê³¼ datetimeì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    (json.dumpì˜ default íŒŒë¼ë¯¸í„°ìš©)
    
    Args:
        o: ë³€í™˜í•  ê°ì²´
        
    Returns:
        JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…
    """
    # numpy ì •ìˆ˜ íƒ€ì…
    if isinstance(o, (np.integer, np.int32, np.int64)):
        return int(o)
    
    # numpy ì‹¤ìˆ˜ íƒ€ì…
    if isinstance(o, (np.floating, np.float32, np.float64)):
        return float(o)
    
    # numpy ë°°ì—´
    if isinstance(o, np.ndarray):
        return o.tolist()
    
    # datetime íƒ€ì…
    if isinstance(o, (dt.datetime, dt.date)):
        return o.isoformat()
    
    # pandas Timestamp
    if hasattr(o, 'isoformat'):
        return o.isoformat()
    
    # set, tupleì„ listë¡œ
    if isinstance(o, (set, tuple)):
        return list(o)
    
    raise TypeError(f"Object of type {type(o)} is not JSON serializable")


# =============================================================================
# ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# =============================================================================

def add_day_type_feature(df: pd.DataFrame, date_column: str = 'txn_date') -> pd.DataFrame:
    """
    ë‚ ì§œ íƒ€ì… í”¼ì²˜ ì¶”ê°€ (weekday/weekend/holiday)
    
    ìš°ì„ ìˆœìœ„: weekend > holiday > weekday
    
    Args:
        df: ì…ë ¥ DataFrame
        date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
        
    Returns:
        day_type ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    if date_column not in df.columns:
        logger.warning(f"ë‚ ì§œ ì»¬ëŸ¼ '{date_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return df
    
    df_copy = df.copy()
    
    # datetime ë³€í™˜
    df_copy['_dt'] = pd.to_datetime(df_copy[date_column])
    
    # í•œêµ­ ê³µíœ´ì¼ ê°ì²´ ìƒì„± (2020-2025)
    kr_holidays = holidays.KR(years=range(2020, 2026))
    
    # ìš”ì¼ ê³„ì‚° (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
    df_copy['_dow'] = df_copy['_dt'].dt.dayofweek
    
    # day_type ê²°ì •
    def get_day_type(row):
        if row['_dow'] >= 5:  # í† ìš”ì¼(5) ë˜ëŠ” ì¼ìš”ì¼(6)
            return 'weekend'
        elif row['_dt'].date() in kr_holidays:
            return 'holiday'
        else:
            return 'weekday'
    
    df_copy['day_type'] = df_copy.apply(get_day_type, axis=1)
    
    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    df_copy = df_copy.drop(columns=['_dt', '_dow'])
    
    # í†µê³„ ë¡œê¹…
    counts = df_copy['day_type'].value_counts()
    logger.info(f"âœ… day_type í”¼ì²˜ ì¶”ê°€: {counts.to_dict()}")
    
    return df_copy


# =============================================================================
# Z-Score ë³€í™˜ í•¨ìˆ˜
# =============================================================================

def remove_outliers_iqr(series: pd.Series, iqr_coefficient: float = 1.5, 
                       remove_lower: bool = True, remove_upper: bool = True,
                       lower_quantile: float = 0.25, upper_quantile: float = 0.75,
                       iqr_coefficient_lower: float = None, iqr_coefficient_upper: float = None) -> pd.Series:
    """
    IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±°
    
    Args:
        series: ì´ìƒì¹˜ë¥¼ ì œê±°í•  ì‹œë¦¬ì¦ˆ
        iqr_coefficient: IQR ê³„ìˆ˜ (ê¸°ë³¸ 1.5) - í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
        remove_lower: í•˜í•œ ì´ìƒì¹˜ ì œê±° ì—¬ë¶€
        remove_upper: ìƒí•œ ì´ìƒì¹˜ ì œê±° ì—¬ë¶€
        lower_quantile: í•˜ìœ„ ë¶„ìœ„ìˆ˜ (ê¸°ë³¸ 0.25)
        upper_quantile: ìƒìœ„ ë¶„ìœ„ìˆ˜ (ê¸°ë³¸ 0.75)
        iqr_coefficient_lower: í•˜í•œ IQR ê³„ìˆ˜ (Noneì¼ ê²½ìš° iqr_coefficient ì‚¬ìš©)
        iqr_coefficient_upper: ìƒí•œ IQR ê³„ìˆ˜ (Noneì¼ ê²½ìš° iqr_coefficient ì‚¬ìš©)
        
    Returns:
        ì´ìƒì¹˜ê°€ ì œê±°ëœ ì‹œë¦¬ì¦ˆ
    """
    Q1 = series.quantile(lower_quantile)
    Q3 = series.quantile(upper_quantile)
    IQR = Q3 - Q1
    
    # í•˜ìœ„ í˜¸í™˜ì„±: ê°œë³„ ê³„ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê³„ìˆ˜ ì‚¬ìš©
    if iqr_coefficient_lower is None:
        iqr_coefficient_lower = iqr_coefficient
    if iqr_coefficient_upper is None:
        iqr_coefficient_upper = iqr_coefficient
    
    result = series
    
    if remove_lower:
        lower_bound = Q1 - iqr_coefficient_lower * IQR
        result = result[result >= lower_bound]
    
    if remove_upper:
        upper_bound = Q3 + iqr_coefficient_upper * IQR
        result = result[result <= upper_bound]
    
    return result


def apply_z_score_transformation(df: pd.DataFrame, config: Dict[str, Any]) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    DataFrameì— Z-score ë³€í™˜ ì ìš©
    
    Args:
        df: ì…ë ¥ DataFrame
        config: Z-score ë³€í™˜ ì„¤ì •
        
    Returns:
        ë³€í™˜ëœ DataFrameê³¼ í†µê³„ ì •ë³´
    """
    stats = {}
    
    if not config.get("enabled", False):
        return df, stats
    
    columns_config = config.get("columns", [])
    
    for col_config in columns_config:
        column_name = col_config.get("column")
        if not column_name or column_name not in df.columns:
            logger.warning(f"Column {column_name} not found in DataFrame")
            continue
        
        # ì ˆëŒ€ê°’ ì ìš©
        abs_column = f"{column_name}_abs"
        df[abs_column] = df[column_name].abs()
        
        group_by = col_config.get("group_by")
        outlier_config = col_config.get("outlier_removal", {})
        z_range_config = col_config.get("z_score_range", {})
        transform_config = col_config.get("linear_transform", {})
        output_suffix = col_config.get("output_suffix", "_zscore")
        keep_original = col_config.get("keep_original", True)
        
        # Z-score í´ë¦¬í•‘ ë²”ìœ„ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
        clip_min = z_range_config.get("clip_min", -np.inf)
        clip_max = z_range_config.get("clip_max", np.inf)
        
        # ê·¸ë£¹ë³„ ë˜ëŠ” ì „ì²´ ì²˜ë¦¬
        if group_by and group_by in df.columns:
            # ê·¸ë£¹ë³„ ì²˜ë¦¬
            z_scores_list = []
            group_stats = []
            
            for group_name, group in df.groupby(group_by):
                abs_values = group[abs_column].dropna()
                
                if len(abs_values) > 0:
                    # ì´ìƒì¹˜ ì œê±°
                    cleaned_values = remove_outliers_iqr(
                        abs_values,
                        iqr_coefficient=outlier_config.get("iqr_coefficient", 1.5),
                        remove_lower=outlier_config.get("remove_lower", True),
                        remove_upper=outlier_config.get("remove_upper", True),
                        lower_quantile=outlier_config.get("lower_quantile", 0.25),
                        upper_quantile=outlier_config.get("upper_quantile", 0.75),
                        iqr_coefficient_lower=outlier_config.get("iqr_coefficient_lower"),
                        iqr_coefficient_upper=outlier_config.get("iqr_coefficient_upper")
                    )
                    
                    if len(cleaned_values) > 0:
                        # ë¡œê·¸ ë³€í™˜ì„ ë¨¼ì € ì ìš©
                        log_base = transform_config.get("log_base", "e")
                        log_epsilon = transform_config.get("log_epsilon", 0.001)
                        
                        # ë¡œê·¸ ë³€í™˜ (cleaned values)
                        cleaned_log = None
                        if log_base == "10":
                            cleaned_log = np.log10(cleaned_values + log_epsilon)
                        elif log_base == "2":
                            cleaned_log = np.log2(cleaned_values + log_epsilon)
                        else:  # ìì—°ë¡œê·¸
                            cleaned_log = np.log(cleaned_values + log_epsilon)
                        
                        # ë¡œê·¸ ë³€í™˜ëœ ê°’ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
                        mean_log = cleaned_log.mean()
                        std_log = cleaned_log.std()
                        
                        if std_log > 0:
                            # ì›ë³¸ ê°’ë“¤ë„ ë¡œê·¸ ë³€í™˜
                            abs_log = None
                            if log_base == "10":
                                abs_log = np.log10(abs_values + log_epsilon)
                            elif log_base == "2":
                                abs_log = np.log2(abs_values + log_epsilon)
                            else:
                                abs_log = np.log(abs_values + log_epsilon)
                            
                            # ë¡œê·¸ ë³€í™˜ëœ ê°’ì— ëŒ€í•´ z-score ê³„ì‚°
                            z_scores = (abs_log - mean_log) / std_log
                            z_scores.index = group.index
                            z_scores_list.append(z_scores)
                        
                        group_stats.append({
                            group_by: group_name,
                            "original_count": len(abs_values),
                            "cleaned_count": len(cleaned_values),
                            "mean": mean_log if 'mean_log' in locals() else np.nan,
                            "std": std_log if 'std_log' in locals() else np.nan,
                            "outliers_removed": len(abs_values) - len(cleaned_values),
                            "log_base": log_base,
                            "log_epsilon": log_epsilon
                        })
            
            # ëª¨ë“  z-scores í•©ì¹˜ê¸°
            if z_scores_list:
                all_z_scores = pd.concat(z_scores_list)
                all_z_scores = all_z_scores.reindex(df.index)
            else:
                all_z_scores = pd.Series(np.nan, index=df.index)
            
            # ê°„ì†Œí™”ëœ êµ¬ì¡°: co_vat_idë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ì¡°íšŒ ê°€ëŠ¥
            group_params = {}
            for stat in group_stats:
                co_vat_id = stat[group_by]
                group_params[str(co_vat_id)] = {
                    "mean": stat["mean"],
                    "std": stat["std"],
                    "log_base": stat["log_base"],
                    "log_epsilon": stat["log_epsilon"]
                }
            
            # ë³€í™˜ ì„¤ì • ì €ì¥
            transform_params = {
                "clip_min": clip_min,
                "clip_max": clip_max,
                "target_min": transform_config.get("target_min", 1),
                "target_max": transform_config.get("target_max", 100),
                "round": transform_config.get("round", True)
            }
            
            stats[column_name] = {
                "group_params": group_params,
                "transform_config": transform_params
            }
        else:
            # ì „ì²´ ë°ì´í„° ì²˜ë¦¬
            abs_values = df[abs_column].dropna()
            
            cleaned_values = remove_outliers_iqr(
                abs_values,
                iqr_coefficient=outlier_config.get("iqr_coefficient", 1.5),
                remove_lower=outlier_config.get("remove_lower", True),
                remove_upper=outlier_config.get("remove_upper", True),
                lower_quantile=outlier_config.get("lower_quantile", 0.25),
                upper_quantile=outlier_config.get("upper_quantile", 0.75),
                iqr_coefficient_lower=outlier_config.get("iqr_coefficient_lower"),
                iqr_coefficient_upper=outlier_config.get("iqr_coefficient_upper")
            )
            
            if len(cleaned_values) > 0:
                # ë¡œê·¸ ë³€í™˜ì„ ë¨¼ì € ì ìš©
                log_base = transform_config.get("log_base", "e")
                log_epsilon = transform_config.get("log_epsilon", 0.001)
                
                # ë¡œê·¸ ë³€í™˜ (cleaned values)
                if log_base == "10":
                    cleaned_log = np.log10(cleaned_values + log_epsilon)
                elif log_base == "2":
                    cleaned_log = np.log2(cleaned_values + log_epsilon)
                else:
                    cleaned_log = np.log(cleaned_values + log_epsilon)
                
                mean_log = cleaned_log.mean()
                std_log = cleaned_log.std()
                
                if std_log > 0:
                    # ì „ì²´ ë°ì´í„°ë„ ë¡œê·¸ ë³€í™˜
                    if log_base == "10":
                        df_log = np.log10(df[abs_column] + log_epsilon)
                    elif log_base == "2":
                        df_log = np.log2(df[abs_column] + log_epsilon)
                    else:
                        df_log = np.log(df[abs_column] + log_epsilon)
                    
                    all_z_scores = (df_log - mean_log) / std_log
                else:
                    all_z_scores = pd.Series(np.nan, index=df.index)
            else:
                all_z_scores = pd.Series(np.nan, index=df.index)
            
            # ê°„ì†Œí™”ëœ êµ¬ì¡°: ì „ì²´ ë°ì´í„°ì˜ ê²½ìš°
            transform_params = {
                "clip_min": clip_min,
                "clip_max": clip_max,
                "target_min": transform_config.get("target_min", 1),
                "target_max": transform_config.get("target_max", 100),
                "round": transform_config.get("round", True)
            }
            
            stats[column_name] = {
                "global_params": {
                    "mean": mean_log if 'mean_log' in locals() else np.nan,
                    "std": std_log if 'std_log' in locals() else np.nan,
                    "log_base": log_base if 'log_base' in locals() else "e",
                    "log_epsilon": log_epsilon if 'log_epsilon' in locals() else 0.001
                },
                "transform_config": transform_params
            }
        
        # Z-score í´ë¦¬í•‘
        z_scores_clipped = all_z_scores.clip(lower=clip_min, upper=clip_max)
        
        # ì„ í˜• ë³€í™˜ë§Œ ì ìš© (ë¡œê·¸ëŠ” ì´ë¯¸ z-score ê³„ì‚° ì „ì— ì ìš©ë¨)
        if transform_config.get("enabled", False):
            target_min = transform_config.get("target_min", 1)
            target_max = transform_config.get("target_max", 100)
            
            # ì„ í˜• ë³€í™˜
            range_original = clip_max - clip_min
            range_target = target_max - target_min
            
            transformed_values = target_min + (z_scores_clipped - clip_min) * range_target / range_original
            
            if transform_config.get("round", True):
                # NaN ê°’ì„ ì²˜ë¦¬í•˜ë©´ì„œ ì •ìˆ˜ë¡œ ë³€í™˜
                mask_finite = np.isfinite(transformed_values)
                transformed_values[mask_finite] = np.round(transformed_values[mask_finite]).astype(int)
                transformed_values[mask_finite] = transformed_values[mask_finite].clip(target_min, target_max)
            
            # ìƒˆ ì»¬ëŸ¼ì— ì €ì¥
            new_column_name = f"{column_name}{output_suffix}"
            df[new_column_name] = transformed_values
            
            # ì¶”ë¡ ì— í•„ìš”ì—†ëŠ” í†µê³„ëŠ” ì œê±°
        else:
            # ë³€í™˜ ì—†ì´ z-scoreë§Œ ì €ì¥
            new_column_name = f"{column_name}{output_suffix}"
            df[new_column_name] = z_scores_clipped
        
        # ì„ì‹œ ì ˆëŒ€ê°’ ì»¬ëŸ¼ ì œê±°
        df.drop(columns=[abs_column], inplace=True)
        
        # ì›ë³¸ ì»¬ëŸ¼ ì œê±° ì˜µì…˜
        if not keep_original and column_name in df.columns:
            df.drop(columns=[column_name], inplace=True)
        
        logger.info(f"âœ… Z-score ë³€í™˜ ì™„ë£Œ: {column_name} â†’ {new_column_name}")
    
    return df, stats