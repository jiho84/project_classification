#!/usr/bin/env python3
"""
MLOps Pipeline Nodes - Kedro Container ë°©ì‹
ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì™„ì „íˆ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬
"""

import pandas as pd
import numpy as np
import json
import os
import time
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìœ„í•´ ì¶”ê°€
from typing import Dict, Any, Tuple, List, Optional
import logging
from datetime import datetime
from sklearn.model_selection import train_test_split
from omegaconf import DictConfig

# í†µí•©ëœ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ import
from .util import (
    # Config utilities
    get_stage_config,
    
    # Data utilities
    validate_required_columns,
    get_column_statistics,
    apply_row_filters,
    calculate_class_distribution,
    
    # Label mapping utilities
    create_label_mapping,
    
    # Feature engineering utilities
    apply_robust_standardization,
    apply_z_score_transformation,
    add_day_type_feature,
    
    # Text encoding utilities
    prepare_split_data,
    perform_tokenizer_analysis,
)
from .util import prepare_metadata_for_json
from .util import optimize_dataframe_dtypes, reduce_metadata_size, get_dataframe_memory_info

logger = logging.getLogger(__name__)


# =============================================================================
# ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
# =============================================================================

def cleanup_memory(stage_name: str = "") -> None:
    """
    ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
    
    Args:
        stage_name: ìŠ¤í…Œì´ì§€ ì´ë¦„ (ë¡œê¹…ìš©)
    """
    before_gc = gc.collect()
    gc.collect()
    after_gc = gc.collect()
    
    if stage_name:
        logger.debug(f"ğŸ§¹ {stage_name} ë©”ëª¨ë¦¬ ì •ë¦¬: {before_gc} ê°ì²´ í•´ì œ")
    
    return


# =============================================================================
# Stage 0: íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ì˜
# =============================================================================

def stage_0_column_standardization(
    raw_data: pd.DataFrame, 
    parameters: DictConfig
) -> Tuple[pd.DataFrame, Dict[str, Any], float, int]:
    """
    Stage 0: ì»¬ëŸ¼ëª… í‘œì¤€í™” ë° íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ì˜
    
    Args:
        raw_data: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        df: í‘œì¤€í™”ëœ DataFrame
        metadata: ì´ˆê¸° ë©”íƒ€ë°ì´í„°
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì¶œë ¥ ë°ì´í„° í–‰ ìˆ˜
    """
    start_time = time.time()
    
    logger.info("="*60)
    logger.info("ğŸš‰ Stage 0: ì»¬ëŸ¼ëª… í‘œì¤€í™” ë° íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ì˜")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_0")
    stage_config = config["stage_config"]
    
    # ì›ë³¸ ì»¬ëŸ¼ ì €ì¥
    original_columns = list(raw_data.columns)
    df = raw_data
    
    # ì»¬ëŸ¼ëª… í‘œì¤€í™”
    rename_dict = {}
    if stage_config.get("column_name_standardization", {}).get("enabled", True):
        mapping_file = stage_config.get("column_name_standardization", {}).get("mapping_file")
        if mapping_file and os.path.exists(mapping_file):
            logger.info(f"ğŸ“‚ ì»¬ëŸ¼ ë§¤í•‘ íŒŒì¼ ë¡œë“œ: {mapping_file}")
            with open(mapping_file, 'r', encoding='utf-8') as f:
                columns_mapping = json.load(f)
            
            for col in original_columns:
                if col in columns_mapping:
                    rename_dict[col] = columns_mapping[col]
                    logger.info(f"  ğŸ“ {col} â†’ {columns_mapping[col]}")
            
            if rename_dict:
                df = df.rename(columns=rename_dict)
                logger.info(f"âœ… {len(rename_dict)}ê°œ ì»¬ëŸ¼ëª… í‘œì¤€í™” ì™„ë£Œ")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ì˜
    target_column = stage_config.get("target_column", "")
    if not target_column:
        raise ValueError("íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if target_column not in df.columns:
        raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
    
    logger.info(f"ğŸ¯ íƒ€ê²Ÿ ì»¬ëŸ¼: {target_column}")
    
    # ë©”ëª¨ë¦¬ ìµœì í™”
    logger.info("ğŸ”§ DataFrame ë©”ëª¨ë¦¬ ìµœì í™” ì¤‘...")
    df = optimize_dataframe_dtypes(df, categorical_threshold=0.3, exclude_columns=[target_column])
    
    # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
    metadata = {
        "column_mapping": {
            "original_columns": original_columns,  # Stage 9ì—ì„œ í•„ìš”
            "original_columns_count": len(original_columns),
            "standardized_columns": list(df.columns),  # í‘œì¤€í™”ëœ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            "standardized_columns_count": len(df.columns),
            "rename_dict": rename_dict,
            "columns_renamed": len(rename_dict)
        },
        "target_column": target_column,
        "num_features": len([col for col in df.columns if col != target_column]),
        "pipeline_start_time": datetime.now().isoformat(),
        "memory_info": get_dataframe_memory_info(df)
    }
    
    logger.info(f"âœ… Stage 0 ì™„ë£Œ: {len(df):,} rows, {len(df.columns)} columns")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ê¸°
    processing_time = time.time() - start_time
    output_count = len(df)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 0")
    
    return df, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 1: ë°ì´í„° í•„í„°ë§
# =============================================================================

def stage_1_data_validation_filtering(
    df: pd.DataFrame,
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[pd.DataFrame, Dict[str, Any], float, int]:
    """
    Stage 1: ìŠ¤í‚¤ë§ˆ/ë¬´ê²°ì„± ê²€ì¦ ë° í•„í„°ë§
    
    Args:
        df: DataFrame
        metadata: Stage 0ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        df: í•„í„°ë§ëœ DataFrame
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì¶œë ¥ ë°ì´í„° í–‰ ìˆ˜
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 1: ë°ì´í„° ê²€ì¦ ë° í•„í„°ë§")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_1")
    stage_config = config["stage_config"]
    validation_config = stage_config.get("validation", {})
    filtering_config = stage_config.get("filtering", {})
    
    input_shape = df.shape
    
    # ê²€ì¦ ìˆ˜í–‰
    validation_results = {}
    if validation_config.get("enable_validation", True):
        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
        required_columns = validation_config.get("validation_rules", {}).get("required_columns", [])
        missing_columns = validate_required_columns(df, required_columns)
        
        if missing_columns:
            logger.warning(f"âš ï¸ ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {missing_columns}")
        
        # ì»¬ëŸ¼ë³„ í†µê³„ ìˆ˜ì§‘
        column_statistics = get_column_statistics(df)
        
        # íŠ¹ì • ì»¬ëŸ¼ null ë¹„ìœ¨ ì²´í¬
        cust_type_code_null_ratio = None
        if "cust_type_code" in df.columns:
            cust_type_code_null_ratio = column_statistics.get("cust_type_code", {}).get("null_ratio", 0)
            logger.info(f"ğŸ“Š cust_type_code null ë¹„ìœ¨: {cust_type_code_null_ratio:.2%}")
        
        validation_results = {
            "missing_required_columns": missing_columns,
            "cust_type_code_null_ratio": cust_type_code_null_ratio
        }
    
    # í•„í„°ë§ ì ìš©
    filtering_stats = {}
    if filtering_config.get("enabled", True):
        df, filtering_stats = apply_row_filters(df, filtering_config)
        logger.info(f"ğŸ”½ í•„í„°ë§ í›„: {len(df):,} rows (ì œê±°: {input_shape[0] - len(df):,})")
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì )
    metadata = {
        **metadata,  # ì´ì „ ë©”íƒ€ë°ì´í„° ìœ ì§€
        "column_statistics": column_statistics if validation_config.get("enable_validation", True) else {},
        "validation_results": validation_results,
        "filtering_stats": filtering_stats
    }
    
    logger.info(f"âœ… Stage 1 ì™„ë£Œ: {len(df):,} rows")
    
    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    processing_time = time.time() - start_time
    output_count = len(df)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 1")
    
    return df, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 2: ë°ì´í„° ê²€ì¦
# =============================================================================

def stage_2_data_quality_check(
    df: pd.DataFrame,
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[pd.DataFrame, Dict[str, Any], float, int, int, float]:
    """
    Stage 2: ë°ì´í„° ì •ì œ ë° í•„í„°ë§
    
    Args:
        df: DataFrame
        metadata: Stage 1ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        df: ì •ì œëœ DataFrame
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì¶œë ¥ ë°ì´í„° í–‰ ìˆ˜
        duplicate_rows: ì¤‘ë³µ í–‰ ìˆ˜
        class_imbalance_ratio: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨
    """
    start_time = time.time()
    initial_rows = len(df)
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 2: ë°ì´í„° ì •ì œ ë° í•„í„°ë§")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_2")
    stage_config = config["stage_config"]
    
    target_column = metadata["target_column"]
    
    # 1. ì»¬ëŸ¼ ì œì™¸
    exclude_columns = stage_config.get("exclude_columns", [])
    if exclude_columns:
        df = df.drop(columns=[col for col in exclude_columns if col in df.columns])
        logger.info(f"ğŸ”½ ì œì™¸ëœ ì»¬ëŸ¼: {exclude_columns}")
    
    # 2. í–‰ í•„í„°ë§
    row_filters = stage_config.get("row_filters", {})
    filter_stats = {}
    for col, filter_config in row_filters.items():
        if col in df.columns:
            before_count = len(df)
            operator = filter_config.get("operator", "equals")
            values = filter_config.get("values", [])
            
            if operator == "between" and len(values) == 2:
                df = df[(df[col] >= values[0]) & (df[col] <= values[1])]
            elif operator == "in":
                df = df[df[col].isin(values)]
            elif operator == "equals" and values:
                df = df[df[col] == values[0]]
            
            after_count = len(df)
            filter_stats[col] = {
                "removed": before_count - after_count,
                "filter_type": operator,
                "values": values
            }
            logger.info(f"ğŸ”½ {col} í•„í„°ë§: {before_count - after_count}ê°œ í–‰ ì œê±°")
    
    # 3. NULL ê°’ ì²˜ë¦¬
    drop_null_columns = stage_config.get("drop_null_columns", [])
    null_stats = {}
    for col in drop_null_columns:
        if col in df.columns:
            before_count = len(df)
            df = df.dropna(subset=[col])
            null_stats[col] = before_count - len(df)
            logger.info(f"ğŸ”½ {col} NULL ì œê±°: {null_stats[col]}ê°œ í–‰")
    
    # 4. ì¤‘ë³µ ì œê±°
    duplicate_count = 0
    dedup_config = stage_config.get("deduplication", {})
    if dedup_config.get("enabled", False):
        subset_columns = dedup_config.get("subset_columns", None)
        keep = dedup_config.get("keep", "first")
        before_count = len(df)
        df = df.drop_duplicates(subset=subset_columns, keep=keep)
        duplicate_count = before_count - len(df)
        logger.info(f"ğŸ”½ ì¤‘ë³µ ì œê±°: {duplicate_count}ê°œ í–‰")
    
    # 5. í´ë˜ìŠ¤ í•„í„°ë§
    class_filter_config = stage_config.get("class_filter", {})
    class_filter_stats = {}
    if class_filter_config:
        min_samples = class_filter_config.get("min_samples", 2)
        filter_column = class_filter_config.get("target_column", target_column)
        
        if filter_column and filter_column in df.columns:
            class_counts = df[filter_column].value_counts()
            valid_classes = class_counts[class_counts >= min_samples].index
            before_count = len(df)
            df = df[df[filter_column].isin(valid_classes)]
            
            class_filter_stats = {
                "removed_classes": len(class_counts) - len(valid_classes),
                "removed_rows": before_count - len(df),
                "min_samples": min_samples
            }
            logger.info(f"ğŸ”½ í´ë˜ìŠ¤ í•„í„°ë§: {class_filter_stats['removed_classes']}ê°œ í´ë˜ìŠ¤, "
                       f"{class_filter_stats['removed_rows']}ê°œ í–‰ ì œê±°")
    
    # 6. í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
    class_distribution = None
    if target_column and target_column in df.columns:
        class_distribution = calculate_class_distribution(df, target_column)
        logger.info(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: {class_distribution['num_classes']}ê°œ í´ë˜ìŠ¤")
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    metadata = {
        **metadata,
        "stage_2_stats": {
            "initial_rows": initial_rows,
            "final_rows": len(df),
            "total_removed": initial_rows - len(df),
            "exclude_columns": exclude_columns,
            "row_filter_stats": filter_stats,
            "null_removal_stats": null_stats,
            "deduplication_stats": {
                "enabled": dedup_config.get("enabled", False),
                "duplicates_removed": duplicate_count
            },
            "class_filter_stats": class_filter_stats
        },
        "class_distribution": class_distribution
    }
    
    logger.info(f"âœ… Stage 2 ì™„ë£Œ: {initial_rows:,} â†’ {len(df):,} í–‰ (ì œê±°: {initial_rows - len(df):,})")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë©”íŠ¸ë¦­
    processing_time = time.time() - start_time
    output_count = len(df)
    duplicate_rows_metric = duplicate_count
    class_imbalance_metric = class_distribution.get('class_imbalance_ratio', 1.0) if class_distribution else 1.0
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 2")
    
    return df, prepare_metadata_for_json(metadata), processing_time, output_count, duplicate_rows_metric, class_imbalance_metric


# =============================================================================
# Stage 3: ë°ì´í„° ì •ì œ
# =============================================================================

def stage_3_data_cleaning(
    df: pd.DataFrame,
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[pd.DataFrame, Dict[str, Any], float, int]:
    """
    Stage 3: ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬
    
    Args:
        df: DataFrame
        metadata: Stage 2ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        df: ì •ì œëœ DataFrame
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì¶œë ¥ ë°ì´í„° í–‰ ìˆ˜
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 3: ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_3")
    stage_config = config["stage_config"]
    
    target_column = metadata["target_column"]
    
    # Stage 3ì—ì„œëŠ” ê²°ì¸¡ê°’ ì²˜ë¦¬ ì œê±° - Stage 8ì—ì„œë§Œ ì²˜ë¦¬
    cleaning_stats = {}
    logger.info("â„¹ï¸ ê²°ì¸¡ê°’ ì²˜ë¦¬ëŠ” Stage 8ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
    
    # ë¼ë²¨ ë§¤í•‘ ìƒì„± ë˜ëŠ” ë¡œë“œ
    label_mapping = {}
    if target_column and target_column in df.columns:
        label_mapping_config = stage_config.get("label_mapping", {})
        # parameters.ymlì—ì„œ existing_path ê°€ì ¸ì˜¤ê¸°
        existing_path = label_mapping_config.get("existing_path", "metadata/mapping/label_mapping.json")
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        project_path = "/home/user/projects/my_project/pj_mlflow_ds3"
        mapping_path = os.path.join(project_path, existing_path)
        
        # create_label_mapping í•¨ìˆ˜ í˜¸ì¶œ (auto ëª¨ë“œë¡œ)
        label_mapping = create_label_mapping(
            data=df,
            target_column=target_column,
            total_num_classes=label_mapping_config.get("total_num_classes", 300),
            dummy_prefix=label_mapping_config.get("dummy_class_prefix", "DUMMY_"),
            mode="auto",  # auto ëª¨ë“œëŠ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ ìë™ ê²°ì •
            existing_path=mapping_path
        )
        
        # íŒŒì¼ ì €ì¥ (í•­ìƒ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€)
        os.makedirs(os.path.dirname(mapping_path), exist_ok=True)
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(label_mapping, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ë¼ë²¨ ë§¤í•‘ ì €ì¥ ì™„ë£Œ: {mapping_path}")
        
        # ì‹ ê·œ í´ë˜ìŠ¤ ì •ë³´ ì¶œë ¥
        if label_mapping.get("new_classes_added"):
            logger.info(f"ğŸ†• ì¶”ê°€ëœ ì‹ ê·œ í´ë˜ìŠ¤: {label_mapping['new_classes_added']}")
        
        logger.info(f"ğŸ·ï¸ ë¼ë²¨ ë§¤í•‘ ìƒíƒœ: {label_mapping['num_classes']}ê°œ í´ë˜ìŠ¤ "
                   f"(ì‹¤ì œ: {label_mapping['num_base_classes']}, ë”ë¯¸: {label_mapping['num_dummy_classes']})")
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì )
    metadata = {
        **metadata,
        "cleaning_stats": cleaning_stats,
        "label_mapping": label_mapping,  # ì¤‘ìš”í•œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        "label_mapping_summary": {
            "num_classes": label_mapping.get('num_classes', 0),
            "num_base_classes": label_mapping.get('num_base_classes', 0),
            "num_dummy_classes": label_mapping.get('num_dummy_classes', 0)
        }
    }
    
    logger.info(f"âœ… Stage 3 ì™„ë£Œ: {len(df):,} rows")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ê¸°
    processing_time = time.time() - start_time
    output_count = len(df)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 3")
    
    return df, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 4: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# =============================================================================

def stage_4_feature_engineering(
    df: pd.DataFrame,
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[pd.DataFrame, Dict[str, Any], float, int, float]:
    """
    Stage 4: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    
    Args:
        df: DataFrame
        metadata: Stage 3ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        df: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ëœ DataFrame
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì¶œë ¥ ë°ì´í„° í–‰ ìˆ˜
        outlier_removal_ratio: ì´ìƒì¹˜ ì œê±° ë¹„ìœ¨
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 4: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_4")
    stage_config = config["stage_config"]
    
    company_scaling_stats = {}
    feature_engineering_summary = {}
    outlier_stats = {}
    
    # Robust standardization ì ìš©
    if stage_config.get("robust_standardization", {}).get("enable", False):
        robust_config = stage_config["robust_standardization"]
        
        df, company_stats, feature_summary, outlier_stats = apply_robust_standardization(
            data=df,
            config=robust_config
        )
        
        # íšŒì‚¬ë³„ ìŠ¤ì¼€ì¼ë§ í†µê³„
        if company_stats:
            company_scaling_stats = {
                "stats": company_stats,
                "global_params": {
                    "outlier_method": robust_config.get("outlier_bounds", {}).get("method", "iqr"),
                    "scaling_range": robust_config.get("scaling_range", [1, 100])
                }
            }
        
        feature_engineering_summary = feature_summary
        
        logger.info(f"ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {feature_summary.get('features_added', 0)}ê°œ í”¼ì²˜ ì¶”ê°€")
    
    # Z-score ë³€í™˜ ì ìš©
    z_score_stats = {}
    z_score_config = stage_config.get("z_score_transformation", {})
    if z_score_config.get("enabled", False):
        logger.info("ğŸ“Š Z-score ë³€í™˜ ì‹œì‘...")
        df, z_score_stats = apply_z_score_transformation(df, z_score_config)
        
        # Z-score í†µê³„ ë¡œê¹…
        for col_name, col_stats in z_score_stats.items():
            if "group_stats" in col_stats:
                logger.info(f"  - {col_name}: {col_stats['total_groups']}ê°œ ê·¸ë£¹ ì²˜ë¦¬")
            else:
                logger.info(f"  - {col_name}: ì „ì²´ ë°ì´í„° ì²˜ë¦¬")
            
            if "transformed_mean" in col_stats:
                logger.info(f"    ë³€í™˜ í›„ í‰ê· : {col_stats['transformed_mean']:.2f}, "
                          f"í‘œì¤€í¸ì°¨: {col_stats['transformed_std']:.2f}")
                logger.info(f"    í´ë¦¬í•‘: í•˜í•œ {col_stats['clipped_at_min']}ê°œ, "
                          f"ìƒí•œ {col_stats['clipped_at_max']}ê°œ")
    
    # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ìƒì„±
    date_features_stats = {}
    date_features_config = stage_config.get("date_features", {})
    if date_features_config.get("enabled", False):
        logger.info("ğŸ“… ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ìƒì„± ì‹œì‘...")
        features = date_features_config.get("features", [])
        
        if "day_type" in features:
            logger.info("  - day_type í”¼ì²˜ ì¶”ê°€ ì¤‘...")
            df = add_day_type_feature(df, date_column='txn_date')
            date_features_stats["day_type"] = "added"
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì )
    metadata = {
        **metadata,
        "feature_engineering_summary": feature_engineering_summary,
        "outlier_stats": outlier_stats,
        "company_scaling_stats": company_scaling_stats,
        "z_score_stats": z_score_stats,
        "date_features_stats": date_features_stats
    }
    
    logger.info(f"âœ… Stage 4 ì™„ë£Œ: {len(df.columns)} features")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ê¸°
    processing_time = time.time() - start_time
    output_count = len(df)
    outlier_ratio = outlier_stats.get('outlier_ratio', 0.0)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 4")
    
    return df, prepare_metadata_for_json(metadata), processing_time, output_count, outlier_ratio


# =============================================================================
# Stage 5: í”¼ì²˜ ì„ íƒ
# =============================================================================

def stage_5_feature_selection(
    df: pd.DataFrame,
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[pd.DataFrame, Dict[str, Any], float, int]:
    """
    Stage 5: í”¼ì²˜ ì„ íƒ ë° ë¼ë²¨ ë§¤í•‘ ì—…ë°ì´íŠ¸
    
    Args:
        df: DataFrame
        metadata: Stage 4ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        df: í”¼ì²˜ ì„ íƒëœ DataFrame
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì¶œë ¥ ë°ì´í„° í¬ê¸°
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 5: í”¼ì²˜ ì„ íƒ")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_5")
    stage_config = config["stage_config"]
    
    target_column = metadata["target_column"]
    
    # í”¼ì²˜ ì„ íƒ - YAMLì˜ feature_columns ì‚¬ìš©
    feature_columns = stage_config.get("feature_columns", [])
    
    if not feature_columns:
        # feature_columnsê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©
        logger.warning("âš ï¸ feature_columnsê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        feature_columns = [col for col in df.columns if col != target_column]
    else:
        # ì§€ì •ëœ feature_columns ì¤‘ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
        available_features = [col for col in feature_columns if col in df.columns]
        missing_features = [col for col in feature_columns if col not in df.columns]
        
        if missing_features:
            logger.warning(f"âš ï¸ ë‹¤ìŒ í”¼ì²˜ë“¤ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {missing_features}")
        
        feature_columns = available_features
        logger.info(f"ğŸ“‹ YAMLì—ì„œ ì§€ì •ëœ {len(feature_columns)}ê°œ í”¼ì²˜ ì‚¬ìš©")
    
    # ì„ íƒëœ í”¼ì²˜ë¡œ ë°ì´í„° í•„í„°ë§ ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
    if feature_columns:
        # feature_columnsì˜ ìˆœì„œëŒ€ë¡œ ì»¬ëŸ¼ ì •ë ¬ + íƒ€ê²Ÿ ì»¬ëŸ¼ì€ ë§ˆì§€ë§‰ì—
        selected_columns = feature_columns + ([target_column] if target_column in df.columns else [])
        df = df[selected_columns]
        logger.info(f"âœ… {len(feature_columns)}ê°œ í”¼ì²˜ ì„ íƒ ë° ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì„ íƒëœ í”¼ì²˜: {feature_columns[:5]}{'...' if len(feature_columns) > 5 else ''}")
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì )
    metadata = {
        **metadata,
        "feature_columns": feature_columns,  # ì¤‘ìš”: ì„ íƒëœ í”¼ì²˜ ëª©ë¡
        "feature_selection_info": {
            "method": "yaml_config",
            "num_features_selected": len(feature_columns),
            "feature_names": feature_columns
        }
    }
    
    logger.info(f"âœ… Stage 5 ì™„ë£Œ: {len(feature_columns)} features selected")
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    processing_time = time.time() - start_time
    output_count = len(df)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 5")
    
    return df, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 6: ë°ì´í„° ë¶„í•  (ì¤‘ìš” ë¶„ê¸°ì !)
# =============================================================================

def stage_6_data_splitting(
    df: pd.DataFrame,
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any], float, int]:
    """
    Stage 6: ë°ì´í„°ë¥¼ train/validation/testë¡œ ë¶„í• 
    ì´ ì§€ì ë¶€í„° ë°ì´í„°ê°€ 3ê°œë¡œ ë‚˜ë‰©ë‹ˆë‹¤!
    
    Args:
        df: DataFrame
        metadata: Stage 5ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        data: {"train": DataFrame, "val": DataFrame, "test": DataFrame}
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì´ ë°ì´í„° í¬ê¸°
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 6: ë°ì´í„° ë¶„í•  (ì¤‘ìš” ë¶„ê¸°ì !)")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_6")
    stage_config = config["stage_config"]
    splitting_config = stage_config.get("splitting", {})
    
    target_column = metadata["target_column"]
    
    # ë¶„í•  ë¹„ìœ¨ - parameters.ymlì˜ ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
    train_ratio = splitting_config.get("train_ratio", 0.7)
    val_ratio = splitting_config.get("validation_ratio", 0.15)
    test_ratio = splitting_config.get("test_ratio", 0.15)
    
    # ì„¤ì •ëœ ë¹„ìœ¨ ë¡œê·¸ ì¶œë ¥
    logger.info(f"ğŸ“Š ì„¤ì •ëœ ë¶„í•  ë¹„ìœ¨:")
    logger.info(f"   - Train: {train_ratio:.1%}")
    logger.info(f"   - Validation: {val_ratio:.1%}")
    logger.info(f"   - Test: {test_ratio:.1%}")
    
    # stratify ì„¤ì • - ì•ˆì „í•œ ì²˜ë¦¬
    def safe_stratify(data, target_col, min_samples_per_class=2):
        """Stratifyê°€ ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ê³  stratify ì»¬ëŸ¼ ë°˜í™˜"""
        # splitting_configì—ì„œ stratify_column í™•ì¸
        stratify_column = splitting_config.get("stratify_column", target_col)
        if not stratify_column or stratify_column not in data.columns:
            return None
        
        # ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        class_counts = data[stratify_column].value_counts()
        min_class_count = class_counts.min()
        
        # splitting_configì—ì„œ min_samples_per_class ì„¤ì • ì‚¬ìš©
        min_required = splitting_config.get("min_samples_per_class", min_samples_per_class)
        if min_class_count < min_required:
            logger.warning(f"âš ï¸ ìµœì†Œ í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜ê°€ {min_class_count}ê°œë¡œ stratify ë¶ˆê°€. ë¬´ì‘ìœ„ ë¶„í•  ì‚¬ìš©")
            return None
        
        return data[stratify_column]
    
    # 1ì°¨ ë¶„í• : train + val vs test
    stratify_col = safe_stratify(df, target_column)
    train_val_df, test_df = train_test_split(
        df,
        test_size=test_ratio,
        random_state=splitting_config.get("random_state", 42),
        stratify=stratify_col
    )
    
    # 2ì°¨ ë¶„í• : train vs val
    val_size = val_ratio / (train_ratio + val_ratio)
    stratify_col = safe_stratify(train_val_df, target_column)
    
    train_df, val_df = train_test_split(
        train_val_df,
        test_size=val_size,
        random_state=splitting_config.get("random_state", 42),
        stratify=stratify_col
    )
    
    # ë¶„í•  í†µê³„
    split_stats = {
        "train": {"rows": len(train_df), "ratio": len(train_df) / len(df)},
        "validation": {"rows": len(val_df), "ratio": len(val_df) / len(df)},
        "test": {"rows": len(test_df), "ratio": len(test_df) / len(df)},
        "total": len(df)
    }
    
    logger.info(f"ğŸ“Š ë¶„í•  ì™„ë£Œ:")
    logger.info(f"   - Train: {split_stats['train']['rows']:,} ({split_stats['train']['ratio']:.1%})")
    logger.info(f"   - Val: {split_stats['validation']['rows']:,} ({split_stats['validation']['ratio']:.1%})")
    logger.info(f"   - Test: {split_stats['test']['rows']:,} ({split_stats['test']['ratio']:.1%})")
    
    # ë°ì´í„° êµ¬ì¡° ë³€ê²½: {"full": df} â†’ {"train": df, "val": df, "test": df}
    data = {
        "train": train_df,
        "val": val_df,
        "test": test_df
    }
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì )
    metadata = {
        **metadata,
        "split_stats": split_stats,
        "split_ratios": {
            "train": train_ratio,
            "validation": val_ratio,
            "test": test_ratio
        }
    }
    
    logger.info(f"âœ… Stage 6 ì™„ë£Œ: ë°ì´í„°ê°€ 3ê°œë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ê¸°
    processing_time = time.time() - start_time
    output_count = len(train_df) + len(val_df) + len(test_df)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 6")
    
    return data, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 7: ë°ì´í„° ìŠ¤ì¼€ì¼ë§
# =============================================================================

def stage_7_data_scaling(
    data: Dict[str, pd.DataFrame],
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any], float, int]:
    """
    Stage 7: ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ì„ íƒì )
    
    Args:
        data: {"train": DataFrame, "val": DataFrame, "test": DataFrame}
        metadata: Stage 6ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        data: {"train": ìŠ¤ì¼€ì¼ë§ëœ DataFrame, "val": ..., "test": ...}
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì´ ë°ì´í„° í¬ê¸°
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 7: ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_7")
    stage_config = config["stage_config"]
    
    # ë°ì´í„° íƒ€ì… í™•ì¸ (ë””ë²„ê¹…)
    logger.info(f"Data type: {type(data)}")
    if isinstance(data, dict):
        for key, value in data.items():
            logger.info(f"  {key}: {type(value)}")
    
    # PartitionedDataset ì²˜ë¦¬ - callableì´ë©´ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    if isinstance(data, dict) and all(callable(v) for v in data.values()):
        logger.info("PartitionedDataset detected - loading actual data...")
        loaded_data = {}
        for key, loader in data.items():
            loaded_data[key] = loader()
            logger.info(f"  Loaded {key}: {type(loaded_data[key])}, shape: {loaded_data[key].shape}")
        data = loaded_data
    
    # ë°ì´í„° ì§ì ‘ ì‚¬ìš© (ë³µì‚¬í•˜ì§€ ì•ŠìŒ)
    scaled_data = data
    
    # ìŠ¤ì¼€ì¼ë§ ì„¤ì •
    scaling_enabled = stage_config.get("scaling_enabled", False)
    scaling_info = {"enabled": False, "features_scaled": []}
    
    if scaling_enabled:
        logger.info("ğŸ“ ìŠ¤ì¼€ì¼ë§ í™œì„±í™”ë¨")
        scaling_features = stage_config.get("scaling_features", [])
        
        if scaling_features:
            logger.info(f"   {len(scaling_features)}ê°œ í”¼ì²˜ì— ìŠ¤ì¼€ì¼ë§ ì ìš©")
            # ì—¬ê¸°ì„œ ì‹¤ì œ ìŠ¤ì¼€ì¼ë§ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            # ì˜ˆ: StandardScaler, MinMaxScaler ë“±
            
            scaling_info = {
                "enabled": True,
                "features_scaled": scaling_features,
                "scaling_method": stage_config.get("scaling_method", "standard")
            }
    else:
        logger.info("â­ï¸ ìŠ¤ì¼€ì¼ë§ ê±´ë„ˆëœ€")
    
    # ë°ì´í„° ì»¨í…Œì´ë„ˆ (3ê°œ ìœ ì§€)
    data = scaled_data
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì )
    metadata = {
        **metadata,
        "scaling_info": scaling_info
    }
    
    logger.info(f"âœ… Stage 7 ì™„ë£Œ")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ê¸°
    processing_time = time.time() - start_time
    output_count = len(scaled_data["train"]) + len(scaled_data["val"]) + len(scaled_data["test"])
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 7")
    
    return scaled_data, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 8: ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ (í…ìŠ¤íŠ¸ ì¸ì½”ë”©)
# =============================================================================

def stage_8_text_encoding(
    data: Dict[str, pd.DataFrame],
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[Dict[str, Any], Dict[str, Any], float, int]:
    """
    Stage 8: ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
    
    Args:
        data: {"train": DataFrame, "val": DataFrame, "test": DataFrame}
        metadata: Stage 7ì˜ ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        data: {"train": {text, labels}, "val": {...}, "test": {...}}
        metadata: ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° (ëˆ„ì )
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
        output_count: ì´ ìƒ˜í”Œ ìˆ˜
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 8: ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ (í…ìŠ¤íŠ¸ ì¸ì½”ë”©)")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_8")
    stage_config = config["stage_config"]
    
    target_column = metadata["target_column"]
    feature_columns = metadata["feature_columns"]
    label_mapping = metadata["label_mapping"]
    
    # í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„¤ì •
    text_encoding_config = stage_config.get("text_encoding", {})
    
    # ê° ë¶„í• ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ì¸ì½”ë”© ìˆ˜í–‰
    logger.info("ğŸ“ í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì‹œì‘...")
    
    # PartitionedDataset ì²˜ë¦¬ - callableì´ë©´ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    if isinstance(data, dict) and all(callable(v) for v in data.values()):
        logger.info("PartitionedDataset detected - loading actual data...")
        loaded_data = {}
        for key, loader in data.items():
            loaded_data[key] = loader()
            logger.info(f"  Loaded {key}: {type(loaded_data[key])}, shape: {loaded_data[key].shape}")
        data = loaded_data
    
    # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
    if isinstance(data, dict) and len(data) > 0:
        # ì²« ë²ˆì§¸ ë¶„í• ì˜ ì»¬ëŸ¼ì„ í™•ì¸ (ëª¨ë“  ë¶„í• ì´ ë™ì¼í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì•¼ í•¨)
        sample_df = next(iter(data.values()))
        actual_columns = list(sample_df.columns)
        
        # feature_columnsì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        valid_feature_columns = [col for col in feature_columns if col in actual_columns]
        
        logger.info(f"ğŸ“Š ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ ìˆ˜: {len(actual_columns)}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„° í”¼ì²˜ ìˆ˜: {len(feature_columns)}")
        logger.info(f"ğŸ“Š ì‚¬ìš©í•  í”¼ì²˜ ìˆ˜: {len(valid_feature_columns)}")
        
        # ë””ë²„ê¹…: ì‹¤ì œ ì»¬ëŸ¼ê³¼ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼ ë¹„êµ
        if len(valid_feature_columns) < len(feature_columns):
            missing_columns = [col for col in feature_columns if col not in actual_columns]
            logger.warning(f"âš ï¸ ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_columns[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            
        # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if target_column not in actual_columns:
            logger.error(f"âŒ íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_column}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
            logger.info(f"ì‹¤ì œ ì»¬ëŸ¼ ëª©ë¡: {actual_columns}")
            # íƒ€ê²Ÿ ì»¬ëŸ¼ ì°¾ê¸° ì‹œë„
            # ê³„ì •ê³¼ëª©ì½”ë“œë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš©
            possible_target = [col for col in actual_columns if 'ê³„ì •ê³¼ëª©ì½”ë“œ' in col or 'acct_code' in col.lower()]
            if possible_target:
                target_column = possible_target[0]
                logger.info(f"ğŸ”„ ëŒ€ì²´ íƒ€ê²Ÿ ì»¬ëŸ¼ ì‚¬ìš©: {target_column}")
            else:
                # ë§ˆì§€ë§‰ ì‹œë„ - acctê°€ í¬í•¨ëœ ì»¬ëŸ¼
                possible_target = [col for col in actual_columns if 'acct' in col.lower() or 'ê³„ì •' in col]
                if possible_target:
                    target_column = possible_target[0]
                    logger.info(f"ğŸ”„ ëŒ€ì²´ íƒ€ê²Ÿ ì»¬ëŸ¼ ì‚¬ìš©: {target_column}")
        
        # ì‚¬ìš©í•  í”¼ì²˜ ì—…ë°ì´íŠ¸
        feature_columns = valid_feature_columns
    
    encoded_data = {}
    for split_name, df in data.items():
        encoded = prepare_split_data(
            df, split_name, target_column, feature_columns, 
            label_mapping, text_encoding_config
        )
        
        encoded_data[split_name] = {
            "text": encoded["text"],
            "labels": encoded["labels"],
            "original_labels": encoded["original_labels"]
        }
    
    # í† í¬ë‚˜ì´ì € ë¶„ì„ (ì„ íƒì )
    tokenizer_analysis = {}
    if stage_config.get("tokenizer_analysis", {}).get("enabled", False):
        all_texts = []
        all_labels = []
        
        for split_data in encoded_data.values():
            all_texts.extend(split_data['text'])
            all_labels.extend(split_data['labels'])
        
        tokenizer_analysis = perform_tokenizer_analysis(
            all_texts, all_labels,
            stage_config.get("tokenizer_analysis", {}),
            label_mapping
        )
    
    # ë°ì´í„° êµ¬ì¡° ë³€ê²½: DataFrame â†’ ì¸ì½”ë”©ëœ ë°ì´í„°
    data = encoded_data
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (ëˆ„ì ) - ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ë¥¼ ëª¨ë‘ ë³´ì¡´
    metadata.update({
        "text_encoding_config": text_encoding_config,
        "tokenizer_analysis": tokenizer_analysis,
        "encoding_timestamp": datetime.now().isoformat()
    })
    
    logger.info(f"âœ… Stage 8 ì™„ë£Œ: í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì™„ë£Œ")
    
    # ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ê¸°
    processing_time = time.time() - start_time
    output_count = sum(len(d["text"]) for d in encoded_data.values())
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 8")
    
    return data, prepare_metadata_for_json(metadata), processing_time, output_count


# =============================================================================
# Stage 9: ìµœì¢… íŒ¨í‚¤ì§• ë° ë¶„ì„
# =============================================================================

def stage_9_final_packaging(
    data: Dict[str, Any],
    metadata: Dict[str, Any],
    parameters: DictConfig
) -> Tuple[Dict[str, Any], Dict[str, Any], float]:
    """
    Stage 9: ìµœì¢… ë°ì´í„°ì…‹ íŒ¨í‚¤ì§• ë° ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¶„ì„
    
    Args:
        data: {"train": {...}, "val": {...}, "test": {...}}
        metadata: ëˆ„ì ëœ ëª¨ë“  ë©”íƒ€ë°ì´í„°
        parameters: íŒŒë¼ë¯¸í„° ì„¤ì •
        
    Returns:
        final_data: ìµœì¢… ë°ì´í„°ì…‹
        final_metadata: ì „ì²´ ë©”íƒ€ë°ì´í„°
        processing_time: ì²˜ë¦¬ ì‹œê°„(ì´ˆ)
    """
    start_time = time.time()
    
    logger.info("\n" + "="*60)
    logger.info("ğŸš‰ Stage 9: ìµœì¢… íŒ¨í‚¤ì§• ë° ë¶„ì„")
    logger.info("="*60)
    
    config = get_stage_config(parameters, "stage_9")
    stage_config = config["stage_config"]
    
    # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„± - train/val/testë¥¼ ìµœìƒìœ„ë¡œ ì´ë™
    final_data = {
        **data,  # train, val, testë¥¼ ìµœìƒìœ„ë¡œ í’€ì–´ëƒ„
        "dataset_info": {
            "creation_timestamp": datetime.now().isoformat(),
            "pipeline_version": stage_config.get("pipeline_version", "1.0.0"),
            "splits": list(data.keys()),
            "sample_counts": {k: len(v["text"]) for k, v in data.items()}
        }
    }
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    pipeline_start_time = datetime.fromisoformat(metadata["pipeline_start_time"])
    end_time = datetime.now()
    execution_time = (end_time - pipeline_start_time).total_seconds()
    
    # ë©”íƒ€ë°ì´í„° íŒ¨í‚¤ì§• ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    packaging_config = stage_config.get("metadata_packaging", {})
    
    # dataset_all.pklì— í¬í•¨í•  ì„ íƒì  ë©”íƒ€ë°ì´í„° êµ¬ì„±
    selected_metadata = {}
    
    # ë¼ë²¨ ë§¤í•‘
    if packaging_config.get("include_label_mapping", True):
        selected_metadata["label_mapping"] = metadata.get("label_mapping", {})
        logger.info("âœ“ ë¼ë²¨ ë§¤í•‘ í¬í•¨")
    
    # í”¼ì²˜ ì¹¼ëŸ¼ ëª©ë¡
    if packaging_config.get("include_feature_columns", True):
        selected_metadata["feature_columns"] = metadata.get("feature_columns", [])
        logger.info("âœ“ í”¼ì²˜ ì¹¼ëŸ¼ ëª©ë¡ í¬í•¨")
    
    # ì¹¼ëŸ¼ëª… ë§¤í•‘
    if packaging_config.get("include_columns_mapping", True):
        selected_metadata["columns_mapping"] = metadata.get("column_mapping", {})
        logger.info("âœ“ ì¹¼ëŸ¼ëª… ë§¤í•‘ í¬í•¨")
    
    # íšŒì‚¬ë³„ ìŠ¤ì¼€ì¼ë§ í†µê³„ - ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    # if packaging_config.get("include_company_scaling_stats", False):
    #     selected_metadata["company_scaling_stats"] = metadata.get("company_scaling_stats", {})
    #     logger.info("âœ“ íšŒì‚¬ë³„ ìŠ¤ì¼€ì¼ë§ í†µê³„ í¬í•¨")
    
    # í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„¤ì •
    if packaging_config.get("include_text_encoding_config", True):
        selected_metadata["text_encoding_config"] = metadata.get("text_encoding_config", {})
        logger.info("âœ“ í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„¤ì • í¬í•¨")
    
    # íƒ€ê²Ÿ ì¹¼ëŸ¼ëª…
    if packaging_config.get("include_target_column", True):
        selected_metadata["target_column"] = metadata.get("target_column", "")
        logger.info("âœ“ íƒ€ê²Ÿ ì¹¼ëŸ¼ëª… í¬í•¨")
    
    # ë°ì´í„° ë¶„í•  ì •ë³´
    if packaging_config.get("include_split_info", True):
        selected_metadata["split_info"] = metadata.get("split_stats", {})
        logger.info("âœ“ ë°ì´í„° ë¶„í•  ì •ë³´ í¬í•¨")
    
    # Z-score ë³€í™˜ íŒŒë¼ë¯¸í„°
    if packaging_config.get("include_z_score_params", True):
        selected_metadata["z_score_params"] = metadata.get("z_score_stats", {})
        logger.info("âœ“ Z-score ë³€í™˜ íŒŒë¼ë¯¸í„° í¬í•¨")
    
    # dataset_all.pklì— í¬í•¨ë  ìµœì¢… ë°ì´í„° êµ¬ì¡°
    final_data["metadata"] = selected_metadata
    
    logger.info(f"ğŸ“¦ ì„ íƒëœ ë©”íƒ€ë°ì´í„° í•­ëª© ìˆ˜: {len(selected_metadata)}")
    
    # ì „ì²´ ë©”íƒ€ë°ì´í„°ë¥¼ í‰íƒ„í•˜ê³  ì¤‘ë³µì—†ëŠ” êµ¬ì¡°ë¡œ ì¬êµ¬ì„±
    final_metadata = {
        # === íŒŒì´í”„ë¼ì¸ ì •ë³´ ===
        "pipeline_version": stage_config.get("pipeline_version", "1.0.0"),
        "pipeline_start_time": metadata["pipeline_start_time"],
        "pipeline_end_time": datetime.now().isoformat(),
        "pipeline_execution_seconds": execution_time,
        "pipeline_total_stages": 10,
        
        # === ë°ì´í„° í˜•ìƒ ì •ë³´ ===
        "data_original_shape": metadata.get("original_shape", {}),
        "data_final_shape": {
            "train": len(data.get("train", {}).get("text", [])),
            "validation": len(data.get("validation", {}).get("text", [])),
            "test": len(data.get("test", {}).get("text", []))
        },
        
        # === ì»¬ëŸ¼ ì •ë³´ (ì¤‘ë³µ ì œê±°) ===
        "columns_original": metadata.get("column_mapping", {}).get("original_columns", []),
        "columns_standardized": metadata.get("column_mapping", {}).get("standardized_columns", []),
        "columns_mapping": metadata.get("column_mapping", {}).get("rename_dict", {}),
        "columns_feature": metadata.get("feature_columns", []),
        "columns_target": metadata.get("target_column", ""),
        
        # === ë¼ë²¨ ì •ë³´ (ì¤‘ë³µ ì œê±°) ===
        "label_mapping": metadata.get("label_mapping", {}),
        "label_num_classes": metadata.get("label_mapping", {}).get("num_classes", 0),
        "label_num_base_classes": metadata.get("label_mapping", {}).get("num_base_classes", 0),
        "label_num_dummy_classes": metadata.get("label_mapping", {}).get("num_dummy_classes", 0),
        
        # === ë°ì´í„° ë¶„í•  ì •ë³´ ===
        "split_train_ratio": metadata.get("split_stats", {}).get("train_ratio", 0.0),
        "split_val_ratio": metadata.get("split_stats", {}).get("val_ratio", 0.0),
        "split_test_ratio": metadata.get("split_stats", {}).get("test_ratio", 0.0),
        "split_train_count": metadata.get("split_stats", {}).get("train_count", 0),
        "split_val_count": metadata.get("split_stats", {}).get("val_count", 0),
        "split_test_count": metadata.get("split_stats", {}).get("test_count", 0),
        
        # === ë°ì´í„° í’ˆì§ˆ ì •ë³´ ===
        "quality_duplicate_rows": metadata.get("data_quality", {}).get("duplicate_rows", 0),
        "quality_issues": metadata.get("data_quality", {}).get("quality_issues", []),
        "quality_class_distribution": metadata.get("class_distribution", {}),
        
        # === í•„í„°ë§ í†µê³„ ===
        "filter_stage2_removed_rows": metadata.get("stage_2_stats", {}).get("total_removed", 0),
        "filter_stage2_final_rows": metadata.get("stage_2_stats", {}).get("final_rows", 0),
        
        # === í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì •ë³´ ===
        "feature_engineering_added": metadata.get("feature_engineering_summary", {}).get("features_added", 0),
        "feature_engineering_names": metadata.get("feature_engineering_summary", {}).get("new_features", []),
        "feature_outlier_ratio": metadata.get("outlier_stats", {}).get("outlier_ratio", 0.0),
        
        # === í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì •ë³´ ===
        "encoding_method": metadata.get("text_encoding_config", {}).get("method", ""),
        "encoding_separator": metadata.get("text_encoding_config", {}).get("separator", ""),
        "encoding_missing_value": metadata.get("text_encoding_config", {}).get("missing_value", ""),
        "encoding_max_length": metadata.get("text_encoding_config", {}).get("max_length", 0),
        
        # === ìŠ¤ì¼€ì¼ë§ ì •ë³´ - ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ ===
        # "scaling_stats_available": False,
        # "scaling_num_companies": 0,
        
        # === ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ ===
        "stage_processing_times": {
            f"stage_{i}": metadata.get(f"stage_{i}_processing_time", 0.0) 
            for i in range(10)
        },
        
        # === ë‹¨ê³„ë³„ ì¶œë ¥ í–‰ ìˆ˜ ===
        "stage_output_counts": {
            f"stage_{i}": metadata.get(f"stage_{i}_output_count", 0) 
            for i in range(10)
        }
    }
    
    logger.info(f"ğŸ“¦ ìµœì¢… ë°ì´í„°ì…‹ íŒ¨í‚¤ì§• ì™„ë£Œ")
    logger.info(f"â±ï¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    logger.info(f"âœ… Stage 9 ì™„ë£Œ: íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    
    # ì²˜ë¦¬ ì‹œê°„
    processing_time = time.time() - start_time
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_memory("Stage 9")
    
    return final_data, prepare_metadata_for_json(final_metadata), processing_time