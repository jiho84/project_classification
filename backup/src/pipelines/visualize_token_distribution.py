#!/usr/bin/env python3
"""
í† í° ê¸¸ì´ ë¶„í¬ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
MLflowì—ì„œ í† í° ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ ì‹œê°í™”
"""

import json
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import argparse
from pathlib import Path

def load_token_stats(json_path: str):
    """í† í° í†µê³„ JSON íŒŒì¼ ë¡œë“œ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def plot_token_distribution(token_stats: dict, output_dir: str = "./"):
    """í† í° ê¸¸ì´ ë¶„í¬ ì‹œê°í™”"""
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('seaborn-v0_8-darkgrid')
    sns.set_palette("husl")
    
    # Figure ìƒì„±
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle(f'í† í° ê¸¸ì´ ë¶„í¬ ë¶„ì„ - {token_stats["tokenizer_name"]}', fontsize=16)
    
    # 1. íˆìŠ¤í† ê·¸ë¨
    if 'distribution' in token_stats:
        ax = axes[0, 0]
        bins = token_stats['distribution']['bins']
        hist = token_stats['distribution']['histogram']
        
        # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œì‹œ
        ax.bar(bins[:-1], hist, width=np.diff(bins)[0]*0.8, alpha=0.7)
        ax.axvline(token_stats['mean'], color='red', linestyle='--', label=f"í‰ê· : {token_stats['mean']:.1f}")
        ax.axvline(token_stats['percentiles']['p95'], color='orange', linestyle='--', label=f"95%: {token_stats['percentiles']['p95']}")
        
        ax.set_xlabel('í† í° ê¸¸ì´')
        ax.set_ylabel('ìƒ˜í”Œ ìˆ˜')
        ax.set_title('í† í° ê¸¸ì´ íˆìŠ¤í† ê·¸ë¨')
        ax.legend()
    
    # 2. ëˆ„ì  ë¶„í¬ í•¨ìˆ˜ (CDF)
    ax = axes[0, 1]
    percentiles = token_stats['percentiles']
    percentile_values = sorted([(int(k[1:]), v) for k, v in percentiles.items()])
    
    x_vals = [0] + [v for _, v in percentile_values] + [token_stats['max']]
    y_vals = [0] + [p/100 for p, _ in percentile_values] + [1.0]
    
    ax.plot(x_vals, y_vals, marker='o', markersize=8, linewidth=2)
    ax.grid(True, alpha=0.3)
    ax.set_xlabel('í† í° ê¸¸ì´')
    ax.set_ylabel('ëˆ„ì  í™•ë¥ ')
    ax.set_title('ëˆ„ì  ë¶„í¬ í•¨ìˆ˜ (CDF)')
    
    # ì£¼ìš” ë°±ë¶„ìœ„ìˆ˜ í‘œì‹œ
    for p, v in percentile_values:
        ax.annotate(f'{p}%: {v}', xy=(v, p/100), xytext=(v+10, p/100),
                   arrowprops=dict(arrowstyle='->', alpha=0.5))
    
    # 3. í†µê³„ ìš”ì•½
    ax = axes[1, 0]
    ax.axis('off')
    
    stats_text = f"""
    ğŸ“Š í† í° ê¸¸ì´ í†µê³„ ìš”ì•½
    
    í† í¬ë‚˜ì´ì €: {token_stats['tokenizer_name']}
    ë¶„ì„ ìƒ˜í”Œ ìˆ˜: {token_stats['sample_size']:,}
    
    ê¸°ë³¸ í†µê³„:
    â€¢ í‰ê· : {token_stats['mean']:.1f} í† í°
    â€¢ í‘œì¤€í¸ì°¨: {token_stats['std']:.1f}
    â€¢ ìµœì†Œê°’: {token_stats['min']} í† í°
    â€¢ ìµœëŒ€ê°’: {token_stats['max']} í† í°
    
    ë°±ë¶„ìœ„ìˆ˜:
    â€¢ 50% (ì¤‘ì•™ê°’): {token_stats['percentiles']['p50']} í† í°
    â€¢ 75%: {token_stats['percentiles']['p75']} í† í°
    â€¢ 90%: {token_stats['percentiles']['p90']} í† í°
    â€¢ 95%: {token_stats['percentiles']['p95']} í† í°
    â€¢ 99%: {token_stats['percentiles']['p99']} í† í°
    """
    
    ax.text(0.1, 0.9, stats_text, transform=ax.transAxes, 
            fontsize=12, verticalalignment='top', fontfamily='monospace')
    
    # 4. ê¶Œì¥ max_length ê³„ì‚°
    ax = axes[1, 1]
    ax.axis('off')
    
    # ê¶Œì¥ max_length ê³„ì‚° (95% ì»¤ë²„ë¦¬ì§€ ê¸°ì¤€)
    recommended_length = token_stats['percentiles']['p95']
    # 32ì˜ ë°°ìˆ˜ë¡œ ì˜¬ë¦¼ (íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´)
    recommended_length = ((recommended_length + 31) // 32) * 32
    
    coverage_text = f"""
    ğŸ¯ ê¶Œì¥ max_length ì„¤ì •
    
    95% ì»¤ë²„ë¦¬ì§€ ê¸°ì¤€:
    â€¢ ì‹¤ì œ 95% ë°±ë¶„ìœ„ìˆ˜: {token_stats['percentiles']['p95']} í† í°
    â€¢ ê¶Œì¥ max_length: {recommended_length} í† í°
      (32ì˜ ë°°ìˆ˜ë¡œ ì˜¬ë¦¼)
    
    ì»¤ë²„ë¦¬ì§€ ë¶„ì„:
    â€¢ max_length=128: ~{sum(1 for v in hist[:3] for _ in range(v)) / token_stats['sample_size'] * 100:.1f}% ì»¤ë²„
    â€¢ max_length=256: ~{sum(1 for v in hist[:6] for _ in range(v)) / token_stats['sample_size'] * 100:.1f}% ì»¤ë²„
    â€¢ max_length=512: ~{sum(1 for v in hist[:11] for _ in range(v)) / token_stats['sample_size'] * 100:.1f}% ì»¤ë²„
    
    ğŸ’¡ ì¶”ì²œ:
    - ë©”ëª¨ë¦¬ íš¨ìœ¨: max_length = 256
    - ì„±ëŠ¥ ì¤‘ì‹¬: max_length = {recommended_length}
    - ì™„ì „ ì»¤ë²„: max_length = 512
    """
    
    ax.text(0.1, 0.9, coverage_text, transform=ax.transAxes,
            fontsize=12, verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.3))
    
    # ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì €ì¥
    plt.tight_layout()
    output_file = output_path / "token_length_distribution.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {output_file}")
    
    # ì¶”ê°€ë¡œ ê°„ë‹¨í•œ ìš”ì•½ ì°¨íŠ¸ë„ ìƒì„±
    fig2, ax2 = plt.subplots(1, 1, figsize=(10, 6))
    
    # ë°•ìŠ¤í”Œë¡¯ ìŠ¤íƒ€ì¼ì˜ ì‹œê°í™”
    positions = [1]
    box_data = [[
        token_stats['min'],
        token_stats['percentiles']['p25'] if 'p25' in token_stats['percentiles'] else token_stats['percentiles']['p50'],
        token_stats['percentiles']['p50'],
        token_stats['percentiles']['p75'],
        token_stats['max']
    ]]
    
    bp = ax2.boxplot(box_data, positions=positions, widths=0.6, patch_artist=True,
                     showmeans=True, meanline=True)
    
    # ìŠ¤íƒ€ì¼ë§
    for patch in bp['boxes']:
        patch.set_facecolor('lightblue')
        patch.set_alpha(0.7)
    
    # ë°±ë¶„ìœ„ìˆ˜ ë¼ë²¨ ì¶”ê°€
    ax2.text(1, token_stats['percentiles']['p95'], f"95%: {token_stats['percentiles']['p95']}", 
            ha='center', va='bottom')
    ax2.text(1, token_stats['percentiles']['p99'], f"99%: {token_stats['percentiles']['p99']}", 
            ha='center', va='bottom')
    
    ax2.set_ylabel('í† í° ê¸¸ì´')
    ax2.set_title(f'í† í° ê¸¸ì´ ë¶„í¬ ìš”ì•½ - {token_stats["tokenizer_name"]}')
    ax2.set_xticklabels(['í† í° ê¸¸ì´ ë¶„í¬'])
    ax2.grid(True, alpha=0.3)
    
    output_file2 = output_path / "token_length_summary.png"
    plt.savefig(output_file2, dpi=300, bbox_inches='tight')
    print(f"âœ… ìš”ì•½ ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: {output_file2}")
    
    plt.show()

def main():
    parser = argparse.ArgumentParser(description='í† í° ê¸¸ì´ ë¶„í¬ ì‹œê°í™”')
    parser.add_argument('json_path', help='í† í° í†µê³„ JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output-dir', default='./', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬)')
    
    args = parser.parse_args()
    
    # í† í° í†µê³„ ë¡œë“œ
    token_stats = load_token_stats(args.json_path)
    
    # ì‹œê°í™” ìƒì„±
    plot_token_distribution(token_stats, args.output_dir)

if __name__ == "__main__":
    main()