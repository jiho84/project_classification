# ì§„ì§œ ë¬¸ì œ: Gradient Explosion

**ì‘ì„±ì¼**: 2025-10-10
**ê·¼ë³¸ ì›ì¸**: Gradient Normì´ 200 ìˆ˜ì¤€ â†’ Gradient Clippingì— ì˜í•´ 99.5% ì˜ë¦¼

---

## ğŸ”¥ ë°œê²¬í•œ ì§„ì§œ ë¬¸ì œ

### Gradient Norm ì¶”ì´

```
Step   Grad Norm   ì •ìƒ ë²”ìœ„ (0.1-1.0)
----   ---------   -------------------
10     197.22      197ë°° ë†’ìŒ! ğŸ˜±
20     215.00      215ë°° ë†’ìŒ!
50     186.94      187ë°° ë†’ìŒ
100    141.44      141ë°° ë†’ìŒ
200    57.88       58ë°° ë†’ìŒ
500    ???         (í™•ì¸ í•„ìš”)
```

**Gradient Normì´ 200 ìˆ˜ì¤€ì´ë©´**:
```python
gradient_clipping = 1.0

# ì‹¤ì œ gradient: 200
# Clipping í›„: 200 Ã— (1.0 / 200) = 1.0

â†’ Gradientì˜ 99.5%ê°€ ì˜ë ¤ë‚˜ê°!
â†’ í•™ìŠµì´ ê±°ì˜ ì•ˆ ë¨
```

---

## ğŸ’¥ ì™œ Gradientê°€ í­ë°œí•˜ëŠ”ê°€?

### ì›ì¸ 1: **ì´ˆê¸° Lossê°€ ë„ˆë¬´ ë†’ìŒ** (77.4)

Cross-entropy lossì—ì„œ:

```python
loss = -log(p_correct)

# ì •ìƒ: ëª¨ë¸ì´ ì •ë‹µ í´ë˜ìŠ¤ì— 1/280 í™•ë¥  ë°°ì •
p_correct â‰ˆ 1/280 = 0.00357
loss = -log(0.00357) = 5.63 (random)

# ë¹„ì •ìƒ: ëª¨ë¸ì´ ì •ë‹µ í´ë˜ìŠ¤ì— ë§¤ìš° ë‚®ì€ í™•ë¥  ë°°ì •
p_correct â‰ˆ 1e-34
loss = -log(1e-34) = 78.2

â†’ Lossê°€ 78ì´ë©´ gradientë„ ë¹„ë¡€í•˜ì—¬ í­ë°œ!
```

---

### ì›ì¸ 2: **Logit ì´ˆê¸°í™” ë¬¸ì œ**

ëª¨ë¸ ì´ˆê¸°í™” ì‹œ logitì´ ì˜ëª»ëœ ìŠ¤ì¼€ì¼:

```python
# ì •ìƒ ì´ˆê¸°í™”: Xavier/He initialization
logits âˆˆ [-0.1, 0.1]  # ì‘ì€ ê°’
â†’ softmax í›„: ëª¨ë“  í´ë˜ìŠ¤ê°€ ë¹„ìŠ·í•œ í™•ë¥  (1/280 â‰ˆ 0.00357)
â†’ Loss â‰ˆ 5.63

# ë¹„ì •ìƒ ì´ˆê¸°í™”
logits âˆˆ [-100, 100]  # ë„ˆë¬´ í° ê°’!
â†’ softmax í›„: ì¼ë¶€ í´ë˜ìŠ¤ê°€ 1.0, ë‚˜ë¨¸ì§€ 0
â†’ ì •ë‹µ í´ë˜ìŠ¤ê°€ 0ì´ë©´ loss = -log(0) = infinity!
â†’ ì‹¤ì œë¡œëŠ” ìˆ˜ì¹˜ ì•ˆì •ì„±ìœ¼ë¡œ ë§¤ìš° í° ê°’ (77.4)
```

**ì˜ì‹¬**: LoRA ì´ˆê¸°í™” ë˜ëŠ” Classification Head ì´ˆê¸°í™” ë¬¸ì œ

---

## ğŸ” ì¦ê±°

### 1. Lossì™€ Gradient Normì˜ ìƒê´€ê´€ê³„

```
Step   Loss    Grad Norm
----   -----   ---------
10     77.4    197.2
50     75.8    186.9
100    63.6    141.4
200    30.0    57.9

â†’ Lossê°€ ê°ì†Œí•˜ë©´ Gradient Normë„ ê°ì†Œ
â†’ ë‘˜ ë‹¤ ì—°ê´€ë˜ì–´ ìˆìŒ
```

### 2. Gradient Clippingì˜ ì˜í–¥

```python
Gradient Clipping = 1.0

Step 10:
- ì›ë˜ gradient norm: 197.2
- Clipping í›„: 1.0
- ê°ì†Œìœ¨: 99.49%

â†’ ê±°ì˜ í•™ìŠµì´ ì•ˆ ë¨
â†’ Lossê°€ ì²œì²œíˆë§Œ ê°ì†Œ
```

### 3. ì´ˆê¸° Lossì˜ ë¹„ì •ìƒì„±

```
Random ì˜ˆì¸¡ Loss: 5.63
ì‹¤ì œ ì´ˆê¸° Loss: 77.4
ë¹„ìœ¨: 13.7ë°°

â†’ ì´ˆê¸°í™”ê°€ randomë³´ë‹¤ 14ë°° ë‚˜ì¨
â†’ ëª¨ë¸ì´ ì˜ë„ì ìœ¼ë¡œ í‹€ë¦° ì˜ˆì¸¡ì„ í•˜ê³  ìˆìŒ
```

---

## ğŸ¯ ê·¼ë³¸ ì›ì¸ ì¶”ì •

### ê°€ì„¤ 1: **Classification Head ì´ˆê¸°í™” ì‹¤íŒ¨**

```python
# AutoModelForSequenceClassification.from_pretrained()
# ë‚´ë¶€ì ìœ¼ë¡œ classification headë¥¼ ì¶”ê°€:

model = Qwen3ForSequenceClassification(
    num_labels=280,
    ...
)

# Classification head (Linear layer):
self.score = nn.Linear(hidden_size, num_labels)

# ë¬¸ì œ: ì´ layerì˜ weight ì´ˆê¸°í™”ê°€ ì˜ëª»ë¨
# ì •ìƒ: N(0, 0.02^2)
# ë¹„ì •ìƒ: N(0, 1^2) ë˜ëŠ” ì´ˆê¸°í™” ì•ˆ ë¨
```

**ê²€ì¦ ë°©ë²•**:
```python
# ëª¨ë¸ ë¡œë“œ í›„
print(model.score.weight.std())

# ì •ìƒ: 0.01 ~ 0.05
# ë¹„ì •ìƒ: 0.5 ~ 10
```

---

### ê°€ì„¤ 2: **LoRAì™€ Classification Head ë¶ˆì¼ì¹˜**

LoRA ì„¤ì •:
```yaml
lora:
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  modules_to_save: ["score"]  # Classification head ì €ì¥
```

**ë¬¸ì œ**:
- LoRAëŠ” Transformer layerë§Œ ì ìš©
- Classification head(`score`)ëŠ” LoRA ì—†ì´ ê·¸ëŒ€ë¡œ í•™ìŠµ
- í•˜ì§€ë§Œ `modules_to_save`ë¡œ ì§€ì •ë˜ì–´ ìˆìŒ

**ì˜ì‹¬**:
- `score`ê°€ ì œëŒ€ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ
- ë˜ëŠ” LoRA weightì™€ full weightê°€ ì¶©ëŒ

---

### ê°€ì„¤ 3: **Gradient Scaling ë¬¸ì œ**

DeepSpeed + bfloat16 + LoRA ì¡°í•©:

```python
# LoRAëŠ” ì‘ì€ rankë¡œ í•™ìŠµ (r=256)
# í•˜ì§€ë§Œ gradientëŠ” ì „ì²´ parameterì—ì„œ ê³„ì‚°
# â†’ Gradientê°€ ë¹„ì •ìƒì ìœ¼ë¡œ í¼

# ì˜ˆ: Full weightì˜ gradientë¥¼ LoRAë¡œ ì••ì¶•í•  ë•Œ
# Scalingì´ ì˜ëª»ë˜ë©´ gradient í­ë°œ
```

---

## ğŸ’Š í•´ê²° ë°©ì•ˆ

### 1. **Gradient Clipping ì™„í™”** (ì¦‰ì‹œ ì¡°ì¹˜)

**í˜„ì¬** (training.yml:102, deepspeed.yml):
```yaml
gradient_clipping: 1.0  # ë„ˆë¬´ ê°•í•¨!
```

**ìˆ˜ì •**:
```yaml
gradient_clipping: 10.0  # 10ë°° ì™„í™”
```

**ì´ìœ **:
- Gradient normì´ 200 ìˆ˜ì¤€ì´ë¯€ë¡œ clippingì„ 10ìœ¼ë¡œ ì˜¬ë ¤ì•¼ í•¨
- 10ìœ¼ë¡œ ì˜¬ë¦¬ë©´ gradientì˜ 95%ê°€ ì‚´ì•„ë‚¨ìŒ (vs í˜„ì¬ 0.5%)
- í•™ìŠµ ì†ë„ 20ë°° ì¦ê°€ ì˜ˆìƒ

---

### 2. **Classification Head ì¬ì´ˆê¸°í™”** (ì½”ë“œ ìˆ˜ì • í•„ìš”)

`main_yaml.py`ì—ì„œ ëª¨ë¸ ë¡œë“œ í›„:

```python
# Line 173 ì´í›„ì— ì¶”ê°€
model = AutoModelForSequenceClassification.from_pretrained(...)

# Classification head ì¬ì´ˆê¸°í™”
if hasattr(model, 'score'):
    torch.nn.init.normal_(model.score.weight, std=0.02)
    if model.score.bias is not None:
        torch.nn.init.zeros_(model.score.bias)
    LOGGER.info("Re-initialized classification head")
```

**íš¨ê³¼**:
- ì´ˆê¸° lossê°€ 5.63 ìˆ˜ì¤€ìœ¼ë¡œ ì •ìƒí™”
- Gradient normë„ 1.0 ìˆ˜ì¤€ìœ¼ë¡œ ì •ìƒí™”

---

### 3. **Learning Rate ì¦ê°€** (gradient clipping ì™„í™” í›„)

Gradient clippingì„ 10ìœ¼ë¡œ ì™„í™”í•œ í›„:

```yaml
learning_rate: 2.0e-5  # 1e-5 â†’ 2e-5
```

**ì´ìœ **:
- í˜„ì¬ëŠ” gradientê°€ 99.5% ì˜ë ¤ì„œ LR 1e-5ì˜ íš¨ê³¼ê°€ 5e-8 ìˆ˜ì¤€
- Clipping ì™„í™”í•˜ë©´ ì‹¤íš¨ LRì´ ê¸‰ì¦
- LRì„ 2ë°°ë¡œ ì˜¬ë ¤ì„œ ë” ë¹ ë¥¸ ìˆ˜ë ´

---

### 4. **LoRA ì„¤ì • ê²€í† ** (ì„ íƒì‚¬í•­)

**í˜„ì¬**:
```yaml
lora:
  r: 256
  lora_alpha: 512
  modules_to_save: ["score"]
```

**ì˜ì‹¬ ì‚¬í•­**:
- `modules_to_save: ["score"]`ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
- LoRAì™€ full parameterê°€ ì¶©ëŒí•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸

**ê²€ì¦**:
```python
# ëª¨ë¸ ë¡œë“œ í›„
for name, param in model.named_parameters():
    if 'score' in name:
        print(f"{name}: requires_grad={param.requires_grad}, shape={param.shape}")
```

---

## ğŸ“Š ì˜ˆìƒ ê°œì„  íš¨ê³¼

### Gradient Clipping ì™„í™” (1.0 â†’ 10.0)

```
í˜„ì¬ (clipping=1.0):
Step 10:
- Gradient norm: 197.2 â†’ clipped to 1.0 (0.5% ìœ ì§€)
- ì‹¤íš¨ í•™ìŠµë¥ : 1e-5 Ã— 0.005 = 5e-8
- Loss ê°ì†Œ: 77.4 â†’ 75.8 (2% ê°ì†Œ)

ìˆ˜ì • í›„ (clipping=10.0):
Step 10:
- Gradient norm: 197.2 â†’ clipped to 10.0 (5% ìœ ì§€)
- ì‹¤íš¨ í•™ìŠµë¥ : 1e-5 Ã— 0.05 = 5e-7 (10ë°° ì¦ê°€)
- Loss ê°ì†Œ ì˜ˆìƒ: 77.4 â†’ 50.0 (35% ê°ì†Œ, 10ë°° ë¹ ë¦„)
```

### Classification Head ì¬ì´ˆê¸°í™”

```
í˜„ì¬:
- ì´ˆê¸° loss: 77.4 (randomì˜ 14ë°°)
- Gradient norm: 197.2

ì¬ì´ˆê¸°í™” í›„:
- ì´ˆê¸° loss: 5.6 (random ìˆ˜ì¤€)
- Gradient norm: 1.0 (ì •ìƒ)
- 200 stepì— loss < 1.0 ë‹¬ì„± ê°€ëŠ¥
```

---

## ğŸ”¬ ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1. Classification Head ìƒíƒœ í™•ì¸

```python
import torch
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "Qwen/Qwen3-4B",
    num_labels=280,
    torch_dtype=torch.bfloat16
)

print("=== Classification Head ===")
print(f"Weight std: {model.score.weight.std().item():.6f}")
print(f"Weight mean: {model.score.weight.mean().item():.6f}")
print(f"Weight min: {model.score.weight.min().item():.6f}")
print(f"Weight max: {model.score.weight.max().item():.6f}")

# ì •ìƒ: std â‰ˆ 0.02, mean â‰ˆ 0, min/max âˆˆ [-0.1, 0.1]
# ë¹„ì •ìƒ: std > 0.5 ë˜ëŠ” min/maxê°€ ê·¹ë‹¨ì 
```

### 2. ì´ˆê¸° ì˜ˆì¸¡ ë¶„í¬ í™•ì¸

```python
# ì²« ë°°ì¹˜ë¡œ ì˜ˆì¸¡
outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])
logits = outputs.logits

print("=== Logits ë¶„ì„ ===")
print(f"Logits mean: {logits.mean().item():.2f}")
print(f"Logits std: {logits.std().item():.2f}")
print(f"Logits min: {logits.min().item():.2f}")
print(f"Logits max: {logits.max().item():.2f}")

# ì •ìƒ: mean â‰ˆ 0, std â‰ˆ 1, min/max âˆˆ [-5, 5]
# ë¹„ì •ìƒ: std > 10 ë˜ëŠ” min/maxê°€ ê·¹ë‹¨ì  (Â±50 ì´ìƒ)
```

### 3. LoRA ìƒíƒœ í™•ì¸

```python
from peft import get_peft_model

# LoRA ì ìš© í›„
print("=== LoRA Parameters ===")
trainable_params = 0
all_params = 0
for name, param in model.named_parameters():
    all_params += param.numel()
    if param.requires_grad:
        trainable_params += param.numel()
        if 'score' in name:
            print(f"Trainable: {name} - shape: {param.shape}")

print(f"Trainable: {trainable_params:,} / {all_params:,} ({trainable_params/all_params*100:.2f}%)")
```

---

## ğŸ“ ì¦‰ì‹œ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ê¸´ê¸‰ ìˆ˜ì • (ì§€ê¸ˆ ë°”ë¡œ)

1. [ ] **Gradient Clipping ì™„í™”**
   - `training.yml:102` gradient_clipping: 1.0 â†’ **10.0**
   - `deepspeed config` gradient_clipping: 1.0 â†’ **10.0**

2. [ ] **ì¬í•™ìŠµ ì‹œì‘**
   - ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
   - ìƒˆë¡œ í•™ìŠµ ì‹œì‘
   - ì´ˆê¸° grad_norm í™•ì¸ (ì—¬ì „íˆ 200 ìˆ˜ì¤€ì¸ì§€)

3. [ ] **50 step í›„ í™•ì¸**
   - Lossê°€ 50 ì´í•˜ë¡œ ë–¨ì–´ì§€ëŠ”ì§€
   - Grad normì´ 50 ì´í•˜ë¡œ ë–¨ì–´ì§€ëŠ”ì§€

### âœ… ì¶”ê°€ í™•ì¸ (1ë‹¨ê³„ ì‹¤íŒ¨ ì‹œ)

4. [ ] **Classification Head ì¬ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€**
   - `src/train/main_yaml.py:173` ì´í›„ì— ì¬ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€
   - ì¬í•™ìŠµ
   - ì´ˆê¸° loss 5-6 ë²”ìœ„ í™•ì¸

5. [ ] **num_labels ëª…ì‹œ**
   - `training.yml:55` num_labels: null â†’ **280**
   - (ì´ê±´ ì´ë¯¸ ìë™ìœ¼ë¡œ 280ìœ¼ë¡œ ì„¤ì •ë˜ê³  ìˆìŒ)

---

## ğŸ“ ê²°ë¡ 

**ì§„ì§œ ë¬¸ì œëŠ”**:
1. âŒ Warmupì´ ì•„ë‹˜ (ë°±ì—…ë„ ë™ì¼)
2. âŒ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì•„ë‹˜ (2ì°¨ ë¬¸ì œ)
3. âœ… **Gradient Explosion** (gradient norm 200)
4. âœ… **ê³¼ë„í•œ Gradient Clipping** (99.5% ì˜ë¦¼)
5. âœ… **ì´ˆê¸°í™” ë¬¸ì œ** (ì´ˆê¸° loss 77.4)

**ì¦‰ì‹œ ì¡°ì¹˜**:
- Gradient clipping: 1.0 â†’ 10.0
- ì¬í•™ìŠµ ì‹œì‘
- 50 stepì— loss < 50 í™•ì¸

**ì˜ˆìƒ ê²°ê³¼**:
- 100 stepì— loss < 20
- 200 stepì— loss < 5
- 500 stepì— loss < 1 (ì†Œìˆ˜ì  ì§„ì…!)

---

## ğŸ“Œ ì°¸ê³ : Gradient Clipping ì´ë¡ 

### Gradient Clippingì´ë€?

```python
# Gradient normì´ thresholdë¥¼ ì´ˆê³¼í•˜ë©´ ìŠ¤ì¼€ì¼ ë‹¤ìš´
if grad_norm > threshold:
    grad = grad Ã— (threshold / grad_norm)
```

### ì ì ˆí•œ Clipping ê°’

```
Gradient Norm ë²”ìœ„    ê¶Œì¥ Clipping
------------------    --------------
0.1 ~ 1.0             1.0 (ì •ìƒ)
1.0 ~ 10.0            5.0
10.0 ~ 100.0          50.0
100.0 ~ 1000.0        100.0 ì´ìƒ

í˜„ì¬: 200 ìˆ˜ì¤€ â†’ 10.0ìœ¼ë¡œ ì‹œì‘, í•„ìš”ì‹œ 50.0ê¹Œì§€
```

### Clippingì´ ë„ˆë¬´ ê°•í•˜ë©´

```python
# Gradientì˜ ëŒ€ë¶€ë¶„ì´ ì˜ë¦¼
# â†’ í•™ìŠµì´ ê±°ì˜ ì•ˆ ë¨
# â†’ Lossê°€ ì²œì²œíˆë§Œ ê°ì†Œ
# â†’ "ë¶€ë¶„ìµœì í™”ì— ë¹ ì§„ ê²ƒì²˜ëŸ¼ ë³´ì„"

# ì‹¤ì œë¡œëŠ” gradientê°€ ë„ˆë¬´ ì•½í•´ì„œ ìµœì í™”ê°€ ì•ˆ ë˜ëŠ” ê²ƒ!
```
