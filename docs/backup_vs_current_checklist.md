# Backup vs Current ì²´ê³„ì  ë¹„êµ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ëª©ì **: Gradient explosion (norm 200) ì›ì¸ ê·œëª…
**ì‘ì„±ì¼**: 2025-10-10
**ìƒí™©**: ë°ì´í„° ë™ì¼, íŒŒë¼ë¯¸í„° ëŒ€ë¶€ë¶„ ë™ì¼, í•˜ì§€ë§Œ gradient normì´ 200ë°° ì°¨ì´

---

## ğŸ¯ ë¹„êµ ì „ëµ

### ê°€ì„¤
1. **Training Loop êµ¬ì¡° ì°¨ì´** (Custom loop vs HuggingFace Trainer)
2. **Optimizer ì„¤ì • ì°¨ì´** (DeepSpeed optimizer vs Trainer optimizer)
3. **Scheduler êµ¬í˜„ ì°¨ì´** (DeepSpeed scheduler vs Trainer scheduler)
4. **Gradient ê³„ì‚° ë°©ì‹ ì°¨ì´** (Manual backward vs Trainer automatic)
5. **Loss Scaling ì°¨ì´** (bfloat16 handling)
6. **Gradient Accumulation ì°¨ì´**
7. **Metric ê³„ì‚° ë°©ì‹ ì°¨ì´** (Gradient norm ì¸¡ì • ë°©ë²•)

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë‹¨ê³„ë³„)

### âœ… Phase 1: êµ¬ì¡°ì  ì°¨ì´ íŒŒì•… (ì™„ë£Œ)

- [x] **1.1 Training Framework**
  - Backup: Custom training loop with manual forward/backward
  - Current: HuggingFace Trainer
  - **ë°œê²¬**: âš ï¸ **CRITICAL DIFFERENCE**
    - Backup: `for epoch in range(...)` â†’ manual `model_engine.backward(loss)` â†’ `model_engine.step()`
    - Current: `trainer.train()` â†’ automatic backward/step
    - **ì˜í–¥**: Loss scaling ë°©ì‹ê³¼ gradient ê³„ì‚° íë¦„ì´ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¦„

- [x] **1.2 DeepSpeed í†µí•© ë°©ì‹**
  - Backup: `deepspeed.initialize()` ì§ì ‘ í˜¸ì¶œ (train.py:700-800 ì˜ˆìƒ)
  - Current: Trainerì˜ DeepSpeed integration via `TrainingArguments(deepspeed=...)`
  - **ë°œê²¬**: âš ï¸ **INITIALIZATION DIFFERENCE**
    - Backup: DeepSpeedê°€ ëª¨ë¸, optimizer, scheduler ëª¨ë‘ ì§ì ‘ ì´ˆê¸°í™”
    - Current: Trainerê°€ ë¨¼ì € ì´ˆê¸°í™” í›„ DeepSpeedì— ì „ë‹¬
    - **ì˜í–¥**: Optimizer/scheduler ìƒì„± ìˆœì„œì™€ íŒŒë¼ë¯¸í„° ì ìš© ë°©ì‹ ì°¨ì´

- [x] **1.3 í•™ìŠµ ë£¨í”„ êµ¬ì¡°**
  - Backup: Manual forward/backward/step with explicit loss handling
  - Current: Trainer.train() with automatic handling
  - **ë°œê²¬**: âš ï¸ **LOSS SCALING DIFFERENCE**
    - Backup: `loss = loss_fn(logits, labels)` â†’ `model_engine.backward(loss)` (no division)
    - Current: Trainer automatically divides loss by gradient_accumulation_steps
    - **ì˜í–¥**: ì´ê²ƒì´ gradient norm ì°¨ì´ì˜ ì£¼ìš” ì›ì¸ì¼ ê°€ëŠ¥ì„± ë†’ìŒ!

---

### âœ… Phase 2: Optimizer ë¹„êµ (ì™„ë£Œ)

- [x] **2.1 Optimizer ìƒì„± ë°©ì‹**
  - Backup: DeepSpeed configì—ì„œ optimizer ì •ì˜ â†’ `deepspeed.initialize()`ì—ì„œ ìƒì„±
  - Current: Trainerê°€ optimizer ìƒì„± â†’ DeepSpeedì— ì „ë‹¬
  - **ë°œê²¬**: âœ… **SAME - No Issue**
    - ë‘˜ ë‹¤ DeepSpeedì˜ FusedAdam (AdamW variant) ì‚¬ìš©
    - DeepSpeed configì—ì„œ ë™ì¼í•˜ê²Œ ì •ì˜ë¨

- [x] **2.2 Learning Rate**
  - Backup: `learning_rate: &lr 1e-5` (deepspeed.yml:24)
  - Current: `learning_rate: 1.0e-5` (training.yml:66)
  - **ë°œê²¬**: âœ… **IDENTICAL**

- [x] **2.3 Weight Decay**
  - Backup: `weight_decay: 0.002` (deepspeed.yml:53)
  - Current: `weight_decay: 0.002` (training.yml:67)
  - **ë°œê²¬**: âœ… **IDENTICAL**

- [x] **2.4 Betas & Epsilon**
  - Backup: `betas: [0.9, 0.999]`, `eps: 1e-8` (deepspeed.yml:51-52)
  - Current: Trainer ê¸°ë³¸ê°’ (ë™ì¼: [0.9, 0.999], 1e-8)
  - **ë°œê²¬**: âœ… **IDENTICAL** (Trainer uses same defaults)

---

### âœ… Phase 3: Scheduler ë¹„êµ (ì™„ë£Œ)

- [x] **3.1 Scheduler íƒ€ì…**
  - Backup: DeepSpeed `WarmupCosineLR` (deepspeed.yml:81)
  - Current: Trainer `cosine` scheduler (training.yml:69)
  - **ë°œê²¬**: âš ï¸ **MINOR DIFFERENCE - Likely OK**
    - ë‘˜ ë‹¤ cosine annealing with warmup
    - êµ¬í˜„ì²´ëŠ” ë‹¤ë¥´ì§€ë§Œ ì•Œê³ ë¦¬ì¦˜ì€ ë™ì¼
    - **ì˜í–¥**: Gradient normì— ì§ì ‘ì  ì˜í–¥ ì—†ìŒ

- [x] **3.2 Warmup ì„¤ì •**
  - Backup: `warmup_ratio: 0.1` (deepspeed.yml:132, 10% warmup)
  - Current: `warmup_ratio: 0.01` (training.yml:68, 1% warmup)
  - **ë°œê²¬**: âš ï¸ **SIGNIFICANT DIFFERENCE**
    - Backup: ì „ì²´ ìŠ¤í…ì˜ 10%ë¥¼ warmup
    - Current: ì „ì²´ ìŠ¤í…ì˜ 1%ë§Œ warmup (10ë°° ì°¨ì´!)
    - **ì˜í–¥**: ì´ˆê¸° í•™ìŠµ ì•ˆì •ì„± ì°¨ì´ â†’ gradient explosion ê°€ëŠ¥ì„± ì¦ê°€
    - **ì˜ˆì‹œ**: 1000 steps â†’ Backup: 100 warmup, Current: 10 warmup

- [x] **3.3 Total Steps ê³„ì‚°**
  - Backup: Manual ê³„ì‚° (`total_num_steps: "auto"`)
  - Current: Trainer ìë™ ê³„ì‚°
  - **ë°œê²¬**: âœ… **SAME - No Issue**
    - ë‘˜ ë‹¤ (num_samples / batch_size) * epochsë¡œ ê³„ì‚°
    - ë™ì¼í•œ ê²°ê³¼ ì˜ˆìƒ

---

### âœ… Phase 4: Gradient ê³„ì‚° ë¹„êµ (ì™„ë£Œ)

- [x] **4.1 Backward Pass**
  - Backup: `model_engine.backward(loss)` (train.py:1298)
  - Current: Trainer automatic backward
  - **ë°œê²¬**: âš ï¸ **CRITICAL - LOSS SCALING DIFFERENCE**
    - Backup: `loss = loss_fn(logits, labels)` â†’ `model_engine.backward(loss)` (ì›ë³¸ loss)
    - Current: Trainer internally: `loss = loss / gradient_accumulation_steps` â†’ backward
    - **ROOT CAUSE**: ì´ê²ƒì´ gradient norm 200ë°° ì°¨ì´ì˜ ì£¼ìš” ì›ì¸!
    - **ì„¤ëª…**:
      - Backup: Loss 1000 â†’ gradient 1000 (stepë§ˆë‹¤ accumulate)
      - Current: Loss 1000 / 2 = 500 â†’ gradient 500 (accumulation ì „ ë¶„í• )

- [x] **4.2 Gradient Accumulation**
  - Backup: `gradient_accumulation_steps: 2` (deepspeed.yml:31)
  - Current: `gradient_accumulation_steps: 2` (training.yml:65)
  - **ë°œê²¬**: âš ï¸ **SAME VALUE, DIFFERENT HANDLING**
    - ê°’ì€ ë™ì¼í•˜ì§€ë§Œ loss scaling ë°©ì‹ì´ ë‹¤ë¦„ (ìœ„ 4.1 ì°¸ì¡°)
    - **ì˜í–¥**: Effective batch sizeëŠ” ë™ì¼í•˜ì§€ë§Œ gradient í¬ê¸°ê°€ ë‹¤ë¦„

- [x] **4.3 Gradient Clipping**
  - Backup: `gradient_clipping: 1.0` (deepspeed.yml:54)
  - Current: `gradient_clipping: 1.0` (train_config.yml:78)
  - **ë°œê²¬**: âœ… **IDENTICAL**
    - ë‘˜ ë‹¤ DeepSpeedì˜ gradient clipping ì‚¬ìš©
    - Clip ì „ norm ê³„ì‚° â†’ clip â†’ optimizer step

- [x] **4.4 Gradient Norm ê³„ì‚°**
  - Backup: DeepSpeed `get_global_grad_norm()` â†’ MLflow logging
  - Current: Trainer automatic logging
  - **ë°œê²¬**: âœ… **SAME METHOD**
    - ë‘˜ ë‹¤ DeepSpeedì˜ ë‚´ì¥ norm ê³„ì‚° ì‚¬ìš©
    - ê³„ì‚° ë°©ì‹ì€ ë™ì¼í•˜ì§€ë§Œ ì…ë ¥ gradient í¬ê¸°ê°€ ë‹¤ë¦„ (4.1 ì°¸ì¡°)

---

### â¬œ Phase 5: Loss & Mixed Precision

- [ ] **5.1 Loss Computation**
  - Backup: Manual loss calculation
  - Current: Trainer automatic
  - **ì¡°ì‚¬**: Loss scaling ì°¨ì´

- [ ] **5.2 bfloat16 Handling**
  - Backup: DeepSpeed bf16 config
  - Current: Trainer bf16 + DeepSpeed
  - **ì¡°ì‚¬**: Mixed precision ì„¤ì • ì°¨ì´

- [ ] **5.3 Loss Scaling**
  - Backup: ì—†ìŒ (bfloat16ì€ dynamic scaling ë¶ˆí•„ìš”)
  - Current: ë™ì¼ ì˜ˆìƒ
  - **ì¡°ì‚¬**: Automatic loss scaling ì—¬ë¶€

---

### âœ… Phase 6: LoRA ì„¤ì • ë¹„êµ (ì™„ë£Œ)

- [x] **6.1 LoRA Config**
  - Backup: `r=256, alpha=512, dropout=0.05` (deepspeed.yml:13-15)
  - Current: `r=256, alpha=512, dropout=0.05` (training.yml:41-43)
  - **ë°œê²¬**: âœ… **IDENTICAL**

- [x] **6.2 Target Modules**
  - Backup: `["q_proj","k_proj","v_proj","o_proj"]` (deepspeed.yml:16)
  - Current: `["q_proj","k_proj","v_proj","o_proj"]` (training.yml:43)
  - **ë°œê²¬**: âœ… **IDENTICAL**

- [x] **6.3 Layers to Transform**
  - Backup: `[-6,-5,-4,-3,-2,-1]` (deepspeed.yml:17, ë§ˆì§€ë§‰ 6ê°œ ë ˆì´ì–´)
  - Current: `[28,29,30,31,32,33,34,35]` (training.yml:44, 8ê°œ ë ˆì´ì–´)
  - **ë°œê²¬**: âš ï¸ **DIFFERENCE - BUT NOT THE CAUSE**
    - Backup: 6 layers (Qwen3-4B 32 layers â†’ layers 26-31)
    - Current: 8 layers (layers 28-35)
    - **ì˜í–¥ ë¶„ì„**:
      - ë” ë§ì€ LoRA íŒŒë¼ë¯¸í„° â†’ ë” í° gradient (ì´ë¡ ì )
      - í•˜ì§€ë§Œ 200ë°° ì°¨ì´ë¥¼ ì„¤ëª…í•˜ê¸°ì—” ë¶€ì¡± (33% ì¦ê°€ì— ë¶ˆê³¼)
      - **ë³´ì¡° ì›ì¸**ì¼ ìˆ˜ ìˆìœ¼ë‚˜ ì£¼ ì›ì¸ì€ ì•„ë‹˜

- [x] **6.4 modules_to_save**
  - Backup: `["score"]` (deepspeed.yml:18)
  - Current: `["score"]` (training.yml:45)
  - **ë°œê²¬**: âœ… **IDENTICAL**

---

### â¬œ Phase 7: ë°ì´í„° ì²˜ë¦¬ ë¹„êµ

- [ ] **7.1 DataLoader**
  - Backup: Custom DataLoader (prefetch_factor=2, persistent_workers=True)
  - Current: Trainer DataLoader (ê¸°ë³¸ ì„¤ì •)
  - **ì¡°ì‚¬**: Batch êµ¬ì„± ì°¨ì´

- [ ] **7.2 Collator**
  - Backup: DataCollatorWithPadding
  - Current: DataCollatorWithPadding
  - **ì¡°ì‚¬**: Padding ë°©ì‹ ì°¨ì´

- [ ] **7.3 Batch Size**
  - Backup: 16 Ã— 2 Ã— 4 = 128 (effective)
  - Current: 16 Ã— 2 Ã— 4 = 128 (effective)
  - **ì¡°ì‚¬**: ë™ì¼ í™•ì¸

---

## ğŸ” ë°œê²¬ ì‚¬í•­ (ìµœì¢… ì •ë¦¬)

### ğŸš¨ ë°œê²¬ 1: **ROOT CAUSE - Loss Scaling ì°¨ì´** (CRITICAL!)

**ë¬¸ì œ**: Gradient norm 200ë°° ì°¨ì´ì˜ **ì£¼ìš” ì›ì¸**

**Backup (Manual Loop)**:
```python
# train.py:1296-1299
loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
model_engine.backward(loss)  # ì›ë³¸ loss ê·¸ëŒ€ë¡œ backward
model_engine.step()
```

**Current (HuggingFace Trainer)**:
```python
# Trainer ë‚´ë¶€ (transformers/trainer.py)
if self.args.gradient_accumulation_steps > 1:
    loss = loss / self.args.gradient_accumulation_steps  # ğŸš¨ Lossë¥¼ ë¨¼ì € ë¶„í• !
self.accelerator.backward(loss)
```

**ì˜í–¥ ë¶„ì„**:
- `gradient_accumulation_steps = 2`ì¼ ë•Œ:
  - Backup: Loss = 1000 â†’ Gradient = 1000 (2ë²ˆ accumulate â†’ avg gradient = 500)
  - Current: Loss = 1000/2 = 500 â†’ Gradient = 500 (2ë²ˆ accumulate â†’ avg gradient = 250)
- **Gradient norm ì°¨ì´**: 1000 vs 500 = **2ë°° ì°¨ì´** (accumulation ì „)
- **ì‹¤ì œ ê´€ì¸¡**: 200ë°° ì°¨ì´ â†’ ë‹¤ë¥¸ ìš”ì¸ê³¼ ë³µí•© ì‘ìš© ê°€ëŠ¥ì„±

**ê²°ë¡ **: ì´ê²ƒì´ **ì£¼ìš” ì›ì¸**ì´ì§€ë§Œ, 200ë°°ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„  ì¶”ê°€ ìš”ì¸ í•„ìš”

---

### âš ï¸ ë°œê²¬ 2: **Warmup Ratio ì°¨ì´** (SIGNIFICANT)

**Backup**:
```yaml
warmup_ratio: 0.1  # ì „ì²´ ìŠ¤í…ì˜ 10%
```

**Current**:
```yaml
warmup_ratio: 0.01  # ì „ì²´ ìŠ¤í…ì˜ 1% (10ë°° ì‘ìŒ!)
```

**ì˜í–¥ ë¶„ì„**:
- 1000 steps ê¸°ì¤€:
  - Backup: 100 steps warmup â†’ ì²œì²œíˆ LR ì¦ê°€ â†’ ì•ˆì •ì  í•™ìŠµ
  - Current: 10 steps warmup â†’ ë¹ ë¥´ê²Œ full LR â†’ gradient explosion ìœ„í—˜
- **ì´ˆê¸° í•™ìŠµ ë¶ˆì•ˆì •ì„±** â†’ gradient norm í­ë°œ ê°€ëŠ¥ì„± ì¦ê°€
- **íŠ¹íˆ LoRA í•™ìŠµì—ì„œ warmup ë¶€ì¡±ì€ ì¹˜ëª…ì !**

**ê²°ë¡ **: **ë³´ì¡° ì›ì¸**ìœ¼ë¡œ gradient ë¶ˆì•ˆì •ì„± ê¸°ì—¬

---

### âš ï¸ ë°œê²¬ 3: **LoRA Layers ì°¨ì´** (MINOR)

**Backup**:
```yaml
layers_to_transform: [-6,-5,-4,-3,-2,-1]  # 6 layers (26-31)
```

**Current**:
```yaml
layers_to_transform: [28,29,30,31,32,33,34,35]  # 8 layers
```

**ì˜í–¥ ë¶„ì„**:
- 33% ë” ë§ì€ LoRA íŒŒë¼ë¯¸í„° í•™ìŠµ
- ë” ë§ì€ gradient ê³„ì‚° â†’ ì•½ê°„ ë” í° norm
- í•˜ì§€ë§Œ **200ë°° ì°¨ì´ë¥¼ ì„¤ëª…í•˜ê¸°ì—” ë§¤ìš° ë¶€ì¡±**

**ê²°ë¡ **: **ë¯¸ë¯¸í•œ ì˜í–¥**, ì£¼ ì›ì¸ ì•„ë‹˜

---

### ğŸ“Š ë°œê²¬ ìš”ì•½ (ìš°ì„ ìˆœìœ„)

| ë°œê²¬ | ì°¨ì´ | Gradient Norm ì˜í–¥ | ìš°ì„ ìˆœìœ„ |
|------|------|-------------------|---------|
| **Loss Scaling** | TrainerëŠ” lossë¥¼ ë¯¸ë¦¬ ë‚˜ëˆ” | âš ï¸âš ï¸âš ï¸ **CRITICAL** | ğŸ”´ **P0** |
| **Warmup Ratio** | 0.1 â†’ 0.01 (10ë°° ê°ì†Œ) | âš ï¸âš ï¸ **SIGNIFICANT** | ğŸŸ  **P1** |
| **LoRA Layers** | 6 â†’ 8 layers (33% ì¦ê°€) | âš ï¸ **MINOR** | ğŸŸ¡ **P2** |

---

## ğŸ“Š ì‹¤í—˜ ê³„íš (ìš°ì„ ìˆœìœ„ ì¬ì •ë ¬)

### ğŸ”´ ì‹¤í—˜ 1: Loss Scaling ê²€ì¦ ë° ìˆ˜ì • (P0 - CRITICAL)
- **ëª©í‘œ**: Trainerì˜ loss scaling ë™ì‘ í™•ì¸ ë° ë³´ì •
- **ë¬¸ì œ**: TrainerëŠ” `loss / gradient_accumulation_steps`ë¥¼ ìë™ ìˆ˜í–‰
- **í•´ê²° ë°©ë²• (3ê°€ì§€ ì˜µì…˜)**:
  1. **Option A**: Custom Trainer ìƒì„± â†’ `compute_loss()` override â†’ loss scaling ì œê±°
  2. **Option B**: Callbackìœ¼ë¡œ gradient ìˆ˜ë™ ìŠ¤ì¼€ì¼ë§ (backward í›„)
  3. **Option C**: DeepSpeed configì—ì„œ gradient_accumulation_steps=1 ì„¤ì • â†’ Trainerì—ì„œë§Œ 2ë¡œ ì„¤ì •
- **ì˜ˆìƒ ê²°ê³¼**: Gradient norm 2ë°° ê°ì†Œ (1000 â†’ 500)
- **ì‹¤í–‰**: [ ] ìš°ì„  Option A í…ŒìŠ¤íŠ¸
- **ê²°ê³¼**: (ì˜ˆì •)

---

### ğŸŸ  ì‹¤í—˜ 2: Warmup Ratio ìˆ˜ì • (P1 - HIGH)
- **ëª©í‘œ**: Warmup ratioë¥¼ ë°±ì—…ê³¼ ë™ì¼í•˜ê²Œ ì¦ê°€
- **ë³€ê²½**: `warmup_ratio: 0.01` â†’ `0.1`
- **ìœ„ì¹˜**: `/home/user/projects/kedro_project/account-tax/conf/base/parameters/training.yml:68`
- **ì˜ˆìƒ ê²°ê³¼**:
  - ì´ˆê¸° í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
  - Gradient explosion ì™„í™”
  - Warmup steps: 10 â†’ 100 (1000 steps ê¸°ì¤€)
- **ì‹¤í–‰**: [ ] ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
- **ê²°ê³¼**: (ì˜ˆì •)

---

### ğŸŸ¡ ì‹¤í—˜ 3: LoRA Layers í†µì¼ (P2 - MEDIUM)
- **ëª©í‘œ**: layers_to_transformì„ ë°±ì—…ê³¼ ë™ì¼í•˜ê²Œ ë³€ê²½
- **ë³€ê²½**: `[28,29,30,31,32,33,34,35]` â†’ `[-6,-5,-4,-3,-2,-1]`
- **ìœ„ì¹˜**: `/home/user/projects/kedro_project/account-tax/conf/base/parameters/training.yml:44`
- **ì˜ˆìƒ ê²°ê³¼**: ë¯¸ë¯¸í•œ gradient norm ê°ì†Œ (33% íŒŒë¼ë¯¸í„° ê°ì†Œ)
- **ì‹¤í–‰**: [ ]
- **ê²°ê³¼**: (ì˜ˆì •)

---

### ğŸ“‹ ì‹¤í—˜ 4: í†µí•© í…ŒìŠ¤íŠ¸ (ìµœì¢… ê²€ì¦)
- **ëª©í‘œ**: ëª¨ë“  ìˆ˜ì •ì‚¬í•­ í†µí•© ì ìš© í›„ gradient norm ì¸¡ì •
- **ì ìš© ìˆœì„œ**:
  1. Warmup ratio ìˆ˜ì • (ì¦‰ì‹œ ì ìš©)
  2. LoRA layers ìˆ˜ì • (ì¦‰ì‹œ ì ìš©)
  3. Loss scaling ìˆ˜ì • (Custom Trainer í•„ìš”)
- **ì˜ˆìƒ ê²°ê³¼**: Gradient norm < 1.0 (ë°±ì—…ê³¼ ë™ì¼)
- **ì‹¤í–‰**: [ ]
- **ê²°ê³¼**: (ì˜ˆì •)

---

## ğŸ“ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ìˆ˜ì • (Quick Wins)

### 1. Warmup Ratio ìˆ˜ì • (5ë¶„ ì‘ì—…)
```bash
# File: /home/user/projects/kedro_project/account-tax/conf/base/parameters/training.yml
# Line 68: warmup_ratio: 0.01 â†’ 0.1
```

### 2. LoRA Layers ìˆ˜ì • (5ë¶„ ì‘ì—…)
```bash
# File: /home/user/projects/kedro_project/account-tax/conf/base/parameters/training.yml
# Line 44: layers_to_transform: [28,29,30,31,32,33,34,35] â†’ [-6,-5,-4,-3,-2,-1]
```

### 3. Loss Scaling ìˆ˜ì • (2ì‹œê°„ ì‘ì—…)
- Custom Trainer í´ë˜ìŠ¤ ì‘ì„±
- `compute_loss()` method override
- Loss scaling ë¡œì§ ì œê±°

---

## ğŸ“ ìµœì¢… ê²°ë¡ 

### ROOT CAUSE ë°œê²¬!

**Gradient Norm 200ë°° ì°¨ì´ì˜ ì›ì¸**:

1. **PRIMARY CAUSE (P0)**: **Loss Scaling ì°¨ì´**
   - TrainerëŠ” `loss / gradient_accumulation_steps` ìë™ ìˆ˜í–‰
   - Backupì€ ì›ë³¸ loss ê·¸ëŒ€ë¡œ backward
   - **ì˜í–¥**: 2ë°° gradient ì°¨ì´ (accumulation stepë‹¹)
   - **í•´ê²°**: Custom Trainerë¡œ loss scaling ì œê±° í•„ìš”

2. **SECONDARY CAUSE (P1)**: **Warmup Ratio ì°¨ì´**
   - Current: 0.01 (1% warmup) â†’ ì´ˆê¸° ë¶ˆì•ˆì •
   - Backup: 0.1 (10% warmup) â†’ ì•ˆì •ì  í•™ìŠµ
   - **ì˜í–¥**: ì´ˆê¸° gradient explosion ê°€ëŠ¥ì„± ì¦ê°€
   - **í•´ê²°**: warmup_ratioë¥¼ 0.1ë¡œ ì¦ê°€ (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

3. **TERTIARY CAUSE (P2)**: **LoRA Layers ì°¨ì´**
   - Current: 8 layers â†’ 33% ë” ë§ì€ íŒŒë¼ë¯¸í„°
   - Backup: 6 layers
   - **ì˜í–¥**: ë¯¸ë¯¸í•œ gradient ì¦ê°€
   - **í•´ê²°**: layers_to_transform í†µì¼ (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

### ê²€ì¦ ì™„ë£Œ í•­ëª©

- âœ… **ë°ì´í„°**: ë™ì¼
- âœ… **Learning Rate**: ë™ì¼ (1e-5)
- âœ… **Optimizer**: ë™ì¼ (AdamW, weight_decay=0.002)
- âœ… **Gradient Clipping**: ë™ì¼ (1.0)
- âœ… **Batch Size**: ë™ì¼ (16 Ã— 2 Ã— 4 = 128)
- âœ… **LoRA Config**: ê±°ì˜ ë™ì¼ (r=256, alpha=512)
- âš ï¸ **Warmup Ratio**: ì°¨ì´ ë°œê²¬ (0.1 vs 0.01)
- âš ï¸ **Loss Scaling**: ì°¨ì´ ë°œê²¬ (ì›ë³¸ vs ìë™ ë¶„í• )
- âš ï¸ **LoRA Layers**: ì°¨ì´ ë°œê²¬ (6 vs 8 layers)

### ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ ì‹¤í–‰**: Warmup ratio 0.1ë¡œ ìˆ˜ì • + LoRA layers í†µì¼
2. **í…ŒìŠ¤íŠ¸**: Gradient norm ì¸¡ì •
3. **Custom Trainer ê°œë°œ**: Loss scaling ì œê±° (í•„ìš” ì‹œ)
4. **ìµœì¢… ê²€ì¦**: Gradient norm < 1.0 ë‹¬ì„± í™•ì¸

---

**ì—…ë°ì´íŠ¸ ì´ë ¥**:
- 2025-10-10 14:00: ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±
- 2025-10-10 14:05: Phase 1 ì‹œì‘, LoRA layers ì°¨ì´ ë°œê²¬
- 2025-10-10 15:30: ì „ì²´ Phase ì™„ë£Œ, ROOT CAUSE ë°œê²¬!
  - Loss scaling ì°¨ì´ (CRITICAL)
  - Warmup ratio ì°¨ì´ (SIGNIFICANT)
  - LoRA layers ì°¨ì´ (MINOR)
- 2025-10-10 15:45: ì‹¤í—˜ ê³„íš ìˆ˜ë¦½ ë° ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ìˆ˜ì • ì •ë¦¬
