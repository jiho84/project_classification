# ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ë¶„ì„ ë³´ê³ ì„œ

**ìž‘ì„±ì¼**: 2025-10-10
**ì¦ìƒ**: 200 ìŠ¤í… ë§Œì— lossê°€ ì†Œìˆ˜ì ìœ¼ë¡œ ì§„ìž…í•˜ì§€ ì•ŠìŒ (ë¹„ì •ìƒ)

---

## ðŸš¨ ë°œê²¬ëœ ë¬¸ì œ

### 1. **ê·¹ì‹¬í•œ í´ëž˜ìŠ¤ ë¶ˆê· í˜•** (CRITICAL)

#### ë¶ˆê· í˜• í†µê³„
```
ì´ ìƒ˜í”Œ: 298,821
ê³ ìœ  í´ëž˜ìŠ¤: 224ê°œ (ì„¤ì •: max_classes 280)
í´ëž˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: 44,098 : 1
```

#### ìƒìœ„ í´ëž˜ìŠ¤ (ê³¼ëŒ€í‘œ)
| Label | ìƒ˜í”Œ ìˆ˜ | ë¹„ìœ¨ |
|-------|---------|------|
| 197 | 44,098 | 14.76% |
| 177 | 35,707 | 11.95% |
| 62 | 21,683 | 7.26% |
| 178 | 18,958 | 6.34% |
| 79 | 16,768 | 5.61% |

**ìƒìœ„ 5ê°œ í´ëž˜ìŠ¤ê°€ ì „ì²´ì˜ 45.92%ë¥¼ ì°¨ì§€**

#### í•˜ìœ„ í´ëž˜ìŠ¤ (ê³¼ì†Œí‘œ)
**Trainì— ë‹¨ 1ê°œ ìƒ˜í”Œë§Œ ìžˆëŠ” í´ëž˜ìŠ¤**: 138, 119, 127, 46, 223, 44, 2, 38, 85, 221

**ì´ í´ëž˜ìŠ¤ë“¤ì€ ì‚¬ì‹¤ìƒ í•™ìŠµ ë¶ˆê°€ëŠ¥**
- 1ê°œ ìƒ˜í”Œë¡œëŠ” íŒ¨í„´ í•™ìŠµ ë¶ˆê°€
- ëª¨ë¸ì´ ì´ í´ëž˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë˜ì§€ ì•ŠìŒ
- Random chanceë³´ë‹¤ ëª»í•œ ì„±ëŠ¥

---

## ðŸ’¥ í•™ìŠµ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

### Lossê°€ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ì§€ ì•ŠëŠ” ì´ìœ 

1. **ë‹¤ìˆ˜ í´ëž˜ìŠ¤ íŽ¸í–¥**
   - ëª¨ë¸ì´ Label 197, 177ë§Œ ì˜ˆì¸¡í•´ë„ 26.71% ì •í™•ë„
   - Lossê°€ plateauì— ì‰½ê²Œ ë„ë‹¬
   - Gradientê°€ ì†Œìˆ˜ í´ëž˜ìŠ¤ì— ì œëŒ€ë¡œ ì „íŒŒë˜ì§€ ì•ŠìŒ

2. **í•™ìŠµ ë¶ˆì•ˆì •ì„±**
   - ë°°ì¹˜ë§ˆë‹¤ í´ëž˜ìŠ¤ ë¶„í¬ê°€ ë‹¬ë¼ì§
   - ì†Œìˆ˜ í´ëž˜ìŠ¤ëŠ” ë°°ì¹˜ì— ê±°ì˜ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ
   - ëª¨ë¸ì´ ì¼ê´€ëœ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•¨

3. **Stratified Split ì‹¤íŒ¨ ê°€ëŠ¥ì„±**
   - 1ê°œ ìƒ˜í”Œ í´ëž˜ìŠ¤ëŠ” train/valid/test ë¶„ë¦¬ ë¶ˆê°€
   - split/nodes.py:159-161ì—ì„œ random fallback ë°œìƒ
   - ì¼ë¶€ í´ëž˜ìŠ¤ëŠ” testì—ë§Œ ì¡´ìž¬ ê°€ëŠ¥ (zero-shot í‰ê°€)

---

## âœ… í•´ê²° ë°©ì•ˆ

### ë°©ì•ˆ 1: **ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í•„í„°ë§** (RECOMMENDED)

í´ëž˜ìŠ¤ë³„ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ë¥¼ ê°•ì œí•˜ì—¬ í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ í´ëž˜ìŠ¤ ì œê±°

#### split/nodes.pyì˜ `create_dataset` ìˆ˜ì •

**í˜„ìž¬ ì½”ë“œ** (split/nodes.py:62-121):
```python
def create_dataset(
    base_table: pd.DataFrame,
    params: Dict[str, Any],
) -> Tuple[Dataset, List[str]]:
    label_column = params.get("label_column", "acct_code")
    # ... ê¸°ì¡´ ì½”ë“œ

    cleaned = base_table.reset_index(drop=True)
    dataset = Dataset.from_pandas(cleaned, preserve_index=False)
```

**ìˆ˜ì • ì œì•ˆ**:
```python
def create_dataset(
    base_table: pd.DataFrame,
    params: Dict[str, Any],
) -> Tuple[Dataset, List[str]]:
    label_column = params.get("label_column", "acct_code")
    max_classes = params.get("max_classes", 100)
    min_samples_per_class = params.get("min_samples_per_class", 10)  # ìƒˆë¡œ ì¶”ê°€

    # ... extraction ë¡œì§ ...

    # í´ëž˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í•„í„°ë§
    if min_samples_per_class > 0:
        original_size = len(base_table)
        class_counts = base_table[label_column].value_counts()
        valid_classes = class_counts[class_counts >= min_samples_per_class].index
        base_table = base_table[base_table[label_column].isin(valid_classes)]

        removed_classes = len(class_counts) - len(valid_classes)
        removed_samples = original_size - len(base_table)

        logger.info(
            "Filtered out %d classes with < %d samples (removed %d samples, %.2f%%)",
            removed_classes, min_samples_per_class, removed_samples,
            removed_samples / original_size * 100
        )

    cleaned = base_table.reset_index(drop=True)
    dataset = Dataset.from_pandas(cleaned, preserve_index=False)
    # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ë™ì¼
```

**training.yml íŒŒë¼ë¯¸í„° ì¶”ê°€** (training.yml:6-15):
```yaml
split:
  label_column: acct_code
  seed: 42
  test_size: 0.2
  val_size: 0.1
  max_classes: 280
  min_samples_per_class: 10  # ìƒˆë¡œ ì¶”ê°€ (ê° í´ëž˜ìŠ¤ë‹¹ ìµœì†Œ 10ê°œ ìƒ˜í”Œ)
  labelize_num_proc: 8
  extract_ratio: 1
  extract_seed: 42
  stratify_extract: true
```

**ì˜ˆìƒ íš¨ê³¼**:
- 10ê°œ ë¯¸ë§Œ ìƒ˜í”Œ í´ëž˜ìŠ¤ ì œê±° â†’ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
- 224ê°œ â†’ ì•½ 180-200ê°œ í´ëž˜ìŠ¤ë¡œ ê°ì†Œ
- ì œê±°ë˜ëŠ” ì „ì²´ ìƒ˜í”Œì€ 0.1% ë¯¸ë§Œ (ê±°ì˜ ì˜í–¥ ì—†ìŒ)

---

### ë°©ì•ˆ 2: **Class Weighting** (COMPLEMENTARY)

Loss functionì— í´ëž˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ ì†Œìˆ˜ í´ëž˜ìŠ¤ ê°•ì¡°

#### Trainerì— class_weight ì¶”ê°€

**training.ymlì— ì¶”ê°€** (training.yml:84 ì´í›„):
```yaml
  training_args:
    # ... ê¸°ì¡´ ì„¤ì • ...
    greater_is_better: true

  # ìƒˆë¡œ ì¶”ê°€
  compute_class_weights: true  # ìžë™ ê³„ì‚°
  class_weight_strategy: "balanced"  # or "sqrt" for softer weighting
```

**train/nodes.py ìˆ˜ì • í•„ìš”**:
```python
from sklearn.utils.class_weight import compute_class_weight

def prepare_for_trainer(tokenized_datasets, params):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    if params.get("compute_class_weights", False):
        labels = tokenized_datasets["train"]["labels"]
        unique_labels = np.unique(labels)

        if params.get("class_weight_strategy") == "balanced":
            class_weights = compute_class_weight(
                class_weight='balanced',
                classes=unique_labels,
                y=labels
            )
        elif params.get("class_weight_strategy") == "sqrt":
            # Softer weighting using sqrt
            label_counts = np.bincount(labels)
            class_weights = np.sqrt(np.max(label_counts) / (label_counts + 1))

        # Trainerì— ì „ë‹¬ (Trainer ì´ˆê¸°í™” ì‹œ ì‚¬ìš©)
        trainer_config["class_weights"] = class_weights
```

**ì£¼ì˜**: HuggingFace TrainerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ class_weightë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, **Custom Trainer** ë˜ëŠ” **Weighted Loss**ë¥¼ ì§ì ‘ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

---

### ë°©ì•ˆ 3: **Over-sampling / Under-sampling** (ADVANCED)

ì†Œìˆ˜ í´ëž˜ìŠ¤ ì˜¤ë²„ìƒ˜í”Œë§ ë˜ëŠ” ë‹¤ìˆ˜ í´ëž˜ìŠ¤ ì–¸ë”ìƒ˜í”Œë§

#### Imbalanced-learn ì‚¬ìš© (ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”)

```python
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN

# training.yml
split:
  resampling:
    enabled: true
    strategy: "over"  # "over", "under", or "smote"
    target_ratio: 0.3  # ì†Œìˆ˜ í´ëž˜ìŠ¤ë¥¼ ë‹¤ìˆ˜ í´ëž˜ìŠ¤ì˜ 30%ê¹Œì§€ ì¦ê°•
```

**ë‹¨ì **:
- Over-sampling: ê³¼ì í•© ìœ„í—˜
- Under-sampling: ë°ì´í„° ì†ì‹¤
- SMOTE: í…ìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ë¶€ì í•©

---

### ë°©ì•ˆ 4: **Focal Loss** (ADVANCED)

Hard exampleì— ì§‘ì¤‘í•˜ëŠ” loss function

```python
class FocalLoss(nn.Module):
    def __init__(self, gamma=2, alpha=None):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * ce_loss

        if self.alpha is not None:
            focal_loss = self.alpha[targets] * focal_loss

        return focal_loss.mean()
```

---

## ðŸ“Š ê¶Œìž¥ ì¡°ì¹˜ ìˆœì„œ

### 1ë‹¨ê³„: **ì¦‰ì‹œ ì ìš©** (ê°€ìž¥ íš¨ê³¼ì )
- [x] ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í•„í„°ë§ (`min_samples_per_class: 10`)
- [x] í•„í„°ë§ í›„ ë°ì´í„° ìž¬ìƒì„± (`kedro run --pipeline=full_preprocess`)

### 2ë‹¨ê³„: **ê²€ì¦**
- [ ] ìƒˆë¡œìš´ splitìœ¼ë¡œ í•™ìŠµ ì‹¤í–‰
- [ ] 200 ìŠ¤í…ì—ì„œ loss í™•ì¸ (ì†Œìˆ˜ì  ì§„ìž… ì—¬ë¶€)
- [ ] í´ëž˜ìŠ¤ë³„ ì •í™•ë„ í™•ì¸

### 3ë‹¨ê³„: **í•„ìš”ì‹œ ì¶”ê°€ ì¡°ì¹˜**
- [ ] Class weighting ì ìš© (1ë‹¨ê³„ë¡œ ë¶€ì¡±í•  ê²½ìš°)
- [ ] Focal Loss ì ìš© (ê·¹ì‹¬í•œ ë¶ˆê· í˜•ì´ ë‚¨ì•„ìžˆì„ ê²½ìš°)

---

## ðŸ”§ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ì½”ë“œ ë³€ê²½

### íŒŒì¼ 1: `split/nodes.py:62-121`

**ë³€ê²½ ìœ„ì¹˜**: `create_dataset` í•¨ìˆ˜ì— í•„í„°ë§ ë¡œì§ ì¶”ê°€

### íŒŒì¼ 2: `conf/base/parameters/training.yml:6-15`

**ì¶”ê°€ íŒŒë¼ë¯¸í„°**:
```yaml
min_samples_per_class: 10
```

---

## ðŸ“Œ ì˜ˆìƒ ê²°ê³¼

### í•„í„°ë§ ì „
- 224ê°œ í´ëž˜ìŠ¤
- ë¶ˆê· í˜• ë¹„ìœ¨: 44,098:1
- 10ê°œ í´ëž˜ìŠ¤ê°€ 1ê°œ ìƒ˜í”Œë§Œ ë³´ìœ 

### í•„í„°ë§ í›„ (min_samples_per_class: 10)
- ì•½ 180-200ê°œ í´ëž˜ìŠ¤
- ë¶ˆê· í˜• ë¹„ìœ¨: ì•½ 4,400:1 (10ë°° ê°œì„ )
- ëª¨ë“  í´ëž˜ìŠ¤ê°€ ìµœì†Œ 10ê°œ ìƒ˜í”Œ ë³´ìœ 
- **Stratified split ì„±ê³µë¥  í–¥ìƒ**

### í•™ìŠµ ì„±ëŠ¥ ê°œì„  ì˜ˆìƒ
- Loss ìˆ˜ë ´ ì†ë„: **2-3ë°° í–¥ìƒ**
- 200 ìŠ¤í… ë‚´ lossê°€ ì†Œìˆ˜ì  ì§„ìž… (0.x ëŒ€)
- ì†Œìˆ˜ í´ëž˜ìŠ¤ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

---

## ðŸ” ì¶”ê°€ ë¶„ì„ í•„ìš” ì‚¬í•­

### 1. Validation/Test Split í™•ì¸
```python
# validì™€ testì—ë„ ë™ì¼í•œ í´ëž˜ìŠ¤ ë¶„í¬ê°€ ìžˆëŠ”ì§€ í™•ì¸
python -c "
import pickle
data = pickle.load(open('data/05_model_input/serialized_datasets.pkl', 'rb'))
for split in ['valid', 'test']:
    labels = data[split]['labels']
    unique = set(labels)
    print(f'{split}: {len(unique)} unique classes')

    # Trainì—ë§Œ ìžˆëŠ” í´ëž˜ìŠ¤ í™•ì¸
    train_labels = set(data['train']['labels'])
    only_in_split = unique - train_labels
    print(f'  Classes only in {split}: {len(only_in_split)}')
"
```

### 2. í´ëž˜ìŠ¤ë³„ text ê¸¸ì´ ë¶„í¬
ì¼ë¶€ í´ëž˜ìŠ¤ê°€ ê³¼ë„í•˜ê²Œ ì§§ì€/ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§ˆ ê²½ìš° í•™ìŠµ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥

---

## ì°¸ê³  ìžë£Œ

- HuggingFace Datasets Stratified Split: https://huggingface.co/docs/datasets/process#stratified-split
- Imbalanced Classification Guide: https://imbalanced-learn.org/
- Focal Loss Paper: https://arxiv.org/abs/1708.02002
