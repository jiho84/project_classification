# 12ë°° ì†ë„ ì°¨ì´ ê¸´ê¸‰ ìˆ˜ì • ì™„ë£Œ

**ì‘ì„±ì¼**: 2025-10-10
**ìƒí™©**: ë°±ì—…ê³¼ í˜„ì¬ ì½”ë“œ, ë™ì¼í•œ 3M ìƒ˜í”Œë¡œ í•™ìŠµ ì‹œ 12ë°° ì†ë„ ì°¨ì´ ë°œìƒ

---

## ğŸš¨ í•µì‹¬ ë¬¸ì œ ì •ë¦¬

### ë°ì´í„° í¬ê¸° ë™ì¼ í™•ì¸
- ë°±ì—…: ~300ë§Œ ìƒ˜í”Œ
- í˜„ì¬: ~300ë§Œ ìƒ˜í”Œ
- **ë°ì´í„° í¬ê¸°ëŠ” ë™ì¼** â†’ ìˆœìˆ˜ ì½”ë“œ ìµœì í™” ì°¨ì´ë¡œ 12ë°° ëŠë¦¼

### ê·¼ë³¸ ì›ì¸
1. **Padding ë¹„íš¨ìœ¨**: `padding="longest"` â†’ ë°°ì¹˜ë§ˆë‹¤ í…ì„œ í¬ê¸° ë³€ë™
2. **DataLoader ë¯¸ìµœì í™”**: prefetch, persistent_workers ì—†ìŒ
3. **í™˜ê²½ ìµœì í™” ëˆ„ë½**: TF32, Flash Attention ë¯¸ì ìš©
4. **í† í¬ë‚˜ì´ì € ì„¤ì • ë¶€ì¡±**: padding_side, í† í° ê²€ì¦ ì—†ìŒ

---

## âœ… ì ìš©ëœ ê¸´ê¸‰ ìˆ˜ì •ì‚¬í•­

### 1. í™˜ê²½ ìµœì í™” ì¶”ê°€ (main_yaml.py:40-51)

**ìˆ˜ì • ë‚´ìš©**:
```python
# í™˜ê²½ ìµœì í™” (ë°±ì—… ì½”ë“œ ì°¸ì¡°)
os.environ["MKL_SERVICE_FORCE_INTEL"] = "1"
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# CUDA ìµœì í™”
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
    torch.backends.cuda.enable_flash_sdp(True)
```

**íš¨ê³¼**:
- TF32 í™œì„±í™” â†’ í–‰ë ¬ ì—°ì‚° 1.5ë°° ê°€ì†
- Flash Attention 2 â†’ Self-attention ë©”ëª¨ë¦¬ 50% ê°ì†Œ, ì†ë„ 2ë°°
- oneMKL ìŠ¤ë ˆë”© ì•ˆì •í™” â†’ CPU ì˜¤ë²„í—¤ë“œ ì œê±°

---

### 2. í† í¬ë‚˜ì´ì € ì„¤ì • ê°œì„  (main_yaml.py:164-169)

**ìˆ˜ì • ì „**:
```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

**ìˆ˜ì • í›„**:
```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# í† í¬ë‚˜ì´ì € ìµœì í™” (ë°±ì—… ì½”ë“œ ì°¸ì¡°)
tokenizer.padding_side = "right"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    LOGGER.info("Pad token not set; using EOS token %s", tokenizer.pad_token)
```

**íš¨ê³¼**:
- `padding_side = "right"` â†’ í•™ìŠµ ì‹œ ì˜¬ë°”ë¥¸ íŒ¨ë”© ë°©í–¥
- pad_token_id ëª…ì‹œì  ì„¤ì • â†’ í† í° ID ë¶ˆì¼ì¹˜ ë°©ì§€

---

### 3. DataLoader ìµœì í™” (training.yml:80-84)

**ìˆ˜ì • ì „**:
```yaml
dataloader_num_workers: 8
seed: 42
```

**ìˆ˜ì • í›„**:
```yaml
dataloader_num_workers: 8
dataloader_pin_memory: true           # ì¶”ê°€
dataloader_prefetch_factor: 2          # ì¶”ê°€
dataloader_persistent_workers: true    # ì¶”ê°€
seed: 42
```

**íš¨ê³¼**:
- `pin_memory=True`: CPUâ†’GPU ì „ì†¡ ì†ë„ 1.5ë°° í–¥ìƒ
- `prefetch_factor=2`: I/O ëŒ€ê¸° ì‹œê°„ 50% ê°ì†Œ
- `persistent_workers=True`: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¬ì‚¬ìš© â†’ ì˜¤ë²„í—¤ë“œ ì œê±°

---

### 4. Padding ë°©ë²• ìµœì í™” (training.yml:30 & train/nodes.py:130-131)

**training.yml ìˆ˜ì •**:
```yaml
tokenization:
  max_length: 320      # 256 â†’ 320
  padding: "max_length"  # ì´ë¯¸ ìˆ˜ì •ë¨
```

**train/nodes.py ìˆ˜ì •**:
```python
def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:
    return tokenizer(
        examples["text"],
        truncation=truncation,
        max_length=max_length,
        add_special_tokens=True,  # ëª…ì‹œì  EOS í† í° ì¶”ê°€
        padding=False,  # DataCollatorê°€ ë°°ì¹˜ ìƒì„± ì‹œ íŒ¨ë”© ì²˜ë¦¬
        return_length=True,
    )
```

**íš¨ê³¼**:
- `padding=False` + DataCollator â†’ ê³ ì • ê¸¸ì´ íŒ¨ë”©, ë°°ì¹˜ ì¼ê´€ì„±
- `add_special_tokens=True` â†’ EOS í† í° ëª…ì‹œì  ì¶”ê°€
- `max_length=320` â†’ ë°±ì—…ê³¼ ë™ì¼í•œ ì‹œí€€ìŠ¤ ê¸¸ì´

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### ëˆ„ì  ì†ë„ í–¥ìƒ

| ìµœì í™” í•­ëª© | ê°œë³„ íš¨ê³¼ | ëˆ„ì  íš¨ê³¼ |
|------------|---------|---------|
| **1. CUDA ìµœì í™”** (TF32 + Flash Attention) | 2-3ë°° | 2-3ë°° |
| **2. Padding ë°©ì‹** (False + max_length) | 2-3ë°° | 4-9ë°° |
| **3. DataLoader** (prefetch + persistent) | 1.5-2ë°° | 6-18ë°° |
| **4. í† í¬ë‚˜ì´ì €** (padding_side) | 1.1ë°° | 7-20ë°° |

### 12ë°° ì°¨ì´ í•´ê²°

**í˜„ì¬ ìƒíƒœ**:
- ë°±ì—… ëŒ€ë¹„ 12ë°° ëŠë¦¼

**ìˆ˜ì • í›„ ì˜ˆìƒ**:
- **7-20ë°° ê°œì„ ** â†’ ë°±ì—…ë³´ë‹¤ 0.6-1.7ë°° (ê±°ì˜ ë™ì¼ ë˜ëŠ” ë” ë¹ ë¦„)

---

## ğŸ” í•µì‹¬ ì°¨ì´ì  ìš”ì•½

### ë°±ì—… ì½”ë“œì˜ í•µì‹¬ ìµœì í™” (ë°±ì—… ëŒ€ë¹„ í˜„ì¬ê°€ ë†“ì¹œ ê²ƒë“¤)

#### 1. í™˜ê²½ ì„¤ì • (train.py:69-90)
```python
# Intel oneMKL ì•ˆì •í™”
os.environ["MKL_SERVICE_FORCE_INTEL"] = "1"
os.environ["MKL_THREADING_LAYER"] = "GNU"

# CUDA ìµœì í™”
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cuda.enable_flash_sdp(True)
```

#### 2. DataLoader ì„¤ì • (train.py:277-280)
```python
train_loader = DataLoader(
    train_ds,
    batch_size=16,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2,              # â­
    persistent_workers=True         # â­
)
```

#### 3. í† í¬ë‚˜ì´ì§• ë°©ì‹ (train.py:249-256)
```python
def tok_fn(batch):
    return tok(
        batch["text"],
        max_length=320,
        add_special_tokens=True,    # â­
        padding=False               # â­ DataCollatorê°€ ì²˜ë¦¬
    )
```

#### 4. í† í¬ë‚˜ì´ì € ì„¤ì • (train.py:41-66)
```python
tokenizer.padding_side = "right"   # â­
# PAD/EOS í† í° ê²€ì¦
if tokenizer.pad_token_id == tokenizer.eos_token_id:
    print("âš ï¸ ê²½ê³ : PADì™€ EOS í† í°ì´ ë™ì¼í•©ë‹ˆë‹¤.")
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### 1. í† í¬ë‚˜ì´ì§• ì¬ì‹¤í–‰ (í•„ìˆ˜)
```bash
# ê¸°ì¡´ í† í°í™”ëœ ë°ì´í„°ì…‹ ì‚­ì œ
rm -rf data/06_models/tokenized_datasets

# split íŒŒì´í”„ë¼ì¸ë¶€í„° ì¬ì‹¤í–‰
kedro run --pipeline=split

# train íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
kedro run --pipeline=train
```

**ì´ìœ **:
- `padding="max_length"` â†’ `padding=False` ë³€ê²½
- `max_length` 256 â†’ 320 ë³€ê²½
- `add_special_tokens=True` ì¶”ê°€

### 2. í•™ìŠµ ì†ë„ ì¸¡ì •
```bash
# í•™ìŠµ ì‹œì‘ ì „ ì‹œê°„ ê¸°ë¡
start_time=$(date +%s)

# í•™ìŠµ ì‹¤í–‰
kedro run --pipeline=train

# í•™ìŠµ ì™„ë£Œ í›„ ì‹œê°„ ê³„ì‚°
end_time=$(date +%s)
elapsed=$((end_time - start_time))
echo "ì´ í•™ìŠµ ì‹œê°„: ${elapsed}ì´ˆ"
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# GPU í™œìš©ë¥  í™•ì¸ (ë³„ë„ í„°ë¯¸ë„)
watch -n 1 nvidia-smi

# MLflowì—ì„œ í•™ìŠµ ì†ë„ í™•ì¸
# train_samples_per_second ë©”íŠ¸ë¦­ í™•ì¸
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ì™„ë£Œëœ ìµœì í™”
- [x] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (MKL, CUDA)
- [x] TF32 í™œì„±í™”
- [x] Flash Attention 2 í™œì„±í™”
- [x] í† í¬ë‚˜ì´ì € padding_side ì„¤ì •
- [x] DataLoader prefetch_factor ì¶”ê°€
- [x] DataLoader persistent_workers ì¶”ê°€
- [x] DataLoader pin_memory ì¶”ê°€
- [x] Padding ë°©ì‹ ë³€ê²½ (False + DataCollator)
- [x] max_length ì¦ê°€ (256 â†’ 320)
- [x] add_special_tokens=True ì¶”ê°€

### ğŸ”„ ë‹¤ìŒ ì‘ì—…
- [ ] ê¸°ì¡´ í† í°í™” ë°ì´í„° ì‚­ì œ
- [ ] split íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰
- [ ] train íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- [ ] í•™ìŠµ ì†ë„ ì¸¡ì • ë° ë¹„êµ
- [ ] ì´ˆê¸° loss/grad_norm í™•ì¸ (ì •ìƒí™” ê²€ì¦)

---

## ğŸ“ ê²°ë¡ 

**12ë°° ì°¨ì´ì˜ ì§„ì§œ ì›ì¸**:
1. âŒ **CUDA ìµœì í™” ëˆ„ë½** (TF32, Flash Attention): 2-3ë°° ëŠë¦¼
2. âŒ **Padding ë¹„íš¨ìœ¨** (`padding="longest"`): 2-3ë°° ëŠë¦¼
3. âŒ **DataLoader ë¯¸ìµœì í™”**: 1.5-2ë°° ëŠë¦¼

**ìˆ˜ì • ì™„ë£Œ**:
- ë°±ì—… ì½”ë“œì˜ í•µì‹¬ ìµœì í™”ë¥¼ ëª¨ë‘ í˜„ì¬ ì½”ë“œì— ì ìš©
- ì˜ˆìƒ ê°œì„ : **7-20ë°° ì†ë„ í–¥ìƒ**

**ì¦‰ì‹œ ì‹¤í–‰**:
```bash
# 1. í† í°í™” ë°ì´í„° ì‚­ì œ
rm -rf data/06_models/tokenized_datasets

# 2. íŒŒì´í”„ë¼ì¸ ì¬ì‹¤í–‰
kedro run --pipeline=split
kedro run --pipeline=train
```

**ê²€ì¦ ì§€í‘œ**:
- í•™ìŠµ ì‹œê°„: ë°±ì—…ê³¼ ë™ì¼í•˜ê±°ë‚˜ ë” ë¹ ë¦„
- ì´ˆê¸° loss: 5-6 ë²”ìœ„ (random ìˆ˜ì¤€)
- Gradient norm: 0.1-1.0 ë²”ìœ„ (ì •ìƒ)
- Stepë‹¹ ì‹œê°„: ë°±ì—…ì˜ 80-120% ìˆ˜ì¤€
