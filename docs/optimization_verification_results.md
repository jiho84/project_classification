# ìµœì í™” ê²€ì¦ ê²°ê³¼

**ì‘ì„±ì¼**: 2025-10-10
**ìƒí™©**: 12ë°° ì†ë„ ì°¨ì´ ìˆ˜ì • í›„ ì²« í•™ìŠµ ì‹œë„

---

## ğŸ¯ ê²€ì¦ ê²°ê³¼

### ì´ˆê¸° í•™ìŠµ ë©”íŠ¸ë¦­ (Step 10-20)

```
Step 1:  loss=69.53, grad_norm=196.35, lr=1.28e-08
Step 2:  loss=69.25, grad_norm=205.19, lr=2.71e-08
Step 3:  loss=69.33, grad_norm=207.80, lr=4.14e-08
Step 4:  loss=69.20, grad_norm=206.96, lr=5.57e-08
Step 5:  loss=69.80, grad_norm=209.51, lr=7.00e-08
Step 6:  loss=69.71, grad_norm=204.16, lr=8.42e-08
Step 7:  loss=67.83, grad_norm=203.12, lr=9.85e-08
Step 8:  loss=68.18, grad_norm=213.86, lr=1.13e-07
Step 9:  loss=69.08, grad_norm=203.22, lr=1.27e-07
Step 10: loss=69.67, grad_norm=190.84, lr=1.41e-07
...
Step 20: loss=64.19, grad_norm=157.34, lr=2.84e-07
```

---

## ğŸ“Š ë¬¸ì œ ë¶„ì„

### âŒ ì—¬ì „íˆ ë¬¸ì œ ìˆëŠ” ì§€í‘œ

#### 1. **ì´ˆê¸° Loss ì—¬ì „íˆ ë§¤ìš° ë†’ìŒ**
- **ê¸°ëŒ€ê°’**: 5-6 (random ìˆ˜ì¤€, log(280) â‰ˆ 5.63)
- **ì‹¤ì œê°’**: **69.5**
- **ì°¨ì´**: 12ë°° ë†’ìŒ (ì—¬ì „íˆ ë¹„ì •ìƒ)

#### 2. **Gradient Norm ì—¬ì „íˆ í­ë°œ**
- **ì •ìƒ ë²”ìœ„**: 0.1-1.0
- **ì‹¤ì œê°’**: **196-214** (ì—¬ì „íˆ 200ë°° ë†’ìŒ)
- **Gradient Clipping**: 99.5% ì˜ë¦¼ (1.0 / 196 = 0.5%)

#### 3. **Learning Rateê°€ ë„ˆë¬´ ë‚®ìŒ**
- Warmup êµ¬ê°„ (700 steps): step 20ì—ì„œ lr = 2.84e-07
- ëª©í‘œ LR (1e-5)ì˜ **0.003%ë§Œ ë„ë‹¬**

---

## ğŸ” ê·¼ë³¸ ì›ì¸ ì¬ë¶„ì„

### ì´ì „ ë¶„ì„ì˜ ì˜¤ë¥˜

**ì´ì „ ê°€ì„¤** (í‹€ë¦¼):
- Padding ë°©ì‹ ì°¨ì´ â†’ í•™ìŠµ ì†ë„ ì°¨ì´ âœ… (ë§ìŒ)
- í™˜ê²½ ìµœì í™” ë¶€ì¡± â†’ í•™ìŠµ ì†ë„ ì°¨ì´ âœ… (ë§ìŒ)
- ì´ê²ƒë“¤ì´ gradient explosion í•´ê²° âŒ (í‹€ë¦¼!)

**ìƒˆë¡œìš´ ë°œê²¬**:
- **Padding ìµœì í™”ëŠ” ì†ë„ë§Œ ê°œì„ **, gradient explosionì€ í•´ê²° ì•ˆ ë¨
- **ì´ˆê¸° Loss 69.5ëŠ” ì†ë„ì™€ ë¬´ê´€í•œ ë‹¤ë¥¸ ë¬¸ì œ**

---

## ğŸ’¥ ì§„ì§œ ë¬¸ì œ: ë°ì´í„° ë¬¸ì œ

### ê°€ì„¤ 1: Padding Token ë¬¸ì œ

#### ì¦ê±°
ë°±ì—… ì½”ë“œ (train.py:249-256):
```python
def tok_fn(batch):
    return tok(
        batch["text"],
        truncation=True,
        max_length=320,
        add_special_tokens=True,
        padding=False  # â† DataCollatorê°€ ë°°ì¹˜ ì‹œ íŒ¨ë”©
    )
```

í˜„ì¬ ì½”ë“œ (train/nodes.py:125-133):
```python
def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:
    return tokenizer(
        examples["text"],
        truncation=truncation,
        max_length=max_length,
        add_special_tokens=True,
        padding=False,  # â† ë°±ì—…ê³¼ ë™ì¼
        return_length=True,
    )
```

**BUT**: í† í°í™”ëœ ë°ì´í„°ì…‹ í†µê³„
```
train: 2988208 samples, avg text chars: 534.9, max: 620
train: count=2988208, mean=211.5, max=287 (í† í° ê¸¸ì´)
```

**ë¬¸ì œ**:
- `padding=False`ë¡œ í† í¬ë‚˜ì´ì§•í–ˆì§€ë§Œ
- **ì´ë¯¸ í† í°í™”ëœ ë°ì´í„°ì…‹ì—ëŠ” paddingì´ ì—†ìŒ**
- DataCollatorWithPaddingì´ ë°°ì¹˜ ì‹œ íŒ¨ë”©í•˜ì§€ë§Œ, **ì´ˆê¸° lossê°€ ë†’ì€ ê²ƒê³¼ëŠ” ë¬´ê´€**

---

### ê°€ì„¤ 2: Classification Head ì´ˆê¸°í™” (ì¬ê²€í† )

#### ì´ì „ í…ŒìŠ¤íŠ¸ ê²°ê³¼
```python
# ë‹¨ì¼ ìƒ˜í”Œ forward pass
Weight std: 0.019893  # ì •ìƒ âœ…
Single sample loss: 6.2  # ì •ìƒ âœ…

# ë°°ì¹˜ forward pass
ERROR: Cannot handle batch sizes > 1 if no padding token is defined
```

**ìƒˆë¡œìš´ ì˜ë¬¸**:
- ë‹¨ì¼ ìƒ˜í”Œì—ì„œëŠ” loss=6.2 (ì •ìƒ)
- ì‹¤ì œ í•™ìŠµì—ì„œëŠ” loss=69.5 (ë¹„ì •ìƒ)
- **ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë¬¸ì œê°€ ë°œìƒ**í•˜ëŠ” ê²ƒ!

---

### ê°€ì„¤ 3: DataCollator ë¬¸ì œ

#### í˜„ì¬ ì„¤ì • (main_yaml.py:185-188)
```python
collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
)
```

#### ë°±ì—… ì„¤ì • (train.py:269)
```python
collate = DataCollatorWithPadding(tok)
# pad_to_multiple_of ì—†ìŒ
```

**ì˜ì‹¬**:
- `pad_to_multiple_of=8`: bfloat16 ì‚¬ìš© ì‹œ 8ì˜ ë°°ìˆ˜ë¡œ íŒ¨ë”©
- ì´ê²ƒì´ **ê³¼ë„í•œ íŒ¨ë”©**ì„ ìœ ë°œí•˜ì—¬ loss í­ë°œ?

**ê²€ì¦ í•„ìš”**:
- DataCollatorê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ íŒ¨ë”©í•˜ëŠ”ì§€ í™•ì¸
- Attention maskê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸

---

### ê°€ì„¤ 4: í† í¬ë‚˜ì´ì € vocab ë¬¸ì œ

#### Qwen3-4B í† í¬ë‚˜ì´ì € íŠ¹ì„±
```python
# ë°±ì—… ì½”ë“œì—ì„œ í™•ì¸
print(f"PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")
print(f"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")

# Qwen3ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ PADì™€ EOSê°€ êµ¬ë¶„ë˜ì–´ ìˆìŒ
if tokenizer.pad_token_id == tokenizer.eos_token_id:
    print("âš ï¸ ê²½ê³ : PADì™€ EOS í† í°ì´ ë™ì¼í•©ë‹ˆë‹¤.")
```

**í˜„ì¬ ì½”ë“œ**:
```python
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

**ë¬¸ì œ ê°€ëŠ¥ì„±**:
- Qwen3-4BëŠ” ì›ë˜ PAD í† í°ì´ ìˆìŒ
- í•˜ì§€ë§Œ ìœ„ ì½”ë“œê°€ **EOSë¡œ ë®ì–´ì”€**
- ì´ë¡œ ì¸í•´ **EOSì™€ PADê°€ ë™ì¼**í•´ì ¸ í˜¼ë€ ë°œìƒ

---

## ğŸ¯ ì¦‰ì‹œ í™•ì¸ ì‚¬í•­

### 1. í† í¬ë‚˜ì´ì € ìƒíƒœ í™•ì¸

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-4B", trust_remote_code=True)
print(f"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
print(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
print(f"Vocab size: {tokenizer.vocab_size}")
print(f"Same? {tokenizer.pad_token_id == tokenizer.eos_token_id}")
```

**ì˜ˆìƒ**:
- Qwen3-4BëŠ” PADì™€ EOSê°€ ì›ë˜ êµ¬ë¶„ë¨
- í˜„ì¬ ì½”ë“œê°€ ì˜ëª» í†µì¼ì‹œì¼°ì„ ê°€ëŠ¥ì„±

---

### 2. ë°°ì¹˜ ë°ì´í„° í™•ì¸

í•™ìŠµ ì‹œì‘ ì§í›„ ì²« ë°°ì¹˜ í™•ì¸:
```python
# Trainer ë‚´ë¶€ì—ì„œ ì²« ë°°ì¹˜ ì¶œë ¥
first_batch = next(iter(train_dataloader))
print(f"input_ids shape: {first_batch['input_ids'].shape}")
print(f"attention_mask shape: {first_batch['attention_mask'].shape}")
print(f"labels shape: {first_batch['labels'].shape}")

# íŒ¨ë”© í™•ì¸
print(f"input_ids[0]: {first_batch['input_ids'][0]}")
print(f"attention_mask[0]: {first_batch['attention_mask'][0]}")
```

**í™•ì¸ ì‚¬í•­**:
- input_idsê°€ ê³¼ë„í•˜ê²Œ íŒ¨ë”©ë˜ì—ˆëŠ”ì§€
- attention_maskê°€ ì˜¬ë°”ë¥¸ì§€
- labelsê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ëŠ”ì§€

---

### 3. Model forward pass ë””ë²„ê¹…

```python
model.eval()
with torch.no_grad():
    outputs = model(**first_batch)
    logits = outputs.logits
    print(f"Logits shape: {logits.shape}")
    print(f"Logits mean: {logits.mean().item()}")
    print(f"Logits std: {logits.std().item()}")
    print(f"Logits min: {logits.min().item()}")
    print(f"Logits max: {logits.max().item()}")
```

**ì •ìƒ ë²”ìœ„**:
- mean: ~0
- std: 1-10
- min/max: -50 ~ +50

**ë¹„ì •ìƒ (ì˜ì‹¬)**:
- std > 50 â†’ ì´ˆê¸°í™” ë¬¸ì œ
- mean >> 0 â†’ í¸í–¥ ë¬¸ì œ

---

## ğŸš¨ ê¸´ê¸‰ ì¡°ì¹˜ ì‚¬í•­

### 1. í† í¬ë‚˜ì´ì € ìˆ˜ì • (main_yaml.py)

**í˜„ì¬** (main_yaml.py:165-169):
```python
tokenizer.padding_side = "right"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    LOGGER.info("Pad token not set; using EOS token %s", tokenizer.pad_token)
```

**ìˆ˜ì •** (ë°±ì—… ë°©ì‹):
```python
tokenizer.padding_side = "right"

# Qwen3ëŠ” ì›ë˜ PAD í† í°ì´ ìˆìœ¼ë¯€ë¡œ ì¡°ê±´ í™•ì¸
if tokenizer.pad_token_id is None:
    LOGGER.warning("âš ï¸ PAD token not set; using EOS token")
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
else:
    LOGGER.info(f"âœ… PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    LOGGER.info(f"âœ… EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    if tokenizer.pad_token_id == tokenizer.eos_token_id:
        LOGGER.warning("âš ï¸ ê²½ê³ : PADì™€ EOS í† í°ì´ ë™ì¼í•©ë‹ˆë‹¤. ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„± ìˆìŒ.")
```

---

### 2. DataCollator ìˆ˜ì • (main_yaml.py:185-188)

**í˜„ì¬**:
```python
collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
)
```

**ìˆ˜ì •** (ë°±ì—…ê³¼ ë™ì¼):
```python
collator = DataCollatorWithPadding(tokenizer=tokenizer)
# pad_to_multiple_of ì œê±° (ê³¼ë„í•œ íŒ¨ë”© ë°©ì§€)
```

---

### 3. í† í¬ë‚˜ì´ì§• ì¬ì‹¤í–‰ í•„ìš”

ë³€ê²½ì‚¬í•­:
- train/nodes.pyì˜ tokenize_functionì€ ì´ë¯¸ `padding=False` âœ…
- í•˜ì§€ë§Œ **í† í¬ë‚˜ì´ì € ì„¤ì •**ì´ ë‹¬ë¼ì¡Œìœ¼ë¯€ë¡œ ì¬ì‹¤í–‰ í•„ìš”

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ì™„ë£Œëœ ìµœì í™”
- [x] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (MKL, CUDA)
- [x] TF32, Flash Attention í™œì„±í™”
- [x] DataLoader ìµœì í™” (prefetch, persistent_workers)
- [x] Padding ë°©ì‹ ë³€ê²½ (False + DataCollator)
- [x] max_length ì¦ê°€ (320)
- [x] add_special_tokens=True

### âŒ ì—¬ì „íˆ ë¬¸ì œ
- [ ] ì´ˆê¸° loss 69.5 (ê¸°ëŒ€: 5-6)
- [ ] Gradient norm 196 (ê¸°ëŒ€: 0.1-1.0)
- [ ] í•™ìŠµì´ ê±°ì˜ ì§„í–‰ ì•ˆ ë¨

### ğŸ”„ ë‹¤ìŒ ì‘ì—…
- [ ] í† í¬ë‚˜ì´ì € PAD/EOS í† í° í™•ì¸
- [ ] DataCollator pad_to_multiple_of ì œê±°
- [ ] ë°°ì¹˜ ë°ì´í„° ë””ë²„ê¹…
- [ ] ëª¨ë¸ forward pass í™•ì¸
- [ ] ì¬í•™ìŠµ í›„ ê²€ì¦

---

## ğŸ“ ê²°ë¡ 

**ì†ë„ ìµœì í™”**: âœ… ì„±ê³µ
- í™˜ê²½ ìµœì í™”, DataLoader ìµœì í™” ì ìš© ì™„ë£Œ
- í•™ìŠµ ì†ë„ëŠ” ê°œì„ ë˜ì—ˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ

**Loss/Gradient ë¬¸ì œ**: âŒ ë¯¸í•´ê²°
- ì´ˆê¸° loss 69.5 (ì—¬ì „íˆ 12ë°° ë†’ìŒ)
- Gradient norm 196 (ì—¬ì „íˆ 200ë°° ë†’ìŒ)
- **ìƒˆë¡œìš´ ê·¼ë³¸ ì›ì¸ íƒìƒ‰ í•„ìš”**:
  1. í† í¬ë‚˜ì´ì € PAD/EOS í† í° í†µì¼ ë¬¸ì œ (ìµœìš°ì„ )
  2. DataCollator pad_to_multiple_of ê³¼ë„í•œ íŒ¨ë”©
  3. ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ë°ì´í„° ë³€í™˜ ë¬¸ì œ
  4. Model configì™€ í† í¬ë‚˜ì´ì € ë¶ˆì¼ì¹˜

**ì¦‰ì‹œ ì¡°ì¹˜**:
1. í† í¬ë‚˜ì´ì € ìˆ˜ì • (PAD/EOS êµ¬ë¶„ í™•ì¸)
2. DataCollator ë‹¨ìˆœí™” (pad_to_multiple_of ì œê±°)
3. ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€ (ë°°ì¹˜ ë°ì´í„° í™•ì¸)
4. ì¬í•™ìŠµ í›„ ê²€ì¦
