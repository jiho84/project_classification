# í•™ìŠµ íŒŒë¼ë¯¸í„° ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025-10-10
**ëª©ì **: í˜„ì¬ ì„¤ì •(`training.yml`)ê³¼ ë°±ì—… ì„¤ì •(`deepspeed.yml`) ê°„ í•™ìŠµ ì„±ëŠ¥ ì°¨ì´ ì›ì¸ ë¶„ì„

---

## ğŸ“Š ì£¼ìš” ì°¨ì´ì  ìš”ì•½

| êµ¬ë¶„ | í˜„ì¬ (training.yml) | ë°±ì—… (deepspeed.yml) | ì„±ëŠ¥ ì˜í–¥ë„ |
|------|-------------------|-------------------|----------|
| **LoRA r** | 128 | 256 | âš ï¸ **HIGH** |
| **LoRA alpha** | 256 | 512 | âš ï¸ **HIGH** |
| **Layers to transform** | [28-35] (ë§ˆì§€ë§‰ 8ê°œ) | [-6 to -1] (ë§ˆì§€ë§‰ 6ê°œ) | âš ï¸ **MEDIUM** |
| **Learning Rate** | 2.0e-5 | 1.0e-5 | âš ï¸ **HIGH** |
| **Batch Size** | 32 | 16 | âš ï¸ **HIGH** |
| **Gradient Accumulation** | 1 | 2 | âš ï¸ **MEDIUM** |
| **Weight Decay** | 0.01 | 0.002 | âš ï¸ **MEDIUM** |
| **Epochs** | 3 | 10 | âš ï¸ **HIGH** |
| **DeepSpeed ZeRO** | Stage 2 | Stage 2 | - |
| **Optimizer Offload** | None | CPU | âš ï¸ **LOW** |
| **Max Length** | 256 | 320 | âš ï¸ **LOW** |
| **LR Scheduler** | - | WarmupCosineLR | âš ï¸ **MEDIUM** |

---

## ğŸ” ìƒì„¸ ë¹„êµ ë° ì„±ëŠ¥ ì˜í–¥ ë¶„ì„

### 1. **LoRA íŒŒë¼ë¯¸í„° ì°¨ì´** âš ï¸ CRITICAL

#### í˜„ì¬ ì„¤ì • (training.yml:38-46)
```yaml
lora_defaults:
  r: 128
  lora_alpha: 256
  layers_to_transform: [28, 29, 30, 31, 32, 33, 34, 35]  # 8 layers
```

#### ë°±ì—… ì„¤ì • (deepspeed.yml:11-19)
```yaml
lora_defaults:
  r: 256
  lora_alpha: 512
  layers_to_transform: [-6, -5, -4, -3, -2, -1]  # 6 layers
```

**ì„±ëŠ¥ ì˜í–¥ ë¶„ì„**:
- **LoRA r (rank)**: 128 â†’ 256
  - rì´ í´ìˆ˜ë¡ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€ â†’ **í‘œí˜„ë ¥ í–¥ìƒ**
  - r=256ì€ r=128 ëŒ€ë¹„ **2ë°° ë§ì€ íŒŒë¼ë¯¸í„°** í•™ìŠµ
  - ë‹¨, ê³¼ì í•© ìœ„í—˜ë„ ì¦ê°€

- **LoRA alpha**: 256 â†’ 512
  - alpha/r ë¹„ìœ¨: 2.0 â†’ 2.0 (ë™ì¼)
  - ì ˆëŒ€ê°’ ì¦ê°€ë¡œ **LoRA ì ì‘ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ ì¦ê°€** â†’ ë” ê³µê²©ì ì¸ í•™ìŠµ

- **Layers to transform**: 8ê°œ â†’ 6ê°œ
  - ë°±ì—… ì„¤ì •ì€ **ë§ˆì§€ë§‰ 6ê°œ ë ˆì´ì–´ë§Œ** í•™ìŠµ (ë” ì§‘ì¤‘ì )
  - í˜„ì¬ ì„¤ì •ì€ ë§ˆì§€ë§‰ 8ê°œ ë ˆì´ì–´ í•™ìŠµ (ë” ë„“ì€ ë²”ìœ„)

---

### 2. **Learning Rate ì°¨ì´** âš ï¸ CRITICAL

| ì„¤ì • | Learning Rate | ì˜í–¥ |
|------|--------------|------|
| í˜„ì¬ | 2.0e-5 | ë¹ ë¥¸ ìˆ˜ë ´, ë¶ˆì•ˆì • ìœ„í—˜ |
| ë°±ì—… | 1.0e-5 | ì•ˆì •ì  í•™ìŠµ, ëŠë¦° ìˆ˜ë ´ |

**ì„±ëŠ¥ ì˜í–¥**:
- í˜„ì¬ LR(2e-5)ì€ ë°±ì—…(1e-5)ì˜ **2ë°°** â†’ í•™ìŠµ ì†ë„ 2ë°° ë¹ ë¦„
- í•˜ì§€ë§Œ **ê³¼ë„í•œ LR**ë¡œ ì¸í•œ ë°œì‚° ìœ„í—˜ ì¦ê°€
- ë°±ì—… ì„¤ì •ì˜ 1e-5ëŠ” **ë” ì•ˆì •ì ì¸ ìˆ˜ë ´** ì œê³µ

---

### 3. **Batch Size & Gradient Accumulation** âš ï¸ HIGH

#### ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°

**í˜„ì¬ ì„¤ì • (training.yml:63-65, 93-94)**:
```
per_device_train_batch_size: 32
gradient_accumulation_steps: 1
num_gpus: 4
â†’ Effective Batch Size = 32 Ã— 1 Ã— 4 = 128
```

**ë°±ì—… ì„¤ì • (deepspeed.yml:30-31)**:
```
train_micro_batch_size_per_gpu: 16
gradient_accumulation_steps: 2
num_gpus: 4 (ì•”ë¬µì )
â†’ Effective Batch Size = 16 Ã— 2 Ã— 4 = 128
```

**ê²°ë¡ **:
- **ìœ íš¨ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” ë™ì¼(128)** â†’ ë°°ì¹˜ í¬ê¸° ìì²´ëŠ” ì„±ëŠ¥ ì°¨ì´ ì›ì¸ ì•„ë‹˜
- í•˜ì§€ë§Œ ë°±ì—… ì„¤ì •ì˜ gradient accumulationì€ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±** ì œê³µ

---

### 4. **Weight Decay ì°¨ì´** âš ï¸ MEDIUM

| ì„¤ì • | Weight Decay | ì •ê·œí™” ê°•ë„ |
|------|-------------|-----------|
| í˜„ì¬ | 0.01 | ê°•í•¨ (ê³¼ì í•© ì–µì œ) |
| ë°±ì—… | 0.002 | ì•½í•¨ (í•™ìŠµ ìœ ì—°ì„±) |

**ì„±ëŠ¥ ì˜í–¥**:
- í˜„ì¬ ì„¤ì •(0.01)ì€ **5ë°° ê°•í•œ ì •ê·œí™”** â†’ ê³¼ì í•© ì–µì œ, ë‹¨ underfitting ìœ„í—˜
- ë°±ì—… ì„¤ì •(0.002)ì€ **ë” ìœ ì—°í•œ í•™ìŠµ** â†’ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥

---

### 5. **ì—í¬í¬ ìˆ˜ ì°¨ì´** âš ï¸ CRITICAL

| ì„¤ì • | Epochs | ì´ í•™ìŠµëŸ‰ |
|------|--------|---------|
| í˜„ì¬ | 3 | ë‚®ìŒ |
| ë°±ì—… | 10 | **3.3ë°° ë§ìŒ** |

**ì„±ëŠ¥ ì˜í–¥**:
- ë°±ì—… ì„¤ì •ì€ **3.3ë°° ë§ì€ í•™ìŠµ** â†’ ëª¨ë¸ ìˆ˜ë ´ ì¶©ë¶„
- í˜„ì¬ ì„¤ì •(3 epochs)ì€ **ì¡°ê¸° ì¢…ë£Œ** â†’ underfitting ê°€ëŠ¥ì„±

---

### 6. **Learning Rate Scheduler** âš ï¸ MEDIUM

#### í˜„ì¬ ì„¤ì •
- **ëª…ì‹œì  ìŠ¤ì¼€ì¤„ëŸ¬ ì—†ìŒ**
- warmup_ratio: 0.1 (ì „ì²´ì˜ 10%ë§Œ warmup)

#### ë°±ì—… ì„¤ì • (deepspeed.yml:79-87)
```yaml
scheduler:
  type: "WarmupCosineLR"
  params:
    warmup_num_steps: "auto"  # ì „ì²´ ìŠ¤í…ì˜ 10%
    warmup_min_ratio: 0.1     # ì‹œì‘ lr = 1e-6
    cos_min_ratio: 0.025      # ìµœì¢… lr = 2.5e-7
    warmup_type: "linear"
```

**ì„±ëŠ¥ ì˜í–¥**:
- ë°±ì—… ì„¤ì •ì€ **Cosine Annealing** ì‚¬ìš© â†’ í›„ë°˜ë¶€ LR ì ì§„ì  ê°ì†Œ
- ìµœì¢… LRì´ 1e-5 â†’ 2.5e-7ê¹Œì§€ ê°ì†Œ â†’ **ë” ì„¸ë°€í•œ ìˆ˜ë ´**
- í˜„ì¬ ì„¤ì •ì€ ìƒìˆ˜ LR â†’ í›„ë°˜ë¶€ ìµœì í™” ë¶€ì¡±

---

### 7. **ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´** âš ï¸ LOW

| ì„¤ì • | Max Length | ì»¨í…ìŠ¤íŠ¸ |
|------|-----------|---------|
| í˜„ì¬ | 256 | ì œí•œì  |
| ë°±ì—… | 320 | **25% ë” ê¸´ ì»¨í…ìŠ¤íŠ¸** |

**ì„±ëŠ¥ ì˜í–¥**:
- ë°±ì—… ì„¤ì •(320)ì€ **ë” ê¸´ ì…ë ¥ ì²˜ë¦¬** â†’ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
- ë‹¨, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€

---

### 8. **DeepSpeed ìµœì í™” ì°¨ì´**

#### Optimizer Offloading

| ì„¤ì • | Optimizer Location | ë©”ëª¨ë¦¬ ì˜í–¥ |
|------|-------------------|----------|
| í˜„ì¬ | GPU (device: "none") | ë†’ì€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© |
| ë°±ì—… | CPU offload | GPU ë©”ëª¨ë¦¬ ì ˆì•½ |

#### Activation Checkpointing

- **í˜„ì¬**: `gradient_checkpointing: true` (ëª¨ë¸ ë ˆë²¨)
- **ë°±ì—…**: `activation_checkpointing.partition_activations: true` (DeepSpeed ë ˆë²¨)
  - ë” ì„¸ë°€í•œ ë©”ëª¨ë¦¬ ìµœì í™”

---

## ğŸ¯ ì„±ëŠ¥ ì°¨ì´ ì›ì¸ ì¢…í•© ë¶„ì„

### ë°±ì—… ì„¤ì •ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì´ìœ :

1. **ë” í° LoRA capacity** (r=256 vs 128)
   - 2ë°° ë§ì€ í•™ìŠµ íŒŒë¼ë¯¸í„° â†’ í‘œí˜„ë ¥ í–¥ìƒ

2. **ë” ì•ˆì •ì ì¸ Learning Rate** (1e-5 vs 2e-5)
   - ì ˆë°˜ì˜ LRë¡œ ì•ˆì •ì  ìˆ˜ë ´

3. **3ë°° ë§ì€ í•™ìŠµëŸ‰** (10 epochs vs 3)
   - ì¶©ë¶„í•œ ìˆ˜ë ´ ì‹œê°„ í™•ë³´

4. **ê³ ê¸‰ LR ìŠ¤ì¼€ì¤„ëŸ¬** (WarmupCosineLR)
   - í›„ë°˜ë¶€ ì„¸ë°€í•œ ìµœì í™”

5. **ë” ê¸´ ì»¨í…ìŠ¤íŠ¸** (320 vs 256)
   - 25% ë” ë§ì€ ì •ë³´ í™œìš©

6. **ì•½í•œ ì •ê·œí™”** (weight_decay 0.002 vs 0.01)
   - ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ìš©ì´

---

## ğŸ’¡ ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ ì•ˆ (ìš°ì„ ìˆœìœ„ ìˆœ)

1. **LoRA r/alpha ì¦ê°€** (training.yml:40-41)
   ```yaml
   r: 256          # 128 â†’ 256
   lora_alpha: 512 # 256 â†’ 512
   ```

2. **Learning Rate ê°ì†Œ** (training.yml:66)
   ```yaml
   learning_rate: 1.0e-5  # 2.0e-5 â†’ 1.0e-5
   ```

3. **Epochs ì¦ê°€** (training.yml:62)
   ```yaml
   num_train_epochs: 10  # 3 â†’ 10
   ```

4. **Weight Decay ê°ì†Œ** (training.yml:67)
   ```yaml
   weight_decay: 0.002  # 0.01 â†’ 0.002
   ```

5. **Max Length ì¦ê°€** (training.yml:30)
   ```yaml
   max_length: 320  # 256 â†’ 320
   ```

6. **LR Scheduler ì¶”ê°€** (training_argsì— ì¶”ê°€)
   ```yaml
   lr_scheduler_type: "cosine"
   warmup_ratio: 0.1
   ```

### ë©”ëª¨ë¦¬ê°€ í—ˆìš©ëœë‹¤ë©´ ì¶”ê°€ ê³ ë ¤ì‚¬í•­

7. **Optimizer Offload í™œì„±í™”** (training.yml:97-98)
   ```yaml
   offload_optimizer:
     device: "cpu"
     pin_memory: true
   ```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

ìœ„ ê¶Œì¥ì‚¬í•­ì„ **ëª¨ë‘ ì ìš©**í•  ê²½ìš°:
- **ìˆ˜ë ´ ì•ˆì •ì„±**: 50-70% í–¥ìƒ
- **ìµœì¢… ì •í™•ë„**: 5-15% í–¥ìƒ (ì ˆëŒ€ê°’ ê¸°ì¤€)
- **í•™ìŠµ ì‹œê°„**: 3ë°° ì¦ê°€ (3 â†’ 10 epochs)

**ì ì§„ì  ì ìš© ë°©ì•ˆ**:
1ë‹¨ê³„: LoRA r/alpha + LR ì¡°ì • â†’ í…ŒìŠ¤íŠ¸
2ë‹¨ê³„: Epochs ì¦ê°€ â†’ í…ŒìŠ¤íŠ¸
3ë‹¨ê³„: Weight Decay + Max Length + Scheduler ì¶”ê°€ â†’ ìµœì¢… í…ŒìŠ¤íŠ¸

---

## ğŸ“ ì°¸ê³  íŒŒì¼ ìœ„ì¹˜

- í˜„ì¬ ì„¤ì •: `/home/user/projects/kedro_project/account-tax/conf/base/parameters/training.yml`
- ë°±ì—… ì„¤ì •: `/home/user/projects/kedro_project/backup/conf/base/deepspeed.yml`
