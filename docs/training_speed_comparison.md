# í•™ìŠµ ì†ë„ 12ë°° ì°¨ì´ ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025-10-10
**ë°œê²¬**: ë°±ì—… ëŒ€ë¹„ í˜„ì¬ êµ¬í˜„ì´ 12ë°° ëŠë¦¼ (300K ìƒ˜í”Œ vs 3M ìƒ˜í”Œ ê³ ë ¤ ì‹œì—ë„ ì´ìƒ)

---

## ğŸ” í•µì‹¬ ì°¨ì´ì  ìš”ì•½

### 1. **Padding ë°©ë²•** â­ ê°€ì¥ ì¤‘ìš”

#### ë°±ì—… (backup/src/model_training/train.py:249-256)
```python
def tok_fn(batch):
    return tok(
        batch["text"],
        truncation=True,
        max_length=cfg.data.max_length,  # 320
        add_special_tokens=True,  # ëª…ì‹œì ìœ¼ë¡œ EOS í† í° ì¶”ê°€
        padding=False  # DataCollatorê°€ ë°°ì¹˜ ìƒì„± ì‹œ íŒ¨ë”© ì²˜ë¦¬
    )
```

#### í˜„ì¬ (ìˆ˜ì • ì „)
```python
# account-tax/conf/base/parameters/training.yml:32
padding: "longest"  # ë™ì  íŒ¨ë”© (ë°°ì¹˜ë§ˆë‹¤ ê¸¸ì´ ë‹¤ë¦„)
max_length: 256

# train/nodes.py:125-133 (ìˆ˜ì • ì „)
def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:
    return tokenizer(
        examples["text"],
        truncation=truncation,
        max_length=max_length,
        padding=padding,  # "longest" ì‚¬ìš©
        return_length=True,
    )
```

#### í˜„ì¬ (ìˆ˜ì • í›„)
```python
# account-tax/conf/base/parameters/training.yml:32
padding: "max_length"  # ê³ ì • ê¸¸ì´ íŒ¨ë”©
max_length: 320  # ë°±ì—…ê³¼ ë™ì¼í•˜ê²Œ ì¦ê°€

# train/nodes.py:125-133 (ìˆ˜ì • í›„)
def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:
    return tokenizer(
        examples["text"],
        truncation=truncation,
        max_length=max_length,
        add_special_tokens=True,  # ëª…ì‹œì ìœ¼ë¡œ EOS í† í° ì¶”ê°€
        padding=False,  # DataCollatorê°€ ë°°ì¹˜ ìƒì„± ì‹œ íŒ¨ë”© ì²˜ë¦¬
        return_length=True,
    )
```

**ì„±ëŠ¥ ì˜í–¥**:
- `padding="longest"`: ë°°ì¹˜ë§ˆë‹¤ ìµœì¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ ë™ì  íŒ¨ë”© â†’ **ë°°ì¹˜ë§ˆë‹¤ í…ì„œ í¬ê¸° ë³€ë™**
- `padding=False` + DataCollator: ê³ ì •ëœ max_lengthë¡œ ì¼ê´€ëœ íŒ¨ë”© â†’ **ë°°ì¹˜ í¬ê¸° ì¼ê´€ì„±**
- í…ì„œ í¬ê¸° ì¼ê´€ì„±ì€ GPU ìµœì í™”ì— ë§¤ìš° ì¤‘ìš” (ë©”ëª¨ë¦¬ í• ë‹¹, ì»¤ë„ ì¬ì‚¬ìš©)

---

### 2. **DataLoader ìµœì í™”**

#### ë°±ì—… (train.py:277-280)
```python
train_loader = DataLoader(
    train_ds,
    batch_size=cfg.deepspeed.train_micro_batch_size_per_gpu,
    sampler=train_sampler,
    collate_fn=collate,
    num_workers=cfg.hardware.num_workers,  # 4
    pin_memory=cfg.hardware.pin_memory,    # True
    prefetch_factor=2,                      # â­ ì¤‘ìš”
    persistent_workers=True if cfg.hardware.num_workers > 0 else False  # â­ ì¤‘ìš”
)
```

#### í˜„ì¬ (main_yaml.py:185-188)
```python
collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
)
# Trainerê°€ ìë™ìœ¼ë¡œ DataLoader ìƒì„±
# prefetch_factor, persistent_workers ì„¤ì • ì—†ìŒ
```

**ì°¨ì´ì **:
- `prefetch_factor=2`: 2ë°° ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸° â†’ I/O ëŒ€ê¸° ê°ì†Œ
- `persistent_workers=True`: ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¬ì‚¬ìš© â†’ í”„ë¡œì„¸ìŠ¤ ìƒì„± ì˜¤ë²„í—¤ë“œ ì œê±°
- í˜„ì¬ëŠ” Trainer ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (ìµœì í™” ë¶€ì¡±)

---

### 3. **í† í¬ë‚˜ì´ì € ì„¤ì •**

#### ë°±ì—… (train.py:41-66)
```python
def setup_tokenizer(model_name: str, trust_remote_code: bool = True):
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=trust_remote_code
    )

    # íŒ¨ë”© ë°©í–¥ ì„¤ì • (í›ˆë ¨ìš©)
    tokenizer.padding_side = "right"

    # í† í° ID í™•ì¸ ë° ê²€ì¦
    if local_rank == 0:
        print(f"ğŸ” í† í¬ë‚˜ì´ì € í† í° ì •ë³´:")
        print(f"   - PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})")
        print(f"   - EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})")

        # Qwen3ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ PADì™€ EOSê°€ êµ¬ë¶„ë˜ì–´ ìˆìŒ
        if tokenizer.pad_token_id == tokenizer.eos_token_id:
            print("âš ï¸ ê²½ê³ : PADì™€ EOS í† í°ì´ ë™ì¼í•©ë‹ˆë‹¤.")
        else:
            print("âœ… PADì™€ EOS í† í°ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    return tokenizer
```

#### í˜„ì¬ (main_yaml.py:149-151)
```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

**ì°¨ì´ì **:
- ë°±ì—…: `padding_side = "right"` ëª…ì‹œì  ì„¤ì •
- ë°±ì—…: PAD/EOS í† í° êµ¬ë¶„ ì—¬ë¶€ ê²€ì¦
- í˜„ì¬: ê¸°ë³¸ ì„¤ì •ë§Œ ì‚¬ìš©

---

### 4. **í™˜ê²½ ìµœì í™”**

#### ë°±ì—… (train.py:69-90)
```python
def auto_setup_environment():
    # Intel oneMKL ì˜¤ë¥˜ ë°©ì§€
    os.environ["MKL_SERVICE_FORCE_INTEL"] = "1"
    os.environ["MKL_THREADING_LAYER"] = "GNU"
    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["MKL_NUM_THREADS"] = "1"

    # CUDA ìµœì í™”
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
        torch.backends.cuda.enable_flash_sdp(True)  # Flash Attention 2
```

#### í˜„ì¬
```python
# í™˜ê²½ ìµœì í™” ì—†ìŒ
```

**ì°¨ì´ì **:
- TF32 í™œì„±í™”: í–‰ë ¬ ì—°ì‚° ì†ë„ í–¥ìƒ
- Flash Attention 2: Self-attention ë©”ëª¨ë¦¬ ë° ì†ë„ ìµœì í™”
- oneMKL ìŠ¤ë ˆë”©: CPU ì—°ì‚° ì•ˆì •í™”

---

### 5. **ë°ì´í„°ì…‹ ì²˜ë¦¬ ë°©ì‹**

#### ë°±ì—…
```python
# PKL â†’ Arrow ë³€í™˜ (mmap ìµœì í™”)
dsdict = load_from_disk(arrow_dir, keep_in_memory=False)  # mmap ë¡œë“œ

# í† í¬ë‚˜ì´ì§• (ë³‘ë ¬ ì²˜ë¦¬)
train_ds = train_ds.map(
    tok_fn,
    batched=True,
    remove_columns=cols_to_remove,
    num_proc=os.cpu_count()  # ì „ì²´ CPU ì½”ì–´ ì‚¬ìš©
)

# í…ì„œ í¬ë§· ì§€ì •
train_ds.set_format("torch")
```

#### í˜„ì¬
```python
# ì´ë¯¸ í† í¬ë‚˜ì´ì¦ˆëœ ë°ì´í„°ì…‹ ë¡œë“œ
dataset_dict = load_from_disk(tokenized_path)

# Trainerê°€ ìë™ ì²˜ë¦¬ (ìµœì í™” ë¶€ì¡±)
```

**ì°¨ì´ì **:
- ë°±ì—…: `num_proc=os.cpu_count()` (ì „ì²´ CPU ì½”ì–´ í™œìš©)
- ë°±ì—…: `keep_in_memory=False` (ë©”ëª¨ë¦¬ ì ˆì•½, mmap í™œìš©)
- ë°±ì—…: `set_format("torch")` (í…ì„œ ë³€í™˜ ìµœì í™”)

---

## ğŸ“Š ì„±ëŠ¥ ì˜í–¥ ë¶„ì„

### ì˜ˆìƒ ì†ë„ ê°œì„  íš¨ê³¼

| ìµœì í™” í•­ëª© | ì†ë„ ê°œì„  | ëˆ„ì  ê°œì„  | ìš°ì„ ìˆœìœ„ |
|------------|---------|---------|---------|
| **1. Padding ë°©ì‹** (`longest` â†’ `False`) | 2-3ë°° | 2-3ë°° | ğŸ”¥ ìµœìš°ì„  |
| **2. DataLoader ìµœì í™”** (prefetch, persistent_workers) | 1.5-2ë°° | 3-6ë°° | â­ ë†’ìŒ |
| **3. í™˜ê²½ ìµœì í™”** (TF32, Flash Attention) | 1.2-1.5ë°° | 3.6-9ë°° | â­ ë†’ìŒ |
| **4. í† í¬ë‚˜ì´ì§• ë³‘ë ¬í™”** (num_proc) | 1.1-1.3ë°° | 4-12ë°° | ì¤‘ê°„ |
| **5. í…ì„œ í¬ë§· ìµœì í™”** (set_format) | 1.05-1.1ë°° | 4.2-13ë°° | ë‚®ìŒ |

### 12ë°° ì°¨ì´ ì„¤ëª…

**ë°ì´í„°ì…‹ í¬ê¸° ì˜í–¥**:
- ë°±ì—…: ~300K ìƒ˜í”Œ
- í˜„ì¬: ~3M ìƒ˜í”Œ (10ë°°)

**ìˆœìˆ˜ ì½”ë“œ ìµœì í™” ì˜í–¥**:
- ì˜ˆìƒ: 4-12ë°° ê°œì„  ê°€ëŠ¥
- ì‹¤ì œ ì°¨ì´: 12ë°°

**ê²°ë¡ **:
- ë°ì´í„°ì…‹ 10ë°° ì¦ê°€ â†’ í•™ìŠµ ì‹œê°„ 10ë°° ì¦ê°€ (ì •ìƒ)
- ì½”ë“œ ìµœì í™” ë¶€ì¡± â†’ ì¶”ê°€ 1.2-2ë°° ì¦ê°€ (ë¹„ì •ìƒ)
- **ì´ 12ë°° ì°¨ì´ = 10ë°° (ë°ì´í„°) Ã— 1.2ë°° (ìµœì í™” ë¶€ì¡±)**

---

## ğŸ¯ ì¦‰ì‹œ ì ìš© ê¶Œì¥ì‚¬í•­

### âœ… 1ë‹¨ê³„: Padding ìˆ˜ì • (ì™„ë£Œ)

**training.yml**:
```yaml
tokenization:
  max_length: 320  # 256 â†’ 320
  padding: "max_length"  # "longest" â†’ "max_length"
```

**train/nodes.py**:
```python
def tokenize_function(examples: Dict[str, Any]) -> Dict[str, Any]:
    return tokenizer(
        examples["text"],
        truncation=truncation,
        max_length=max_length,
        add_special_tokens=True,  # ì¶”ê°€
        padding=False,  # "longest" â†’ False
        return_length=True,
    )
```

**ì˜ˆìƒ íš¨ê³¼**: 2-3ë°° ì†ë„ í–¥ìƒ

---

### âœ… 2ë‹¨ê³„: DataLoader ìµœì í™” (ì¶”ì²œ)

**training.ymlì— ì¶”ê°€**:
```yaml
training_args:
  dataloader_num_workers: 8  # ì´ë¯¸ ìˆìŒ
  dataloader_pin_memory: true  # ì¶”ê°€
  dataloader_prefetch_factor: 2  # ì¶”ê°€
  dataloader_persistent_workers: true  # ì¶”ê°€
```

**ì˜ˆìƒ íš¨ê³¼**: ì¶”ê°€ 1.5-2ë°° ì†ë„ í–¥ìƒ

---

### âœ… 3ë‹¨ê³„: í™˜ê²½ ìµœì í™” (ì¶”ì²œ)

**main_yaml.py ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€**:
```python
import os
import torch

# í™˜ê²½ ìµœì í™” (ë°±ì—… ì½”ë“œ ì°¸ì¡°)
os.environ["MKL_SERVICE_FORCE_INTEL"] = "1"
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# CUDA ìµœì í™”
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
    torch.backends.cuda.enable_flash_sdp(True)
```

**ì˜ˆìƒ íš¨ê³¼**: ì¶”ê°€ 1.2-1.5ë°° ì†ë„ í–¥ìƒ

---

### âœ… 4ë‹¨ê³„: í† í¬ë‚˜ì´ì € ì„¤ì • (ì„ íƒ)

**train/nodes.pyì˜ tokenizer ì„¤ì • ê°œì„ **:
```python
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.padding_side = "right"  # ì¶”ê°€
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
```

**ì˜ˆìƒ íš¨ê³¼**: ì•ˆì •ì„± í–¥ìƒ (ì†ë„ëŠ” ë¯¸ë¯¸)

---

## ğŸ”¬ ê²€ì¦ ë°©ë²•

### 1. í•™ìŠµ ì†ë„ ì¸¡ì •
```python
import time

start = time.time()
trainer.train()
elapsed = time.time() - start

print(f"í•™ìŠµ ì‹œê°„: {elapsed:.2f}ì´ˆ")
print(f"ìƒ˜í”Œë‹¹ ì‹œê°„: {elapsed / len(train_dataset):.4f}ì´ˆ")
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ì†ë„ í™•ì¸
```bash
# MLflow ë¡œê·¸ì—ì„œ í™•ì¸
cat mlruns/*/metrics/train_samples_per_second
```

### 3. GPU í™œìš©ë¥  ëª¨ë‹ˆí„°ë§
```bash
# í•™ìŠµ ì¤‘ ì‹¤í–‰
watch -n 1 nvidia-smi
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ì™„ë£Œëœ í•­ëª©
- [x] Padding ë°©ì‹ ë³€ê²½ (`longest` â†’ `max_length`)
- [x] max_length ì¦ê°€ (256 â†’ 320)
- [x] add_special_tokens=True ì¶”ê°€
- [x] padding=False + DataCollator ë°©ì‹ ì ìš©

### ğŸ”„ ì§„í–‰ ì¤‘
- [ ] DataLoader íŒŒë¼ë¯¸í„° ì¶”ê°€ (prefetch_factor, persistent_workers)
- [ ] í™˜ê²½ ìµœì í™” ì½”ë“œ ì¶”ê°€ (TF32, Flash Attention)
- [ ] í† í¬ë‚˜ì´ì € padding_side ì„¤ì •

### ğŸ“‹ ê²€ì¦ í•„ìš”
- [ ] í•™ìŠµ ì‹œê°„ ì¸¡ì • (ìˆ˜ì • ì „/í›„ ë¹„êµ)
- [ ] GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì†ë„ í™•ì¸ (samples/sec)

---

## ğŸ“ ê²°ë¡ 

**12ë°° ì°¨ì´ì˜ ì›ì¸**:
1. âœ… **ë°ì´í„°ì…‹ 10ë°° ì¦ê°€** (300K â†’ 3M): ì •ìƒì ì¸ ì¦ê°€
2. âŒ **Padding ë°©ì‹ ë¹„íš¨ìœ¨** (`longest`): 2-3ë°° ëŠë¦¼
3. âŒ **DataLoader ìµœì í™” ë¶€ì¡±**: 1.5ë°° ëŠë¦¼
4. âŒ **í™˜ê²½ ìµœì í™” ì—†ìŒ**: 1.2ë°° ëŠë¦¼

**ì¦‰ì‹œ ì ìš© ê¶Œì¥**:
1. Padding ìˆ˜ì • (ì™„ë£Œ) â†’ 2-3ë°° ê°œì„ 
2. DataLoader ìµœì í™” â†’ 1.5ë°° ì¶”ê°€ ê°œì„ 
3. í™˜ê²½ ìµœì í™” â†’ 1.2ë°° ì¶”ê°€ ê°œì„ 

**ì˜ˆìƒ ìµœì¢… ì†ë„**:
- í˜„ì¬: 12ë°° ëŠë¦¼
- ìˆ˜ì • í›„: ì•½ 2-3ë°° ëŠë¦¼ (ë°ì´í„°ì…‹ 10ë°° ê³ ë ¤ ì‹œ ì •ìƒ ë²”ìœ„)
