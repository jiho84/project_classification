# Architecture Notes

- Use this file to record high-level structural decisions.
- Cross-reference detailed diagrams below; maintain this document as the single source of architectural truth.
- Whenever pipelines or module boundaries change, summarize the update here and link to supporting docs.
- **Last Updated**: 2025-10-17
- **Architecture Version**: 2.3.0

## ì„¤ê³„ ì² í•™ (ëŒ€ì¹­í™” Â· ëª¨ë“ˆí™” Â· ìˆœì„œí™”)

- **ëŒ€ì¹­í™”(Pattern)**: ë™ì¼í•œ ë³¸ì§ˆì˜ ê¸°ëŠ¥ì€ ìœ ì‚¬í•œ íŒ¨í„´ìœ¼ë¡œ ìž‘ì„±í•˜ì—¬ êµ¬ì¡° íŒŒì•… ì‹œ ë¶ˆí•„ìš”í•œ ì¸ì§€ ë¹„ìš©ì„ ì¤„ì¸ë‹¤. ë…¸ë“œ í•¨ìˆ˜, `pipeline.py` êµ¬ì„±, ë¬¸ì„œ êµ¬ì¡° ëª¨ë‘ ì¼ê´€ëœ í˜•ì‹ì„ ìœ ì§€í•œë‹¤.
- **ëª¨ë“ˆí™”(Modularity)**: ë…¸ë“œ ê¸°ë°˜ìœ¼ë¡œ ê¸°ëŠ¥ì„ ë¶„ë¦¬í•˜ê³ , ê° ëª¨ë“ˆì´ ëª…í™•í•œ ìž…ë ¥Â·ì¶œë ¥ ê³„ì•½ì„ ê°–ë„ë¡ ìœ ì§€í•œë‹¤. íŒŒì´í”„ë¼ì¸ì€ ë…¸ë“œ ì¡°í•©ë§Œìœ¼ë¡œ ë³µìž¡í•œ ë™ìž‘ì„ í‘œí˜„í•´ì•¼ í•œë‹¤.
- **ìˆœì„œí™”(Ordering)**: í´ë”/íŒŒì¼ êµ¬ì¡°(ì •ì )ì™€ ì‹¤í–‰ íë¦„(ë™ì )ì˜ ì¸ê³¼ë¥¼ ëª…í™•ížˆ ê¸°ë¡í•œë‹¤. ë¬¸ì„œì—ëŠ” ë‹¨ê³„ë³„ ìˆœì„œ, ë°ì´í„° íë¦„, ì˜ì¡´ ê´€ê³„ë¥¼ ëª…ì‹œí•œë‹¤.

### Dual-Ecosystem Architecture (ì´ì¤‘ ìƒíƒœê³„ ì•„í‚¤í…ì²˜)

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Kedro ë…¸ë“œ ìƒíƒœê³„**ì™€ **ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒíƒœê³„**ë¼ëŠ” ë‘ ê°€ì§€ ì‹¤í–‰ í™˜ê²½ì„ í†µí•©í•œ ì´ì¤‘ êµ¬ì¡°ë¥¼ ì±„íƒí•©ë‹ˆë‹¤.

#### Layer 1: Kedro Node Ecosystem (ì¼€ë“œë¡œ ë…¸ë“œ ìƒíƒœê³„)
- **ì² í•™**: ìˆœë°©í–¥(forward-progressive) ë°ì´í„° íë¦„, ë‹¨ìˆœ ìž…ì¶œë ¥ ì „ë‹¬, ë¸”ë¡ ë‹¨ìœ„ ì¡°í•©
- **ì œì•½**: ë…¸ë“œ ê°„ ì–‘ë°©í–¥ í†µì‹  ë¶ˆê°€, ë³µìž¡í•œ ìƒíƒœ ê³µìœ  ë¶ˆê°€, ë‹¨ì¼ ì‹¤í–‰ ìŠ¤ë ˆë“œ
- **ìž¥ì **: ìž¬í˜„ ê°€ëŠ¥ì„±, ëª…í™•í•œ ì˜ì¡´ì„±, ì‹œê°í™” ê°€ëŠ¥, MLflow ìžë™ ì¶”ì 
- **êµ¬í˜„ ì˜ì—­**:
  - ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ingestion â†’ preprocess â†’ feature)
  - ë°ì´í„° ë¶„í•  ë° ì§ë ¬í™” (split)
  - í† í°í™” ë° í•™ìŠµ ì¤€ë¹„ (train â†’ tokenize_datasets)

#### Layer 2: Subprocess Ecosystem (ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒíƒœê³„)
- **ì² í•™**: ë³µìž¡í•œ ì–‘ë°©í–¥ ìƒí˜¸ìž‘ìš©, ë©€í‹°í”„ë¡œì„¸ìŠ¤ í˜‘ì—…, ë™ì  ìƒíƒœ ê´€ë¦¬
- **íŠ¹ì§•**: DeepSpeed ë¶„ì‚° í•™ìŠµ, GPU ê°„ í†µì‹ , ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬, ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- **ì œì•½**: Kedro ì¶”ì  ì œí•œ, ìˆ˜ë™ MLflow ì—°ë™ í•„ìš”, ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒëª…ì£¼ê¸° ê´€ë¦¬
- **êµ¬í˜„ ì˜ì—­**:
  - ë¶„ì‚° í•™ìŠµ ì‹¤í–‰ (`src/train/main_yaml.py`)
  - LoRA ìµœì í™”, ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
  - ì²´í¬í¬ì¸íŠ¸ ì €ìž¥/ë³µì›
  - í‰ê°€ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

#### Boundary & Integration (ê²½ê³„ ë° í†µí•©)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kedro Node Ecosystem (Layer 1)                                 â”‚
â”‚                                                                 â”‚
â”‚  ingestion â†’ preprocess â†’ feature â†’ split â†’ tokenize_datasets  â”‚
â”‚                                                                 â”‚
â”‚  [ìˆœë°©í–¥ ë°ì´í„° íë¦„, ë‹¨ìˆœ I/O ì „ë‹¬, ìž¬í˜„ ê°€ëŠ¥ì„± ë³´ìž¥]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€ launch_training (Kedro ë…¸ë“œ)
                            â”‚  â€¢ YAML ì„¤ì • ìƒì„±
                            â”‚  â€¢ MLflow ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬ (env vars)
                            â”‚  â€¢ subprocess.run() í˜¸ì¶œ
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Subprocess Ecosystem (Layer 2)                                 â”‚
â”‚                                                                 â”‚
â”‚  main_yaml.py â†’ DeepSpeed ë¶„ì‚° í•™ìŠµ                              â”‚
â”‚  â€¢ ì–‘ë°©í–¥ GPU í†µì‹  (NCCL)                                        â”‚
â”‚  â€¢ ë™ì  ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬                                           â”‚
â”‚  â€¢ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘                                             â”‚
â”‚  â€¢ WeightedTrainer + GPUMemoryCallback                         â”‚
â”‚                                                                 â”‚
â”‚  [ë³µìž¡í•œ ìƒí˜¸ìž‘ìš©, ë©€í‹°í”„ë¡œì„¸ìŠ¤ í˜‘ì—…, ìƒíƒœ ë™ê¸°í™”]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€ í•™ìŠµ ì™„ë£Œ í›„ ì œì–´ê¶Œ ë°˜í™˜
                            â”‚  â€¢ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
                            â”‚  â€¢ ë©”íŠ¸ë¦­ JSON íŒŒì¼
                            â”‚  â€¢ MLflow ì•„í‹°íŒ©íŠ¸ ë¡œê¹… (Kedroê°€ ìˆ˜í–‰)
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kedro Node Ecosystem (Layer 1 ë³µê·€)                            â”‚
â”‚                                                                 â”‚
â”‚  launch_training â†’ MLflow ì•„í‹°íŒ©íŠ¸ ë¡œê¹… â†’ íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Integration Mechanisms (í†µí•© ë©”ì»¤ë‹ˆì¦˜)

1. **Configuration Contract (ì„¤ì • ê³„ì•½)**
   - Kedro: `params:train` â†’ YAML ì§ë ¬í™” â†’ `train_config.yml`
   - Subprocess: YAML ì—­ì§ë ¬í™” â†’ ì‹¤í–‰ ì„¤ì • êµ¬ì„±

2. **MLflow Context Passing (MLflow ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)**
   ```python
   # launch_training ë…¸ë“œ (Kedro Layer)
   env["MLFLOW_TRACKING_URI"] = tracking_uri
   env["MLFLOW_RUN_ID"] = active_run.info.run_id
   env["MLFLOW_NESTED_RUN"] = "true"  # ë¶€ëª¨ run ë³´í˜¸
   ```

3. **Artifact Handoff (ì•„í‹°íŒ©íŠ¸ ì „ë‹¬)**
   - Subprocess â†’ ë¡œì»¬ ë””ë ‰í† ë¦¬ ì €ìž¥ (ì²´í¬í¬ì¸íŠ¸, ë©”íŠ¸ë¦­)
   - Kedro ë…¸ë“œ â†’ MLflow ì•„í‹°íŒ©íŠ¸ ë¡œê¹… (ì œì–´ê¶Œ ë³µê·€ í›„)

4. **Error Propagation (ì—ëŸ¬ ì „íŒŒ)**
   ```python
   subprocess.run(cmd, env=env, check=True)  # CalledProcessError ë°œìƒ ì‹œ Kedro ì¤‘ë‹¨
   ```

#### Design Rationale (ì„¤ê³„ ê·¼ê±°)

| ìš”êµ¬ì‚¬í•­ | Kedro ë‹¨ë… | Subprocess í†µí•© | ì„ íƒ |
|---------|-----------|----------------|-----|
| ë¶„ì‚° í•™ìŠµ (DeepSpeed ZeRO) | âŒ ë¶ˆê°€ëŠ¥ | âœ… ë„¤ì´í‹°ë¸Œ ì§€ì› | **Subprocess** |
| ìž¬í˜„ ê°€ëŠ¥ì„± | âœ… ìžë™ ë³´ìž¥ | âš ï¸ ìˆ˜ë™ ê´€ë¦¬ | **Kedro** |
| ë³µìž¡í•œ GPU í†µì‹  | âŒ ë¶ˆê°€ëŠ¥ | âœ… NCCL/GLOO | **Subprocess** |
| íŒŒì´í”„ë¼ì¸ ì‹œê°í™” | âœ… kedro viz | âŒ ë¶ˆê°€ëŠ¥ | **Kedro** |
| ì²´í¬í¬ì¸íŠ¸ ìž¬ì‹œìž‘ | âš ï¸ ì œí•œì  | âœ… ìœ ì—°í•¨ | **Subprocess** |
| MLflow ìžë™ ì¶”ì  | âœ… kedro-mlflow | âš ï¸ ìˆ˜ë™ ì—°ë™ | **Kedro** |

â†’ **ê²°ë¡ **: ë‘ ìƒíƒœê³„ì˜ ìž¥ì ì„ ê²°í•©í•˜ì—¬ **ë°ì´í„° íŒŒì´í”„ë¼ì¸ì€ Kedro**, **í•™ìŠµ ì‹¤í–‰ì€ Subprocess**ë¡œ ë¶„ë¦¬

#### Common Utilities Bridge (ê³µí†µ ìœ í‹¸ë¦¬í‹° ë¸Œë¦¬ì§€)

`src/account_tax/utils/common.py`ëŠ” ë‘ ìƒíƒœê³„ë¥¼ ì—°ê²°í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬:

```python
# Kedro ë…¸ë“œì—ì„œ ì‚¬ìš©
from account_tax.utils import compose_deepspeed_config, ensure_dir

# Subprocessì—ì„œ ì‚¬ìš©
from account_tax.utils.common import (
    build_training_arguments,
    WeightedTrainer,
    GPUMemoryCallback,
    compute_classification_metrics,
)
```

- **ëŒ€ì¹­í™” ì›ì¹™**: ë™ì¼í•œ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜, ì¼ê´€ëœ ì„¤ì • íŒ¨í„´
- **ëª¨ë“ˆí™” ì›ì¹™**: ê° í•¨ìˆ˜ëŠ” ë‹¨ì¼ ì±…ìž„, ëª…í™•í•œ ìž…ì¶œë ¥ ê³„ì•½
- **ìˆœì„œí™” ì›ì¹™**: `common.py` â†’ Kedro ë…¸ë“œ â†’ Subprocess ìˆœì„œë¡œ ìž„í¬íŠ¸

#### Current Implementation Analysis (í˜„ìž¬ êµ¬í˜„ ë¶„ì„)

##### Kedro Layer Functions (ì¼€ë“œë¡œ ê³„ì¸µ í•¨ìˆ˜)

**Pattern: Pure Data Transformation (ìˆœìˆ˜ ë°ì´í„° ë³€í™˜)**

ëª¨ë“  Kedro ë…¸ë“œëŠ” ë‹¤ìŒ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤:

```python
def node_function(
    input_data: Type,
    params: Dict[str, Any]
) -> OutputType:
    """
    Pipeline Order: [íŒŒì´í”„ë¼ì¸ëª…] step [ë²ˆí˜¸] ([ìˆœì„œ])
    Role: [ì—­í•  ì„¤ëª…]
    """
    # 1. ìž…ë ¥ ê²€ì¦
    # 2. ë°ì´í„° ë³€í™˜
    # 3. ë¡œê¹… (ë¶€ìˆ˜íš¨ê³¼)
    # 4. ì¶œë ¥ ë°˜í™˜
    return transformed_data
```

**ì˜ˆì‹œ: preprocess/nodes.py â†’ clean_data**
```python
def clean_data(data: pd.DataFrame, clean_params: Dict[str, Any]) -> pd.DataFrame:
    initial_rows = len(data)

    # ë‹¨ìˆœ ë³€í™˜: ì¤‘ë³µ ì œê±°
    if clean_params.get('remove_duplicates', True):
        data = data.drop_duplicates()

    # ë‹¨ìˆœ ë³€í™˜: null ì œê±°
    dropna_columns = clean_params.get('dropna_columns', [])
    if dropna_columns:
        data = data.dropna(subset=dropna_columns)

    logger.info(f"Cleaning: {initial_rows} -> {len(data)} rows")
    return data
```

**íŠ¹ì§•**:
- âœ… ìƒíƒœ ì—†ìŒ (stateless)
- âœ… ë¶€ìˆ˜íš¨ê³¼ ìµœì†Œí™” (ë¡œê¹…ë§Œ)
- âœ… í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- âœ… ìž¬í˜„ ê°€ëŠ¥
- âŒ ë³µìž¡í•œ ìƒí˜¸ìž‘ìš© ë¶ˆê°€ëŠ¥

##### Subprocess Layer Functions (ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ê³„ì¸µ í•¨ìˆ˜)

**Pattern: Block-based Pipeline Orchestration (ë¸”ë¡ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)**

**REFACTORED (2025-10-17 v2.3)**: ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë„ Kedro ì² í•™ì„ ë”°ë¼ ë¸”ë¡ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬í˜„ë¨.

**ì„¤ê³„ ë³€ê²½ ìš”ì•½**:
- âŒ **ì´ì „**: 328ì¤„ ëª¨ë†€ë¦¬ì‹ main() í•¨ìˆ˜
- âœ… **í˜„ìž¬**: 10ê°œ ë¸”ë¡ í•¨ìˆ˜ + Kedro-style ì„ ì–¸í˜• íŒŒì´í”„ë¼ì¸ (181ì¤„, 45% ê°ì†Œ)

**ìƒˆë¡œìš´ êµ¬ì¡°: train/main_yaml.py â†’ main**
```python
def main() -> None:
    """Training pipeline in Kedro-declarative style.

    Defines and executes the training pipeline blocks sequentially.
    Each block follows the Kedro node pattern: func, inputs, outputs, name, description.

    Pipeline blocks:
        1. setup_training_context    - Initialize environment
        2. load_datasets              - Load tokenized datasets
        3. initialize_tokenizer       - Configure tokenizer
        4. initialize_model           - Load base model
        5. apply_lora_to_model        - Apply LoRA optimization
        6. build_weighted_trainer     - Create WeightedTrainer
        7. patch_mlflow_callback      - Prevent subprocess hang
        8. execute_training_loop      - Run training
        9. evaluate_and_save_results  - Evaluate and save
       10. cleanup_distributed        - Clean up resources
    """
    logging.basicConfig(level=logging.INFO)

    # Parse configuration
    args = parse_args()
    with open(args.config_yml, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # Define pipeline blocks (Kedro-style declarative format)
    pipeline = [
        {
            "func": setup_training_context,
            "inputs": ["args", "cfg"],
            "outputs": "context",
            "name": "setup_context",
            "description": "Initialize training environment",
        },
        {
            "func": load_datasets,
            "inputs": ["context", "logger"],
            "outputs": "artifacts",
            "name": "load_datasets",
            "description": "Load tokenized datasets",
        },
        {
            "func": initialize_tokenizer,
            "inputs": ["context", "artifacts", "logger"],
            "outputs": "artifacts",
            "name": "initialize_tokenizer",
            "description": "Configure tokenizer and infer num_labels",
        },
        {
            "func": initialize_model,
            "inputs": ["context", "artifacts", "logger"],
            "outputs": "artifacts",
            "name": "initialize_model",
            "description": "Load base model without optimization",
        },
        {
            "func": apply_lora_to_model,
            "inputs": ["context", "artifacts", "logger"],
            "outputs": "artifacts",
            "name": "apply_lora",
            "description": "Apply LoRA optimization if configured",
        },
        {
            "func": build_weighted_trainer,
            "inputs": ["context", "artifacts", "logger", "logger_zero"],
            "outputs": "artifacts",
            "name": "build_trainer",
            "description": "Create WeightedTrainer with all components",
        },
        {
            "func": patch_mlflow_callback,
            "inputs": ["artifacts", "logger_zero"],
            "outputs": "artifacts",
            "name": "patch_mlflow",
            "description": "Override MLflow callback to prevent hang",
        },
        {
            "func": execute_training_loop,
            "inputs": ["context", "artifacts", "logger", "logger_zero"],
            "outputs": "artifacts",
            "name": "execute_training",
            "description": "Run training loop with checkpoint support",
        },
        {
            "func": evaluate_and_save_results,
            "inputs": ["context", "artifacts", "logger_zero"],
            "outputs": None,
            "name": "evaluate_save",
            "description": "Evaluate on test set and save model",
        },
        {
            "func": cleanup_distributed_process_group,
            "inputs": ["logger_zero"],
            "outputs": None,
            "name": "cleanup",
            "description": "Clean up distributed resources",
        },
    ]

    # Execute pipeline
    state = {
        "args": args,
        "cfg": cfg,
        "logger": LOGGER,
        "logger_zero": LOGGER_ZERO,
    }

    for block in pipeline:
        LOGGER.info(f"Executing block [{block['name']}]: {block['description']}")

        # Resolve inputs from state
        inputs = [state[inp] for inp in block["inputs"]]

        # Execute block function
        result = block["func"](*inputs)

        # Store outputs in state
        if block["outputs"]:
            state[block["outputs"]] = result
```

**ë¸”ë¡ í•¨ìˆ˜ ì˜ˆì‹œ: utils/common.py**

**Context Objects (ì»¨í…ìŠ¤íŠ¸ ê°ì²´)**
```python
@dataclass
class TrainingContext:
    """Immutable training environment and configuration."""
    cfg: Dict[str, Any]
    args: argparse.Namespace
    seed: int
    is_rank_zero: bool
    output_dir: Path

@dataclass
class TrainingArtifacts:
    """Mutable artifacts passed between blocks."""
    train_dataset: Dataset
    eval_dataset: Dataset | None
    tokenizer: AutoTokenizer
    model: nn.Module
    trainer: Trainer
    # ... ê¸°íƒ€ ìƒíƒœ
```

**Block Functions (ë¸”ë¡ í•¨ìˆ˜)**
```python
def setup_training_context(args, cfg) -> TrainingContext:
    """Block 1: Initialize environment."""
    set_seed(cfg.get("seed", 42))
    return TrainingContext(cfg=cfg, args=args, seed=seed, is_rank_zero=..., output_dir=...)

def load_datasets(context, logger) -> TrainingArtifacts:
    """Block 2: Load tokenized datasets."""
    dataset_dict = load_from_disk(context.cfg["data"]["tokenized_path"])
    return TrainingArtifacts(
        train_dataset=dataset_dict["train"],
        eval_dataset=dataset_dict.get("validation"),
        test_dataset=dataset_dict.get("test"),
    )

def initialize_tokenizer(context, artifacts, logger) -> TrainingArtifacts:
    """Block 3: Configure tokenizer and infer num_labels."""
    tokenizer = AutoTokenizer.from_pretrained(model_name, ...)
    # Handle missing pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # Infer num_labels from ClassLabel feature
    num_labels = artifacts.train_dataset.features["labels"].num_classes
    artifacts.tokenizer = tokenizer
    artifacts.num_labels = num_labels
    return artifacts

def initialize_model(context, artifacts, logger) -> TrainingArtifacts:
    """Block 4: Load base model without optimization."""
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=artifacts.num_labels,
        ...
    )
    artifacts.model = model
    return artifacts

def apply_lora_to_model(context, artifacts, logger) -> TrainingArtifacts:
    """Block 5: Apply LoRA optimization if configured."""
    lora_cfg = context.cfg.get("lora", {})
    if lora_cfg.get("enable", False):
        artifacts.model = maybe_apply_lora(artifacts.model, lora_cfg, logger)
    return artifacts

def build_weighted_trainer(context, artifacts, logger, logger_zero) -> TrainingArtifacts:
    """Block 6: Create WeightedTrainer with all components."""
    training_args = build_training_arguments(context.cfg["training_args"])
    class_weights_tensor = compute_class_weights(artifacts.train_dataset)

    trainer = WeightedTrainer(
        model=artifacts.model,
        args=training_args,
        train_dataset=artifacts.train_dataset,
        eval_dataset=artifacts.eval_dataset,
        class_weights=class_weights_tensor,
        ...
    )
    artifacts.trainer = trainer
    return artifacts

def patch_mlflow_callback(artifacts, logger_zero) -> TrainingArtifacts:
    """Block 7: Override MLflow callback to prevent subprocess hang."""
    for callback in artifacts.trainer.callback_handler.callbacks:
        if callback.__class__.__name__ == "MLflowCallback":
            # Patch on_train_end() to prevent hang
            callback.on_train_end = lambda *args, **kwargs: None
    return artifacts

def execute_training_loop(context, artifacts, logger, logger_zero) -> TrainingArtifacts:
    """Block 8: Run training loop with checkpoint support."""
    resume_checkpoint = detect_latest_checkpoint(context.output_dir)
    artifacts.trainer.train(resume_from_checkpoint=resume_checkpoint)
    return artifacts

def evaluate_and_save_results(context, artifacts, logger_zero) -> None:
    """Block 9: Evaluate on test set and save model."""
    if artifacts.test_dataset and context.is_rank_zero:
        test_metrics = artifacts.trainer.evaluate(artifacts.test_dataset)
        logger_zero.info(f"Test metrics: {test_metrics}")

    # Distributed save with barriers
    torch.distributed.barrier()
    if context.is_rank_zero:
        artifacts.model.save_pretrained(context.output_dir)
        artifacts.tokenizer.save_pretrained(context.output_dir)
    torch.distributed.barrier()

def cleanup_distributed_process_group(logger_zero) -> None:
    """Block 10: Clean up distributed resources."""
    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()
        logger_zero.info("Destroyed distributed process group")
```

**ìƒˆë¡œìš´ íŠ¹ì§•**:
- âœ… **ë¸”ë¡ êµ¬ì¡°**: 10ê°œ ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ í•¨ìˆ˜ (ì„¸ë°€í•œ ì±…ìž„ ë¶„ë¦¬)
- âœ… **ì„ ì–¸í˜• íŒŒì´í”„ë¼ì¸**: Kedro pipeline.py ìŠ¤íƒ€ì¼ì˜ dict ê¸°ë°˜ êµ¬ì¡°
- âœ… **íŒŒì´í”„ë¼ì¸ íë¦„**: Kedroì²˜ëŸ¼ ëª…í™•í•œ ìž…ì¶œë ¥ ê³„ì•½
- âœ… **í…ŒìŠ¤íŠ¸ ê°€ëŠ¥**: ê° ë¸”ë¡ì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
- âœ… **ëŒ€ì¹­í™”**: Kedro ë…¸ë“œì™€ ì™„ë²½ížˆ ë™ì¼í•œ íŒ¨í„´
- âœ… **ëª¨ë“ˆí™”**: common.pyì—ì„œ ìž„í¬íŠ¸ (ìž¬ì‚¬ìš©ì„±)
- âœ… **ìˆœì„œí™”**: ëª…ì‹œì ì¸ ì‹¤í–‰ ìˆœì„œ (1-10)
- âœ… **ìƒíƒœ ê´€ë¦¬**: TrainingContext(ë¶ˆë³€) + TrainingArtifacts(ê°€ë³€)
- âœ… **ë³µìž¡í•œ ìƒí˜¸ìž‘ìš©**: ë¸”ë¡ ê°„ ê°ì²´ ì „ë‹¬ + state dict íŒ¨í„´

**ë¦¬íŒ©í† ë§ íš¨ê³¼ (v2.3)**:
- ðŸ“‰ ì½”ë“œ ë¼ì¸ ìˆ˜: 328ì¤„ â†’ 181ì¤„ (45% ê°ì†Œ)
- ðŸ“ˆ ìž¬ì‚¬ìš©ì„±: 0ê°œ â†’ 10ê°œ ë¸”ë¡ í•¨ìˆ˜
- ðŸ“ˆ ë¸”ë¡ ì„¸ë¶„í™”: 8ê°œ â†’ 10ê°œ (ëª¨ë¸ ì´ˆê¸°í™” 4ë‹¨ê³„ ë¶„ë¦¬)
- ðŸ“ˆ í…ŒìŠ¤íŠ¸ì„±: í†µí•© í…ŒìŠ¤íŠ¸ë§Œ â†’ ë¸”ë¡ë³„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- ðŸ“ˆ ìœ ì§€ë³´ìˆ˜ì„±: ëª¨ë†€ë¦¬ì‹ â†’ ëª…í™•í•œ ì±…ìž„ ë¶„ë¦¬
- ðŸ“ˆ ìŠ¤íƒ€ì¼ í†µì¼: Kedro pipeline.pyì™€ ë™ì¼í•œ ì„ ì–¸í˜• êµ¬ì¡°

##### Bridge Node: launch_training (ë¸Œë¦¬ì§€ ë…¸ë“œ)

**Pattern: Ecosystem Handoff (ìƒíƒœê³„ ì „í™˜)**

`launch_training`ì€ ë‘ ìƒíƒœê³„ë¥¼ ì—°ê²°í•˜ëŠ” íŠ¹ìˆ˜ ë…¸ë“œ:

```python
def launch_training(
    tokenized_dataset_path: str,    # Kedro ì¶œë ¥
    train_params: Dict[str, Any],   # Kedro íŒŒë¼ë¯¸í„°
) -> Dict[str, Any]:                # Kedro ì¶œë ¥
    """Kedro â†’ Subprocess ì „í™˜ ë…¸ë“œ"""

    # 1. YAML ì„¤ì • ì§ë ¬í™” (Kedro â†’ Subprocess)
    config_path = write_yaml_config(train_params, tokenized_dataset_path)

    # 2. MLflow ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í™˜ê²½ë³€ìˆ˜ ì „ë‹¬
    env = os.environ.copy()
    if mlflow.active_run():
        env["MLFLOW_RUN_ID"] = mlflow.active_run().info.run_id
        env["MLFLOW_NESTED_RUN"] = "true"

    # 3. ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ìƒíƒœê³„ ì „í™˜!)
    cmd = ["deepspeed", "--num_gpus", str(num_gpus), "src/train/main_yaml.py",
           "--config_yml", str(config_path)]
    subprocess.run(cmd, env=env, check=True)

    # 4. ì œì–´ê¶Œ ë³µê·€ í›„ MLflow ì•„í‹°íŒ©íŠ¸ ë¡œê¹… (Kedro Layer)
    if mlflow.active_run():
        mlflow.log_artifacts(output_dir, artifact_path="final_model")

    # 5. Kedro ì¶œë ¥ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {"config_path": str(config_path), "metrics": load_metrics()}
```

**íŠ¹ì§•**:
- âœ… Kedro ë…¸ë“œ ê³„ì•½ ì¤€ìˆ˜ (ìž…ë ¥/ì¶œë ¥)
- âœ… Subprocess ì‹¤í–‰ ë° ì—ëŸ¬ ì „íŒŒ
- âœ… MLflow ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´
- âœ… ì•„í‹°íŒ©íŠ¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬
- âš ï¸ ë™ê¸° ì‹¤í–‰ (ë¸”ë¡œí‚¹)

##### Function Inventory by Ecosystem (ìƒíƒœê³„ë³„ í•¨ìˆ˜ ëª©ë¡)

| í•¨ìˆ˜ ìœ„ì¹˜ | í•¨ìˆ˜ëª… | ìƒíƒœê³„ | ì—­í•  | ìƒíƒœ |
|---------|-------|--------|-----|-----|
| `pipelines/ingestion/nodes.py` | `load_data` | Kedro | ë°ì´í„° ë¡œë“œ | Stateless |
| `pipelines/preprocess/nodes.py` | `clean_data` | Kedro | ì¤‘ë³µ/null ì œê±° | Stateless |
| `pipelines/feature/nodes.py` | `build_features` | Kedro | í”¼ì²˜ ìƒì„± | Stateless |
| `pipelines/split/nodes.py` | `create_dataset` | Kedro | HF Dataset ë³€í™˜ | Stateless |
| `pipelines/train/nodes.py` | `tokenize_datasets` | Kedro | í† í°í™” | Stateless |
| `pipelines/train/nodes.py` | `launch_training` | **Bridge** | ìƒíƒœê³„ ì „í™˜ | Hybrid |
| `train/main_yaml.py` | `main` | Subprocess | ë¶„ì‚° í•™ìŠµ ì‹¤í–‰ | Stateful |
| `utils/common.py` | `WeightedTrainer` | Shared | ê°€ì¤‘ì¹˜ ì†ì‹¤ ê³„ì‚° | Stateful |
| `utils/common.py` | `build_training_arguments` | Shared | TrainingArguments ìƒì„± | Stateless |
| `utils/common.py` | `GPUMemoryCallback` | Subprocess | GPU ëª¨ë‹ˆí„°ë§ | Stateful |

## Change Log

### 2025-10-17 Â· Subprocess Block Refinement & Kedro-Style Declarative Pipeline v2.3
- **REFINEMENT**: Expanded from 8 to 10 blocks with finer granularity (45% code reduction: 328 â†’ 181 lines)
- **Model Initialization Split** (critical 4-stage separation):
  1. `load_datasets` - Data loading only (separated from tokenizer)
  2. `initialize_tokenizer` - Tokenizer configuration only (separated from data)
  3. `initialize_model` - Base model initialization only (separated from LoRA)
  4. `apply_lora_to_model` - LoRA optimization only (separated from base model)
  5. `build_weighted_trainer` - WeightedTrainer construction (renamed from `prepare_trainer_components`)
- **DECLARATIVE PIPELINE**: Converted main() to Kedro pipeline.py style
  - Dict-based block definitions with metadata (`func`, `inputs`, `outputs`, `name`, `description`)
  - State dict pattern for input/output resolution
  - For-loop execution maintaining explicit data flow
  - Visual and structural consistency with Kedro ecosystem
- **10 Block Functions**:
  1. `setup_training_context` - Initialize training environment
  2. `load_datasets` - Load tokenized datasets
  3. `initialize_tokenizer` - Configure tokenizer and infer num_labels
  4. `initialize_model` - Load base model without optimization
  5. `apply_lora_to_model` - Apply LoRA optimization if configured
  6. `build_weighted_trainer` - Create WeightedTrainer with all components
  7. `patch_mlflow_callback` - Override MLflow callback to prevent hang
  8. `execute_training_loop` - Run training loop with checkpoint support
  9. `evaluate_and_save_results` - Evaluate on test set and save model
  10. `cleanup_distributed_process_group` - Clean up distributed resources
- **Architecture Philosophy Compliance**: 100% alignment with ëŒ€ì¹­í™”Â·ëª¨ë“ˆí™”Â·ìˆœì„œí™” principles
  - âœ… ëŒ€ì¹­í™” (Pattern): Perfect symmetry with Kedro pipeline.py declarative structure
  - âœ… ëª¨ë“ˆí™” (Modularity): 10 single-responsibility functions in common.py
  - âœ… ìˆœì„œí™” (Ordering): Explicit 1-10 pipeline flow with metadata
- **Benefits**:
  - Enhanced testability: Each block independently testable
  - Improved maintainability: Single responsibility per block
  - Better reusability: Finer-grained composable functions
  - Consistent style: Matches Kedro ecosystem patterns exactly
- Updated tests to validate all 10 block functions
- Updated architecture.md with v2.3 refinement documentation
- Updated architecture version to 2.3.0

### 2025-10-17 Â· Subprocess Block-based Pipeline Refactoring v2.2 (superseded by v2.3)
- **INITIAL REFACTORING**: Converted subprocess main_yaml.py from monolithic (328 lines) to block-based pipeline
- Added 8 block functions to common.py following Kedro philosophy
- Introduced context objects for state management (`TrainingContext`, `TrainingArtifacts`)
- Added smoke tests for block functions (`tests/train/test_training_blocks.py`)
- **Note**: This version was immediately refined to v2.3 with finer block granularity and declarative pipeline structure

### 2025-10-17 Â· Dual-Ecosystem Architecture Documentation v2.1
- Added comprehensive dual-ecosystem architecture documentation
- Documented Kedro Layer vs Subprocess Layer design philosophy
- Analyzed current implementation patterns and function inventory
- Added integration mechanisms and design rationale
- Documented bridge node pattern (`launch_training`)
- Updated architecture version to 2.1.0

### 2025-09-29 Â· Architecture Documentation Update v2.0
- Corrected pipeline structure: 5 main pipelines (ingestion â†’ preprocess â†’ feature â†’ split â†’ train), not 10 stages
- Added comprehensive Package Version Management section
- Updated function inventory tables to match actual implementations
- Added MLflow integration architecture details
- Documented evaluation pipeline status and missing implementations
- Added system-wide compatibility matrix

### 2025-09-24 Â· Dataset ë¦¬íŒ©í„°ë§

- Feature íŒŒì´í”„ë¼ì¸ ë§ˆì§€ë§‰ ë…¸ë“œë¥¼ `prepare_dataset_inputs`ë¡œ ë‹¨ìˆœí™”í•˜ì—¬ `base_table`ë§Œ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½.
- Split íŒŒì´í”„ë¼ì¸ì— `create_dataset`ì„ ì¶”ê°€í•´ HuggingFace `Dataset` ìƒì„±ê³¼ ë¼ë²¨ ìŠ¬ë¡¯ êµ¬ì„±ì„ ë‹´ë‹¹í•˜ë„ë¡ ì´ë™.
- `serialize_for_nlp`ê°€ `retain_columns` ì˜µì…˜ì„ ì‚¬ìš©í•´ `text`, `labels`, `acct_code`ë§Œ ìœ ì§€í•˜ë„ë¡ ì •ë¦¬.
- ìƒì„¸ íë¦„ê³¼ í•¨ìˆ˜ ëª©ë¡ì€ ì•„ëž˜ â€œì„¸ë¶€ ì•„í‚¤í…ì²˜â€ ì„¹ì…˜ì„ í™•ì¸í•˜ì„¸ìš”.

## Pipeline Status & Boundaries

| Pipeline | Status | Notes |
|----------|--------|-------|
| Ingestion | âœ… Complete | Raw parquet â†’ standardized frame |
| Preprocess | âœ… Complete | Clean/filter/normalize + missing value unification |
| Feature | âœ… Complete | Date/holiday features, selection |
| Split | âœ… Complete | HF Dataset creation, labelize, serialization, token diagnostics |
| Train | âš ï¸ In progress | LoRA/Deepspeed training integration pending |
| Inference | â³ Planned | Mirrors tokenization + model loading |

### Split ì „ Â· í›„ ê²½ê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Split ì´ì „ (ê³µìš© ë°ì´í„° ìƒì„±)
â”‚ Ingestion â”€ Preprocess â”€ Feature â”€ (Scaling) â”‚
â”‚  â€¢ ê²°ì¸¡ì¹˜/ì½”ë“œ/ìˆ˜ì¹˜ ì •ê·œí™”
â”‚  â€¢ íŒŒìƒ í”¼ì²˜ ìƒì„±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“ base_table (ê³µìœ )
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Split ì´í›„ (í•™ìŠµ/ì‹¤í—˜ ì˜ì—­)
â”‚ create_dataset â†’ to_hf_and_split â†’ labelize_and_cast â”‚
â”‚ serialize_for_nlp â†’ analyze_token_lengths â†’ export_*  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Split ì´ì „ íŒŒì´í”„ë¼ì¸**ì€ ìž¬ì‚¬ìš© ê°€ëŠ¥í•œ DataFrame(ìŠ¤ì¼€ì¼ë§ í¬í•¨)ì„ ë§Œë“œëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.
- **Split ì´í›„ íŒŒì´í”„ë¼ì¸**ì€ ì‹¤í—˜ë§ˆë‹¤ ë°”ë€ŒëŠ” ìš”ì†Œ(Split ë¹„ìœ¨, ë¼ë²¨ ì •ìˆ˜í™”, í…ìŠ¤íŠ¸ ì§ë ¬í™”, í† í° ì§„ë‹¨, í•™ìŠµ ì„¤ì •)ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
- `train_pipeline.yml`ì˜ `split.*` íŒŒë¼ë¯¸í„°ë¡œ ë™ì¼ ìž…ë ¥ì— ëŒ€í•´ ì—¬ëŸ¬ í•™ìŠµ ë²„ì „ì„ ìœ ì—°í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

## Package Version Management

### Core Dependencies Matrix

#### Python Runtime
- **Required**: Python >= 3.12, < 3.14
- **Tested**: Python 3.12.x, 3.13.x
- **Rationale**: Balance between modern features and ecosystem compatibility

#### Framework Stack

| Package | Version | Purpose | Compatibility Notes |
|---------|---------|---------|--------------------|
| **Kedro** | 1.0.0 (exact) | Pipeline orchestration | Major version pinned for stability |
| **kedro-datasets** | >=8.1.0 | Extended dataset types | Includes pandas support |
| **kedro-mlflow** | >=1.0.0, <1.1.0 | MLflow integration | Tight version range for API stability |
| **kedro-viz** | 12.0.0 (exact) | Pipeline visualization | Fixed to prevent ipython conflicts |

#### Data Processing Stack

| Package | Version | Purpose | Compatibility Notes |
|---------|---------|---------|--------------------|
| **pandas** | >=2.3.2 | DataFrame operations | Latest 2.x for performance |
| **pyarrow** | >=19.0.0, <20.0.0 | Parquet I/O | Major version constraint |
| **PySpark** | >=4.0.1 | Large-scale processing | 4.x for Spark 3.x compatibility |
| **numpy** | >=1.21.0 | Numerical operations | Compatible with pandas 2.x |

#### ML/AI Stack

| Package | Version | Purpose | Compatibility Notes |
|---------|---------|---------|--------------------|
| **scikit-learn** | >=1.7.1 | ML utilities | Latest for evaluation metrics |
| **datasets** | >=3.0.0, <3.1.0 | HuggingFace datasets | Major version constraint |
| **mlflow** | >=2.22, <3.0 | Experiment tracking | 2.x for stability |
| **mlflow-skinny** | >=2.22, <3.0 | Lightweight MLflow | Must match mlflow version |

#### Development Stack

| Package | Version | Purpose | Compatibility Notes |
|---------|---------|---------|--------------------|
| **ipython** | 8.26.0 (exact) | Interactive shell | <9.0 for kedro-viz compatibility |
| **jupyter** | Latest | Notebooks | Via jupyterlab>=4.4.7 |
| **holidays** | >=0.81 | Holiday features | For date engineering |

### Version Conflict Resolution

1. **IPython < 9.0 constraint**: Required by kedro-viz 12.0.0 to prevent visualization conflicts
2. **PyArrow 19.x**: Pinned to major version for Parquet format stability
3. **MLflow 2.x**: Avoiding 3.x migration until ecosystem matures
4. **Datasets 3.0.x**: Minor version constraint for HuggingFace API stability

### Upgrade Path Recommendations

- **Q1 2026**: Evaluate Kedro 2.x migration when ecosystem stabilizes
- **Q2 2026**: Consider MLflow 3.x after plugin compatibility verified
- **Continuous**: Monitor pyarrow for performance improvements in 20.x

## ì„¸ë¶€ ì•„í‚¤í…ì²˜

### System Architecture Overview

```mermaid
graph LR
    A[Raw Data<br/>Parquet] -->|load| B[Ingestion<br/>Pipeline]
    B -->|standardize| C[Preprocess<br/>Pipeline]
    C -->|clean/filter| D[Feature<br/>Pipeline]
    D -->|engineer| E[Split<br/>Pipeline]
    E -->|HF Dataset| F[Train<br/>Pipeline]
    F -->|tokenize| G[Model Ready<br/>Data]

    B -.->|MLflow| M[Tracking<br/>Store]
    C -.->|params| M
    D -.->|params| M
    E -.->|artifacts| M
    F -.->|artifacts| M
```

### Block Architecture Principles

ì„¤ê³„ìžëŠ” í´ë” â†’ íŒŒì¼ â†’ í•¨ìˆ˜ ë‹¨ìœ„ì˜ ë¸”ë¡ êµ¬ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„° ê³„ì•½ê³¼ ì˜ì¡´ ê´€ê³„ë¥¼ ì¶”ì í•©ë‹ˆë‹¤. ë³¸ ë¬¸ì„œëŠ” í˜„ìž¬ êµ¬í˜„ ìƒíƒœë¥¼ ë°˜ì˜í•˜ë©°, ëª¨ë“  í•¨ìˆ˜ì˜ ì±…ìž„ê³¼ íë¦„ì„ `ëŒ€ë¶„ë¥˜/ì¤‘ë¶„ë¥˜/í•¨ìˆ˜ì´ë¦„/ì—­í• /ìž…ë ¥/ì¶œë ¥/í›„ì†ë¸”ë¡` ê¸°ì¤€ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

#### 1. íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ ê°œìš”

##### 1.1 Pipeline Structure (5 Main Pipelines)
```
Ingestion â†’ Preprocess â†’ Feature â†’ Split â†’ Train
   â†“           â†“           â†“         â†“       â†“
[3 nodes]   [4 nodes]   [4 nodes] [5 nodes] [2 nodes]
```

##### 1.2 Registered Pipeline Combinations
- **`__default__`**: Alias for `full_preprocess` (Ingestion â†’ Split)
- **`full_preprocess`**: ingestion + preprocess + feature + split
- **`full`**: full_preprocess + train (complete pipeline)
- **`data_prep`**: ingestion + preprocess + feature (pre-split preparation)
- **Individual**: `ingestion`, `preprocess`, `feature`, `split`, `train`
- **Disabled**: `evaluation` (missing node implementations)

##### 1.3 Data Flow Architecture
- **Input**: `raw_account_data` (Parquet) â†’ pandas.DataFrame
- **Processing**: DataFrame maintained through feature engineering
- **Transformation**: HuggingFace Dataset creation at split stage
- **Output**: `trainer_ready_data` (tokenized DatasetDict)

##### 1.4 MLflow Integration Points
- **Artifact Storage**: `MlflowArtifactDataset` for `prepared_datasets_mlflow`, `text_datasets_mlflow`
- **Parameter Logging**: Automatic from parameters/*.yml files
- **Experiment Tracking**: Via `account_tax_experiment` with random run names
- **Tracking Store**: Local `mlruns/` directory

#### 2. ë°ì´í„° ê³„ì•½ í•µì‹¬
- ìž…ë ¥ ì›ì²œ: `raw_account_data` (`data/01_raw/row_data.parquet`) â†’ `pandas.DataFrame`
- ì „ì²˜ë¦¬: `standardized_data`~`validated_data`ê¹Œì§€ `DataFrame` ìœ ì§€, íŒŒë¼ë¯¸í„°ëŠ” `conf/base/parameters.yml`ì˜ `preprocess.*`
- íŠ¹ì„±í™”: `prepare_dataset_inputs`ê°€ `base_table`(`DataFrame`)ì„ ì œê³µí•˜ë©° ë¼ë²¨ ìŠ¬ë¡¯ì€ Split ë‹¨ê³„ì—ì„œ ê´€ë¦¬
- ë¶„í• : `create_dataset`ì´ HF `Dataset`ê³¼ ë¼ë²¨ ìŠ¬ë¡¯ì„ ìƒì„±í•˜ê³ , `to_hf_and_split`ì´ ì¸µí™” ë¶„í• ì„ ìˆ˜í–‰, `labelize_and_cast`ê°€ `ClassLabel` ìŠ¤í‚¤ë§ˆì™€ `label_metadata`ë¥¼ ë¶€ì°©í•´ `prepared_datasets`ë¡œ ì „ë‹¬
- í•™ìŠµ ì¤€ë¹„: `serialize_for_nlp`ì—ì„œ í…ìŠ¤íŠ¸ ì§ë ¬í™” â†’ `tokenize_datasets`ì—ì„œ í† í°í™” â†’ `prepare_for_trainer`ê°€ `trainer_ready_data`(datasets, collator, model_config)ë¥¼ ìµœì¢… ì‚°ì¶œ

#### 3. Pipeline Node Documentation

## Ingestion Pipeline (3 nodes)

### 1. **load_data**
   - **ìž…ë ¥**: `raw_data: pd.DataFrame` (from raw_account_data parquet file)
   - **ì¶œë ¥**: `validated_raw_data: pd.DataFrame` (validated non-empty DataFrame)
   - **ì„¤ëª…**: Loads raw accounting data from parquet file and validates it's not empty. Logs data shape and columns for monitoring.
   - **í›„ì†ë¸”ë¡**: standardize_columns

### 2. **standardize_columns**
   - **ìž…ë ¥**: `data: pd.DataFrame` (validated_raw_data)
   - **ì¶œë ¥**: `standardized_data: pd.DataFrame` (with English column names)
   - **ì„¤ëª…**: Standardizes Korean column names to English using predefined mapping. Handles 54 column mappings including accounting codes, party information, and tax fields.
   - **í›„ì†ë¸”ë¡**: extract_metadata, clean_data

### 3. **extract_metadata**
   - **ìž…ë ¥**: `data: pd.DataFrame` (standardized_data)
   - **ì¶œë ¥**: `metadata: Dict[str, Any]` (statistics and schema info)
   - **ì„¤ëª…**: Extracts comprehensive metadata including row/column counts, data types, memory usage, null counts, and column type categorization.
   - **í›„ì†ë¸”ë¡**: External monitoring/logging

## Preprocess Pipeline (4 nodes)

### 1. **clean_data**
   - **ìž…ë ¥**: `data: pd.DataFrame`, `clean_params: Dict` (from params:preprocess.clean)
   - **ì¶œë ¥**: `cleaned_data: pd.DataFrame`
   - **ì„¤ëª…**: Removes duplicate rows and handles missing values. Drops rows with nulls in critical columns specified in dropna_columns parameter.
   - **í›„ì†ë¸”ë¡**: filter_data

### 2. **filter_data**
   - **ìž…ë ¥**: `data: pd.DataFrame`, `filter_params: Dict` (from params:preprocess.filter)
   - **ì¶œë ¥**: `filtered_data: pd.DataFrame`
   - **ì„¤ëª…**: Excludes unnecessary columns specified in exclude_columns parameter. Typically removes id, created_at, batch_no columns.
   - **í›„ì†ë¸”ë¡**: normalize_value

### 3. **normalize_value**
   - **ìž…ë ¥**: `df: pd.DataFrame`, `code_mappings: Optional[Dict]`
   - **ì¶œë ¥**: `normalized_data: pd.DataFrame`
   - **ì„¤ëª…**: Converts code values to human-readable text. Maps VAT message codes, document types, and party type codes to meaningful labels.
   - **í›„ì†ë¸”ë¡**: validate_data

### 4. **validate_data**
   - **ìž…ë ¥**: `data: pd.DataFrame`, `params: Dict`
   - **ì¶œë ¥**: `validated_data: pd.DataFrame`
   - **ì„¤ëª…**: Applies business rule validation. Filters by amount thresholds and validates date formats, removing invalid records.
   - **í›„ì†ë¸”ë¡**: build_features

### 5. **normalize_missing_values** (optional, not in pipeline)
   - **ìž…ë ¥**: `data: pd.DataFrame`, `placeholders: Dict`
   - **ì¶œë ¥**: `normalized_data: pd.DataFrame`
   - **ì„¤ëª…**: Standardizes missing values with appropriate placeholders per data type (categorical: "__missing__", numeric: 0, etc.).
   - **í›„ì†ë¸”ë¡**: Not currently integrated

## Feature Pipeline (4 nodes)

### 1. **build_features**
   - **ìž…ë ¥**: `data: pd.DataFrame`, `params: Dict` (from params:feature.engineering)
   - **ì¶œë ¥**: `feature_data: pd.DataFrame`
   - **ì„¤ëª…**: Creates derived features including holiday features. Calls add_holiday_features internally to add day_type column.
   - **í›„ì†ë¸”ë¡**: select_features

### 2. **add_holiday_features** (helper function)
   - **ìž…ë ¥**: `df: pd.DataFrame`, `date_column: str`
   - **ì¶œë ¥**: `df: pd.DataFrame` (with day_type column)
   - **ì„¤ëª…**: Adds day_type column marking weekends and Korean public holidays as 'holiday', others as 'workday'.
   - **í›„ì†ë¸”ë¡**: Called within build_features

### 3. **select_features**
   - **ìž…ë ¥**: `data: pd.DataFrame`, `params: Dict` (from params:feature.selection)
   - **ì¶œë ¥**: `selected_features: pd.DataFrame`
   - **ì„¤ëª…**: Selects specific features and label columns in configured order. Typically selects 24 features + 1 label column.
   - **í›„ì†ë¸”ë¡**: prepare_dataset_inputs

### 4. **prepare_dataset_inputs**
   - **ìž…ë ¥**: `data: pd.DataFrame`, `params: Dict` (from params:feature.dataset_conversion)
   - **ì¶œë ¥**: `base_table: pd.DataFrame`
   - **ì„¤ëª…**: Prepares cleaned base table for HuggingFace Dataset conversion. Removes null labels and duplicates, logs unique label count.
   - **í›„ì†ë¸”ë¡**: create_dataset

## Split Pipeline (6 main nodes + 3 helpers)

### Helper Functions

#### **_initialize_label_slots** (helper)
   - **ìž…ë ¥**: `max_classes: int`, `dummy_prefix: str`
   - **ì¶œë ¥**: `List[str]` (dummy label slots)
   - **ì„¤ëª…**: Creates deterministic dummy label slots like ["dummy1", "dummy2", ...] up to max_classes.

#### **_upsert_labels_into_slots** (helper)
   - **ìž…ë ¥**: `names: List[str]`, `new_labels: List[str]`, `dummy_prefix: str`
   - **ì¶œë ¥**: `names: List[str]` (updated with real labels)
   - **ì„¤ëª…**: Replaces dummy slots with real labels while preserving index positions.

#### **make_label2id** / **make_id2label** (helpers)
   - **ìž…ë ¥**: `names: List[str]`
   - **ì¶œë ¥**: `Dict[str, int]` or `Dict[int, str]`
   - **ì„¤ëª…**: Creates bidirectional mappings between label names and integer IDs.

### Main Nodes

### 1. **create_dataset**
   - **ìž…ë ¥**: `base_table: pd.DataFrame`, `params: Dict` (from params:split)
   - **ì¶œë ¥**: `dataset: Dataset`, `names: List[str]`
   - **ì„¤ëª…**: Creates HuggingFace Dataset from pandas DataFrame and initializes label slot system with max_classes slots.
   - **í›„ì†ë¸”ë¡**: to_hf_and_split

### 2. **to_hf_and_split**
   - **ìž…ë ¥**: `dataset: Dataset`, `label_col: str`, `seed: int`, `test_size: float`, `val_size: float`
   - **ì¶œë ¥**: `splits: DatasetDict` (train/valid/test)
   - **ì„¤ëª…**: Performs stratified train/validation/test split. Falls back to random split if stratification fails.
   - **í›„ì†ë¸”ë¡**: labelize_and_cast

### 3. **labelize_and_cast**
   - **ìž…ë ¥**: `splits: DatasetDict`, `names: List[str]`, `label_col: str`, `dummy_label: str`, `num_proc: int`
   - **ì¶œë ¥**: `labeled_datasets: DatasetDict` (with integer labels and ClassLabel schema)
   - **ì„¤ëª…**: Maps string labels to integers and applies ClassLabel schema. Attaches label_metadata to DatasetDict.
   - **í›„ì†ë¸”ë¡**: serialize_for_nlp

### 4. **serialize_for_nlp**
   - **ìž…ë ¥**: `dataset_dict: DatasetDict`, `params: Dict` (from params:train.serialization)
   - **ì¶œë ¥**: `text_datasets: DatasetDict` (with text column)
   - **ì„¤ëª…**: Serializes structured data into text format for NLP. Creates "column: value" format strings, retains only text and labels columns.
   - **í›„ì†ë¸”ë¡**: analyze_token_lengths or tokenize_datasets

### 5. **analyze_token_lengths** (optional diagnostic)
   - **ìž…ë ¥**: `dataset_dict: DatasetDict`, `tokenization_params: Dict`, `diagnostics_params: Dict`
   - **ì¶œë ¥**: `dataset_dict: DatasetDict`, `report: Dict` (token statistics)
   - **ì„¤ëª…**: Generates token length statistics and samples. Logs percentiles and mean/max token counts to MLflow.
   - **í›„ì†ë¸”ë¡**: tokenize_datasets

### 6. **export_prepared_partitions** / **export_text_partitions**
   - **ìž…ë ¥**: `dataset_dict: DatasetDict`
   - **ì¶œë ¥**: `partitions: Dict[str, pd.DataFrame]`
   - **ì„¤ëª…**: Converts DatasetDict splits back to partitioned pandas DataFrames for storage.
   - **í›„ì†ë¸”ë¡**: MLflow artifact storage

## Train Pipeline (6 implemented nodes, 2 in pipeline)

### 1. **tokenize_datasets**
   - **ìž…ë ¥**: `dataset_dict: DatasetDict`, `params: Dict` (from params:train.tokenization)
   - **ì¶œë ¥**: `tokenized_datasets: DatasetDict`, `metadata: Dict`
   - **ì„¤ëª…**: Tokenizes text data using HuggingFace tokenizer. Produces input_ids and attention_mask, removes text column to save memory.
   - **í›„ì†ë¸”ë¡**: load_model or prepare_trainer

### 2. **load_model** (not in pipeline)
   - **ìž…ë ¥**: `tokenized_data: DatasetDict`, `params: Dict` (from params:train.model)
   - **ì¶œë ¥**: `model: AutoModelForSequenceClassification`, `metadata: Dict`
   - **ì„¤ëª…**: Loads pre-trained model with proper configuration. Infers num_labels from data, enables gradient checkpointing, handles device mapping.
   - **í›„ì†ë¸”ë¡**: apply_optimization

### 3. **apply_optimization** (not in pipeline)
   - **ìž…ë ¥**: `model: Any`, `params: Dict` (from params:train.optimization)
   - **ì¶œë ¥**: `optimized_model: Any`, `metadata: Dict`
   - **ì„¤ëª…**: Applies LoRA optimization and/or torch.compile. Reduces trainable parameters from billions to millions with LoRA.
   - **í›„ì†ë¸”ë¡**: prepare_trainer

### 4. **prepare_trainer** (in pipeline as prepare_for_trainer)
   - **ìž…ë ¥**: `model: Any`, `tokenized_data: DatasetDict`, `params: Dict`
   - **ì¶œë ¥**: `trainer_components: Dict`, `metadata: Dict`
   - **ì„¤ëª…**: Prepares HuggingFace Trainer with all configurations. Sets up data collator, training arguments, DeepSpeed config, and metrics computation.
   - **í›„ì†ë¸”ë¡**: train_model

### 5. **train_model** (not in pipeline)
   - **ìž…ë ¥**: `trainer_components: Dict`, `params: Dict`
   - **ì¶œë ¥**: `model: Any`, `metrics: Dict`, `artifacts: Dict`
   - **ì„¤ëª…**: Executes model training using HuggingFace Trainer. Saves final model and tokenizer, returns training metrics.
   - **í›„ì†ë¸”ë¡**: evaluate_model

### 6. **evaluate_model** (not in pipeline)
   - **ìž…ë ¥**: `trainer_components: Dict`, `params: Dict`
   - **ì¶œë ¥**: `metrics: Dict`, `artifacts: Dict`
   - **ì„¤ëª…**: Evaluates trained model on validation and test sets. Generates predictions and confusion matrices if requested.
   - **í›„ì†ë¸”ë¡**: Model deployment or further analysis

## Evaluation Pipeline (2 implemented, 4 missing)

### Implemented Nodes

### 1. **evaluate_predictions**
   - **ìž…ë ¥**: `y_true: pd.Series`, `y_pred: pd.Series`
   - **ì¶œë ¥**: `metrics: Dict` (accuracy, precision, recall, f1, confusion matrix)
   - **ì„¤ëª…**: Calculates classification metrics using sklearn. Computes accuracy, precision, recall, F1 score with weighted averaging.
   - **ìƒíƒœ**: âœ… Implemented but not connected to pipeline

### 2. **calculate_tax_impact**
   - **ìž…ë ¥**: `predictions: pd.DataFrame` (with actual and predicted tax categories)
   - **ì¶œë ¥**: `impact: Dict` (misclassified_amount, percentage, category breakdown)
   - **ì„¤ëª…**: Estimates financial impact of tax misclassifications. Calculates misclassified amounts and percentages by category.
   - **ìƒíƒœ**: âœ… Implemented but not connected to pipeline

### Missing Nodes (defined in pipeline but not implemented)

### 3. **evaluate_classification_model** âŒ
   - **Expected ìž…ë ¥**: `y_test`, `y_pred`, `y_proba`
   - **Expected ì¶œë ¥**: `classification_metrics`
   - **ì„¤ëª…**: Should evaluate classification model performance
   - **ìƒíƒœ**: âŒ Pipeline expects this but node not implemented

### 4. **evaluate_tax_classification** âŒ
   - **Expected ìž…ë ¥**: `test_predictions`, `tax_categories`
   - **Expected ì¶œë ¥**: `tax_metrics`
   - **ì„¤ëª…**: Should evaluate tax-specific classification accuracy
   - **ìƒíƒœ**: âŒ Pipeline expects this but node not implemented

### 5. **calculate_business_metrics** âŒ
   - **Expected ìž…ë ¥**: `test_predictions`, `test_actuals`
   - **Expected ì¶œë ¥**: `business_metrics`
   - **ì„¤ëª…**: Should calculate business impact metrics
   - **ìƒíƒœ**: âŒ Pipeline expects this but node not implemented

### 6. **generate_evaluation_report** âŒ
   - **Expected ìž…ë ¥**: `classification_metrics`, `tax_metrics`, `model_info`
   - **Expected ì¶œë ¥**: `evaluation_report`
   - **ì„¤ëª…**: Should generate comprehensive evaluation report
   - **ìƒíƒœ**: âŒ Pipeline expects this but node not implemented

## Utility Functions (not integrated into pipelines)

### 1. **categorize_accounts**
   - **ìž…ë ¥**: `df: pd.DataFrame`, `account_mapping: Dict`
   - **ì¶œë ¥**: `df: pd.DataFrame` (with account_category column)
   - **ì„¤ëª…**: Maps accounts to categories based on provided mapping dictionary. Adds account_category column to DataFrame.
   - **ìƒíƒœ**: âš ï¸ Implemented but unused

### 2. **calculate_tax_categories**
   - **ìž…ë ¥**: `df: pd.DataFrame`
   - **ì¶œë ¥**: `df: pd.DataFrame` (with tax_category column)
   - **ì„¤ëª…**: Derives tax categories based on keyword matching in account descriptions. Creates tax_category column.
   - **ìƒíƒœ**: âš ï¸ Implemented but unused

### 3. **aggregate_by_period**
   - **ìž…ë ¥**: `df: pd.DataFrame`, `period_column: str`, `aggregation_level: str`
   - **ì¶œë ¥**: `df: pd.DataFrame` (with period column)
   - **ì„¤ëª…**: Aggregates data by time period (daily/weekly/monthly/quarterly/yearly). Adds period column for grouping.
   - **ìƒíƒœ**: âš ï¸ Implemented but unused

## System Registry & Entry Points

### 1. **register_pipelines** (pipeline_registry.py)
   - **ìž…ë ¥**: None
   - **ì¶œë ¥**: `Dict[str, Pipeline]` (all registered pipelines)
   - **ì„¤ëª…**: Central registration point for all pipelines. Defines __default__, full_preprocess, full, data_prep, and individual pipeline mappings.
   - **í›„ì†ë¸”ë¡**: Kedro CLI commands

### 2. **main** (__main__.py)
   - **ìž…ë ¥**: CLI arguments
   - **ì¶œë ¥**: Exit code (0 for success)
   - **ì„¤ëª…**: Entry point for Kedro session execution. Handles kedro run and other CLI commands.
   - **í›„ì†ë¸”ë¡**: Terminal/shell execution

#### 4. Architecture Health Check

##### 4.1 Implementation Status
| Component | Status | Notes |
|-----------|--------|-------|
| Ingestion Pipeline | âœ… Complete | 3 nodes fully implemented |
| Preprocess Pipeline | âœ… Complete | 4 nodes with business rules |
| Feature Pipeline | âœ… Complete | 4 nodes including holidays |
| Split Pipeline | âœ… Complete | 5 nodes with HF integration |
| Train Pipeline | âœ… Complete | 2 nodes for tokenization |
| Evaluation Pipeline | âš ï¸ Partial | 2/6 nodes implemented |
| Utils Module | âš ï¸ Unused | 3 functions not integrated |

##### 4.2 Critical Issues
1. **Evaluation Pipeline Gaps**:
   - Missing: `evaluate_classification_model`, `evaluate_tax_classification`
   - Missing: `generate_evaluation_report`, `calculate_business_metrics`
   - Impact: Cannot run evaluation pipeline without implementation
   - Resolution: Either implement missing nodes or update pipeline definition

2. **Unused Utilities**:
   - `categorize_accounts`: Account categorization logic unused
   - `calculate_tax_categories`: Tax category calculation unused
   - `aggregate_by_period`: Period aggregation unused
   - Resolution: Consider integration or removal

##### 4.3 Architectural Strengths
- âœ… Clear separation of concerns across pipelines
- âœ… Consistent data contracts between stages
- âœ… Label metadata preservation through splitâ†’train
- âœ… MLflow artifact tracking at key checkpoints
- âœ… Memory-efficient with MemoryDataset for intermediates

##### 4.4 Configuration Dependencies
- âš ï¸ `split.max_classes` and `split.dummy_prefix` control label slot allocation
- âš ï¸ Changes to these parameters affect downstream ClassLabel schema
- âš ï¸ `feature.selection.features` list must match actual DataFrame columns

#### 5. Architectural Recommendations (ì•„í‚¤í…ì²˜ ê¶Œìž¥ì‚¬í•­)

##### 5.1 Maintaining Design Principles (ì„¤ê³„ ì›ì¹™ ìœ ì§€)

**ëŒ€ì¹­í™”(Pattern) - ì¼ê´€ì„± ìœ ì§€ ì „ëžµ**

1. **Kedro ë…¸ë“œ í…œí”Œë¦¿ ê°•ì œ**
   ```python
   # ëª¨ë“  Kedro ë…¸ë“œëŠ” ì´ í…œí”Œë¦¿ì„ ë”°ë¼ì•¼ í•¨
   def {pipeline_name}_{action}(
       data: InputType,
       params: Dict[str, Any]
   ) -> OutputType:
       """
       Pipeline Order: {pipeline_name} step {number} ({position})
       Role: {responsibility_description}

       Args:
           data: {input_description}
           params: {param_description}

       Returns:
           {output_description}
       """
       logger.info("Starting {action}...")
       result = transform(data, params)
       logger.info("Completed {action}")
       return result
   ```

2. **Subprocess ìŠ¤í¬ë¦½íŠ¸ í…œí”Œë¦¿**
   ```python
   # ëª¨ë“  subprocess ìŠ¤í¬ë¦½íŠ¸ëŠ” main_yaml.py íŒ¨í„´ì„ ë”°ë¦„
   def main() -> None:
       # 1. Parse arguments
       args = parse_args()

       # 2. Load config
       cfg = load_config(args.config_yml)

       # 3. Initialize distributed environment
       setup_distributed()

       # 4. Execute core logic
       execute(cfg)

       # 5. Cleanup distributed resources
       cleanup_distributed()
   ```

**ëª¨ë“ˆí™”(Modularity) - ê²½ê³„ ìœ ì§€ ì „ëžµ**

1. **Kedro ë…¸ë“œ ì±…ìž„ ê²½ê³„**
   - âœ… ë°ì´í„° ë³€í™˜ë§Œ ìˆ˜í–‰
   - âœ… íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì • ì£¼ìž…
   - âœ… ë°˜í™˜ê°’ìœ¼ë¡œë§Œ ì¶œë ¥
   - âŒ ì™¸ë¶€ ìƒíƒœ ë³€ê²½ ê¸ˆì§€
   - âŒ íŒŒì¼ ì‹œìŠ¤í…œ ì§ì ‘ ì ‘ê·¼ ìµœì†Œí™” (Catalog ì‚¬ìš©)

2. **Subprocess ì±…ìž„ ê²½ê³„**
   - âœ… ë³µìž¡í•œ ìƒíƒœ ê´€ë¦¬ í—ˆìš©
   - âœ… ë¶„ì‚° í†µì‹  ì²˜ë¦¬
   - âœ… ë™ì  ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
   - âŒ Kedro ë°ì´í„° ì¹´íƒˆë¡œê·¸ ì§ì ‘ ì ‘ê·¼ ê¸ˆì§€
   - âŒ MLflow ë¶€ëª¨ run ë³€ê²½ ê¸ˆì§€

3. **Bridge ë…¸ë“œ ì±…ìž„**
   - âœ… YAML ì„¤ì • ì§ë ¬í™”
   - âœ… í™˜ê²½ë³€ìˆ˜ ì „ë‹¬
   - âœ… ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ë° ì—ëŸ¬ ì²˜ë¦¬
   - âœ… ì•„í‹°íŒ©íŠ¸ í›„ì²˜ë¦¬ (MLflow ë¡œê¹…)
   - âŒ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ ë¡œì§ í¬í•¨ ê¸ˆì§€

**ìˆœì„œí™”(Ordering) - íë¦„ ëª…í™•í™” ì „ëžµ**

1. **íŒŒì´í”„ë¼ì¸ ìˆœì„œ ë¬¸ì„œí™”**
   - ëª¨ë“  ë…¸ë“œ ë…ìŠ¤íŠ¸ë§ì— "Pipeline Order: {pipeline} step {n}" ëª…ì‹œ
   - `architecture.md`ì— ì „ì²´ íë¦„ ë‹¤ì´ì–´ê·¸ëž¨ ìœ ì§€
   - ê° ë…¸ë“œì˜ "í›„ì†ë¸”ë¡" ëª…ì‹œ

2. **ìƒíƒœê³„ ì „í™˜ íë¦„ ëª…í™•í™”**
   ```
   Kedro Node (tokenize_datasets)
       â†“ outputs: tokenized_dataset_path
   Kedro Node (launch_training) â† Bridge
       â†“ subprocess.run()
   Subprocess (main_yaml.py)
       â†“ writes: checkpoints, metrics
   Kedro Node (launch_training) â† Bridge ë³µê·€
       â†“ mlflow.log_artifacts()
   Pipeline ì¢…ë£Œ
   ```

##### 5.2 Growth Patterns (í™•ìž¥ íŒ¨í„´)

**ìƒˆë¡œìš´ Kedro íŒŒì´í”„ë¼ì¸ ì¶”ê°€ ì‹œ**

1. **ê¸°ì¡´ íŒ¨í„´ ì¤€ìˆ˜**
   ```
   pipelines/{new_pipeline}/
   â”œâ”€â”€ __init__.py
   â”œâ”€â”€ pipeline.py       # create_pipeline() í•¨ìˆ˜
   â””â”€â”€ nodes.py          # stateless ë³€í™˜ í•¨ìˆ˜ë§Œ
   ```

2. **ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸**
   - [ ] `architecture.md`ì— ìƒˆ íŒŒì´í”„ë¼ì¸ ì„¹ì…˜ ì¶”ê°€
   - [ ] Function Inventory í…Œì´ë¸” ì—…ë°ì´íŠ¸
   - [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ íë¦„ ë‹¤ì´ì–´ê·¸ëž¨ ê°±ì‹ 
   - [ ] ë°ì´í„° ê³„ì•½ ë¬¸ì„œí™” (ìž…ë ¥ íƒ€ìž… â†’ ì¶œë ¥ íƒ€ìž…)

**ìƒˆë¡œìš´ Subprocess ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€ ì‹œ**

1. **main_yaml.py íŒ¨í„´ ë³µì œ**
   ```
   src/train/
   â”œâ”€â”€ main_yaml.py           # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
   â”œâ”€â”€ main_inference.py      # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (ì‹ ê·œ)
   â””â”€â”€ main_evaluation.py     # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ (ì‹ ê·œ)
   ```

2. **Bridge ë…¸ë“œ ìƒì„±**
   ```python
   # pipelines/inference/nodes.py
   def launch_inference(
       model_path: str,
       test_data_path: str,
       inference_params: Dict[str, Any]
   ) -> Dict[str, Any]:
       # launch_training íŒ¨í„´ ë³µì œ
       config_path = write_yaml(inference_params)
       subprocess.run(["python", "src/train/main_inference.py",
                      "--config_yml", str(config_path)], check=True)
       return load_results()
   ```

**ìƒˆë¡œìš´ ê³µí†µ ìœ í‹¸ë¦¬í‹° ì¶”ê°€ ì‹œ**

1. **ìƒíƒœê³„ ë¶„ë¥˜**
   - Kedro ì „ìš© â†’ `src/account_tax/utils/kedro_helpers.py`
   - Subprocess ì „ìš© â†’ `src/account_tax/utils/training_helpers.py`
   - ê³µí†µ â†’ `src/account_tax/utils/common.py`

2. **Stateless vs Stateful ëª…ì‹œ**
   ```python
   # common.py
   def stateless_function(input: Type) -> Output:
       """Pure function for both ecosystems."""
       return transform(input)

   class StatefulComponent:
       """For subprocess ecosystem only."""
       def __init__(self):
           self.state = {}
   ```

##### 5.3 Anti-Patterns to Avoid (í”¼í•´ì•¼ í•  ì•ˆí‹°íŒ¨í„´)

1. **âŒ Kedro ë…¸ë“œì—ì„œ subprocess ì§ì ‘ ì‹¤í–‰**
   ```python
   # BAD: preprocess/nodes.py
   def clean_data(data):
       subprocess.run(["python", "cleanup.py"])  # ê¸ˆì§€!
       return data
   ```
   â†’ Bridge ë…¸ë“œë¡œ ë¶„ë¦¬í•˜ê±°ë‚˜ Kedro ë…¸ë“œë¡œ êµ¬í˜„

2. **âŒ Subprocessì—ì„œ Kedro ì¹´íƒˆë¡œê·¸ ì§ì ‘ ì ‘ê·¼**
   ```python
   # BAD: train/main_yaml.py
   from kedro.io import DataCatalog
   catalog = DataCatalog.from_config(...)  # ê¸ˆì§€!
   ```
   â†’ YAML ì„¤ì •ìœ¼ë¡œ ê²½ë¡œ ì „ë‹¬ ë°›ê¸°

3. **âŒ ìƒíƒœ ê³µìœ ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜**
   ```python
   # BAD: utils/common.py
   GLOBAL_MODEL_STATE = None  # ê¸ˆì§€!
   ```
   â†’ ëª…ì‹œì  íŒŒë¼ë¯¸í„° ì „ë‹¬ ë˜ëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš©

4. **âŒ í˜¼í•© ìƒíƒœê³„ ë¡œì§**
   ```python
   # BAD: í•œ í•¨ìˆ˜ì—ì„œ ì–‘ìª½ íŒ¨í„´ í˜¼ìš©
   def hybrid_function(data):
       # Kedro ìŠ¤íƒ€ì¼ ë³€í™˜
       result = transform(data)
       # Subprocess ìŠ¤íƒ€ì¼ ìƒíƒœ ê´€ë¦¬
       self.state.update(result)  # ê¸ˆì§€!
       return result
   ```
   â†’ ìƒíƒœê³„ë³„ë¡œ í•¨ìˆ˜ ë¶„ë¦¬

##### 5.4 Future Improvements

**5.4.1 Short-term (Q4 2025)**
- [ ] Complete evaluation pipeline with subprocess pattern
- [ ] Add inference pipeline following dual-ecosystem design
- [ ] Create Kedro node template generator CLI tool
- [ ] Implement automated architecture validation tests

**5.4.2 Medium-term (Q1 2026)**
- [ ] Migrate to Kedro 2.x (verify subprocess compatibility)
- [ ] Add async subprocess execution for parallel experiments
- [ ] Implement pipeline versioning with ecosystem metadata
- [ ] Create architecture compliance dashboard

**5.4.3 Long-term (2026)**
- [ ] Multi-model ensemble with subprocess orchestration
- [ ] Cloud deployment patterns (Kedro on orchestrator, subprocess on compute)
- [ ] CI/CD pipeline with dual-ecosystem testing
- [ ] AutoML integration via bridge nodes

##### 5.5 Ecosystem Migration Checklist (ìƒíƒœê³„ ì´ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸)

**Kedro ë…¸ë“œ â†’ Subprocess ì´ì „ ì‹œ**

- [ ] ë…¸ë“œ ê¸°ëŠ¥ì´ ë³µìž¡í•œ ìƒíƒœ ê´€ë¦¬ í•„ìš”?
- [ ] ë¶„ì‚° ì²˜ë¦¬ ë˜ëŠ” ë©€í‹°í”„ë¡œì„¸ìŠ¤ í•„ìš”?
- [ ] ì–‘ë°©í–¥ í†µì‹  ë˜ëŠ” ë™ì  ë™ê¸°í™” í•„ìš”?
- [ ] Bridge ë…¸ë“œ íŒ¨í„´ìœ¼ë¡œ ì „í™˜
- [ ] `architecture.md` ì—…ë°ì´íŠ¸
- [ ] ê¸°ì¡´ Kedro ë…¸ë“œ ì œê±° ë˜ëŠ” deprecated ë§ˆí‚¹

**Subprocess â†’ Kedro ë…¸ë“œ ì´ì „ ì‹œ**

- [ ] ë¡œì§ì´ ìˆœìˆ˜ ë³€í™˜ í•¨ìˆ˜ë¡œ ë‹¨ìˆœí™” ê°€ëŠ¥?
- [ ] ìƒíƒœ ê´€ë¦¬ ë¶ˆí•„ìš”?
- [ ] Kedro ì¶”ì  ë° ì‹œê°í™” í•„ìš”?
- [ ] Stateless í•¨ìˆ˜ë¡œ ë¦¬íŒ©í„°ë§
- [ ] Kedro ë…¸ë“œë¡œ êµ¬í˜„
- [ ] ê¸°ì¡´ subprocess ìŠ¤í¬ë¦½íŠ¸ ì œê±°

## MLflow Integration Architecture

### Configuration Structure
```yaml
# conf/base/mlflow.yml
server:
  mlflow_tracking_uri: mlruns    # Local tracking store
tracking:
  experiment:
    name: "account_tax_experiment"
  run:
    name: "${km.random_name:}"   # Dynamic run naming
    nested: false
  params:
    dict_params:
      flatten: true              # Flatten nested params
      recursive: true
      sep: "."
```

### MLflow Dataset Integration

#### Artifact Storage Pattern
```yaml
# MlflowArtifactDataset wrapper pattern
prepared_datasets_mlflow:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset
  dataset:
    type: account_tax.datasets.partitioned_parquet.PartitionedParquetDataset
    path: data/05_model_input/prepared_datasets
  artifact_path: data/prepared_datasets
```

#### Integration Points
1. **Parameter Logging**: Automatic from `parameters/*.yml`
2. **Artifact Storage**: Key datasets saved as MLflow artifacts
3. **Metrics Tracking**: Currently manual (evaluation pipeline incomplete)
4. **Model Registry**: Not yet implemented (future enhancement)

### Storage Strategy Options

| Strategy | Use Case | Configuration |
|----------|----------|---------------|
| **mlflow_only** | Cloud/distributed teams | All artifacts in MLflow |
| **local_only** | Development/debugging | No MLflow tracking |
| **both** (current) | Hybrid approach | Local + MLflow artifacts |

## Data Catalog Architecture

### Memory Management Strategy

#### Dataset Types by Layer
```
01_raw/        : ParquetDataset (persistent)
02_intermediate/: MemoryDataset (transient)
03_primary/    : MemoryDataset (transient)
04_feature/    : MemoryDataset (transient)
05_model_input/: MlflowArtifactDataset (persistent)
06_models/     : MlflowArtifactDataset (persistent)
08_reporting/  : MlflowArtifactDataset (persistent)
```

#### Memory Optimization Patterns
1. **MemoryDataset**: Used for intermediate transformations
2. **Garbage Collection**: Automatic between pipeline stages
3. **Checkpointing**: MLflow artifacts at key stages
4. **Lazy Loading**: Parquet files loaded on demand

### Custom Dataset Implementation

#### PartitionedParquetDataset
- **Purpose**: Handle DatasetDict as partitioned parquet files
- **Location**: `src/account_tax/datasets/partitioned_parquet.py`
- **Features**:
  - Saves train/validation/test splits as separate parquet files
  - Maintains partition metadata
  - Integrates with MLflow artifact storage

## Configuration Patterns

### Multi-Environment Support
```
conf/
â”œâ”€â”€ base/           # Default configuration
â”‚   â”œâ”€â”€ catalog.yml
â”‚   â”œâ”€â”€ mlflow.yml
â”‚   â””â”€â”€ parameters/
â”‚       â”œâ”€â”€ data_pipeline.yml
â”‚       â”œâ”€â”€ train_pipeline.yml
â”‚       â””â”€â”€ inference_pipeline.yml
â”œâ”€â”€ repro/          # Reproducibility configs
â”‚   â”œâ”€â”€ catalog.yml (overrides)
â”‚   â””â”€â”€ parameters/ (fixed seeds)
â””â”€â”€ local/          # Local dev (gitignored)
```

### Parameter Hierarchy
1. **Global**: `conf/base/globals.yml` (BRANCH, AS_OF)
2. **Pipeline**: `parameters/{pipeline}_pipeline.yml`
3. **Node**: Specific parameters within pipeline configs
4. **Runtime**: CLI overrides via `--params`

---

ë³¸ ì„¹ì…˜ì€ ì„¤ê³„ìžÂ·í”Œëž˜ë„ˆÂ·ê°œë°œìžê°€ ë™ì¼í•œ í•¨ìˆ˜ ë¸”ë¡ ì •ë³´ë¥¼ ê³µìœ í•˜ê¸° ìœ„í•œ ìµœì‹  ê¸°ì¤€ìž…ë‹ˆë‹¤. ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì±…ìž„ì„ ë³€ê²½í•  ë•ŒëŠ” í•´ë‹¹ í‘œë¥¼ ì¦‰ì‹œ ê°±ì‹ í•´ ë¸”ë¡í™” ì›ì¹™ì„ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
