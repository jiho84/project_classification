# Training pipeline parameters

# ----------------------------------------------------------------------------
# Split configuration (shared with split pipeline)
# ----------------------------------------------------------------------------
split:
  label_column: acct_code
  seed: 42
  test_size: 0.05
  val_size: 0.025
  max_classes: 260
  labelize_num_proc: 16
  extract_ratio: 0.01  # 1% for 10-minute testing
  extract_seed: 42
  stratify_extract: true

# ----------------------------------------------------------------------------
# Tokenization, model, and training configuration
# ----------------------------------------------------------------------------
train:
  serialization:
    text_columns: []
    separator: "^ "
    key_value_separator: ":"
    include_column_names: true
    num_proc: 16

  tokenization:
    model_name: "Qwen/Qwen3-8B"  # Upgraded from 4B to 8B
    max_length: 256
    truncation: true
    padding: "longest"
    num_proc: 16
    sample_size: 10
    output_dir: "data/06_models/tokenized_datasets"

  # Default LoRA configuration (anchored for reuse)
  lora_defaults: &lora_cfg
    task_type: "SEQ_CLS"
    r: 256  # Reduced from 256 for Qwen3-8B memory efficiency
    lora_alpha: 512  # Reduced from 512 (2x rank)
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj"]
    layers_to_transform: [25, 26, 27 ,28, 29, 30, 31, 32, 33, 34, 35]
    modules_to_save: ["score"]
    bias: "lora_only"

  data:
    train_split: "train"
    eval_split: "valid"
    test_split: "test"

  model:
    name_or_path: "Qwen/Qwen3-8B"  # Upgraded from 4B to 8B
    num_labels: null
    torch_dtype: "bfloat16"
    trust_remote_code: true
    gradient_checkpointing: false  # Disabled: causes 3x slowdown with large batches

  training_args:
    output_dir: "data/06_models/checkpoints"
    max_steps: 150  # 10-minute test: 150 steps total
    num_train_epochs: -1  # Disabled: use max_steps instead
    per_device_train_batch_size: 8  # Reduced for Qwen3-8B (16→12)
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 4  # Increased to maintain effective batch (2→3)
    learning_rate: 2.0e-5  # Baseline LR for batch 32
    weight_decay: 0.002  # Baseline weight decay
    warmup_ratio: 0.02  # 5% warmup (industry standard for full training)
    lr_scheduler_type: "cosine"
    max_grad_norm: 2.0  # Match DeepSpeed gradient_clipping value
    eval_strategy: "steps"  # Evaluate every N steps
    eval_steps: 50  # 10-minute test: eval at step 50, 100, 150
    save_strategy: "steps"  # Save checkpoint every N steps
    save_steps: 50  # 10-minute test: save at step 50, 100
    save_total_limit: 3  # Keep only the latest 3 checkpoints
    logging_steps: 10
    report_to: [mlflow]  # Disable MLflow for faster benchmark
    bf16: true
    fp16: false
    gradient_checkpointing: false  # Disabled: causes 3x slowdown with large batches
    dataloader_num_workers: 0  # PyTorch default (single process, no multiprocessing)
    seed: 42
    load_best_model_at_end: true  # Load best checkpoint at end
    metric_for_best_model: "eval_loss"  # Use validation loss for best model
    greater_is_better: false  # Lower loss is better

  loss:
    use_class_weights: true
    class_weight_alpha: 0.3
    class_weight_min: 0.2
    class_weight_max: 3.0


  lora:
    enable: true
    config:
      <<: *lora_cfg

  deepspeed:
    num_gpus: 4
    config:
      train_micro_batch_size_per_gpu: 8  # Match per_device_train_batch_size
      train_batch_size: 128  # 8 × 4 (accum) × 4 GPUs
      gradient_accumulation_steps: 4
      zero_optimization:
        stage: 2
        reduce_scatter: true
        allgather_partitions: true
        offload_param:
          device: "none"
          pin_memory: true
        offload_optimizer:
          device: "none"
          pin_memory: true
        allgather_bucket_size: 2e8
        reduce_bucket_size: 2e8
        overlap_comm: true
        contiguous_gradients: true
      gradient_clipping: 2.0
      optimizer:
        type: "Adam"  # DeepSpeed will automatically use FusedAdam on GPU
        params:
          lr: 2.0e-5  # Baseline LR for batch 32
          betas: [0.9, 0.999]
          eps: 1.0e-8
          weight_decay: 0.002  # Baseline weight decay
      bf16:
        enabled: true
      fp16:
        enabled: false
      zero_allow_untested_optimizer: true

  training_script:
    path: "src/train/main_yaml.py"
    config_output_path: "data/06_models/run_config/train_config.yml"

  metrics:
    path: "data/06_models/metrics/train_metrics.json"

  resume:
    enabled: true
    checkpoint_path: "data/06_models/checkpoints/checkpoint-50"
