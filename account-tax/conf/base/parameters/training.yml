# Training pipeline parameters

# ----------------------------------------------------------------------------
# Split configuration (shared with split pipeline)
# ----------------------------------------------------------------------------
split:
  label_column: acct_code
  seed: 42
  test_size: 0.2
  val_size: 0.1
  max_classes: 260
  labelize_num_proc: 8
  extract_ratio: 0.1  # 1% for fast testing
  extract_seed: 42
  stratify_extract: true

# ----------------------------------------------------------------------------
# Tokenization, model, and training configuration
# ----------------------------------------------------------------------------
train:
  serialization:
    text_columns: []
    separator: "| "
    key_value_separator: ":"
    include_column_names: true
    num_proc: 1

  tokenization:
    model_name: "Qwen/Qwen3-4B"
    max_length: 256
    truncation: true
    padding: "longest"
    num_proc: 1
    sample_size: 10
    output_dir: "data/06_models/tokenized_datasets"

  # Default LoRA configuration (anchored for reuse)
  lora_defaults: &lora_cfg
    task_type: "SEQ_CLS"
    r: 256
    lora_alpha: 512
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    layers_to_transform: [28, 29, 30, 31, 32, 33, 34, 35]
    modules_to_save: ["score"]
    bias: "lora_only"

  data:
    train_split: "train"
    eval_split: "valid"
    test_split: "test"

  model:
    name_or_path: "Qwen/Qwen3-4B"
    num_labels: null
    torch_dtype: "bfloat16"
    trust_remote_code: true
    gradient_checkpointing: true

  training_args:
    output_dir: "data/06_models/checkpoints"
    num_train_epochs: 1
    per_device_train_batch_size: 32
    per_device_eval_batch_size: 32
    gradient_accumulation_steps: 1
    learning_rate: 2.0e-5
    weight_decay: 0.002
    warmup_ratio: 0.01
    lr_scheduler_type: "cosine"
    eval_strategy: "steps"
    eval_steps: 2000  # Evaluate every 100 steps for fast testing
    save_strategy: "steps"
    save_steps: 2000  # Save every 100 steps for fast testing
    save_total_limit: 3  # Keep last 3 checkpoints (best will be kept as final model)
    logging_steps: 10
    report_to: ["mlflow"]
    bf16: true
    fp16: false
    gradient_checkpointing: true
    dataloader_num_workers: 0
    seed: 42
    load_best_model_at_end: false  # Disabled: causes hang with DeepSpeed + LoRA
    metric_for_best_model: "accuracy"
    greater_is_better: true

  loss:
    use_class_weights: true
    class_weight_alpha: 0.3
    class_weight_min: 0.2
    class_weight_max: 8.0

  lora:
    enable: true
    config:
      <<: *lora_cfg

  deepspeed:
    num_gpus: 4
    config:
      train_micro_batch_size_per_gpu: 32
      train_batch_size: 128
      gradient_accumulation_steps: 1
      zero_optimization:
        stage: 2
        reduce_scatter: true
        allgather_partitions: true
        offload_param:
          device: "none"
          pin_memory: true
        offload_optimizer:
          device: "none"
          pin_memory: true
        allgather_bucket_size: 2e8
        reduce_bucket_size: 2e8
        overlap_comm: true
        contiguous_gradients: true
      gradient_clipping: 1.0
      bf16:
        enabled: true
      fp16:
        enabled: false
      zero_allow_untested_optimizer: true

  training_script:
    path: "src/train/main_yaml.py"
    config_output_path: "data/06_models/run_config/train_config.yml"

  metrics:
    path: "data/06_models/metrics/train_metrics.json"
    class_weight_report: "data/06_models/metrics/class_weight_report.json"
