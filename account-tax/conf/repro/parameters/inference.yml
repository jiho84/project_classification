# Inference pipeline parameters placeholder

inference:
  batch_size: 2048
  output_format: parquet
