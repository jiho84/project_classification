"""Standalone training script executed via DeepSpeed launcher.

Usage (invoked by Kedro node):

    deepspeed --num_gpus <N> src/train/main_yaml.py --config_yml path/to/config.yml

The configuration YAML is generated by the ``launch_training`` node and contains:

- model:        HuggingFace model parameters
- data:         tokenized dataset path and split names
- training_args:Arguments passed to ``TrainingArguments``
- lora:         LoRA enable flag and configuration
- deepspeed:    DeepSpeed config dict (optional)
- metrics:      Optional output path for evaluation metrics
- seed:         Random seed
"""

from __future__ import annotations

import argparse
import logging
import os
import yaml

from account_tax.utils import RankZeroLogger
from account_tax.utils.common import (
    apply_lora_to_model,
    build_weighted_trainer,
    cleanup_distributed_process_group,
    evaluate_and_save_results,
    execute_training_loop,
    initialize_model,
    initialize_tokenizer,
    load_datasets,
    patch_mlflow_callback,
    setup_training_context,
)


def is_rank_zero() -> bool:
    """Check if current process is rank 0 (used for logger initialization)."""
    return os.environ.get("LOCAL_RANK", "0") == "0"


LOGGER = logging.getLogger(__name__)
LOGGER_ZERO = RankZeroLogger(LOGGER, is_rank_zero)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="DeepSpeed Trainer driver")
    parser.add_argument("--config_yml", type=str, required=True, help="Path to training YAML config")
    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--rank", type=int, default=0)
    parser.add_argument("--world_size", type=int, default=1)
    return parser.parse_args()


def main() -> None:
    """Training pipeline in Kedro-declarative style.

    Defines and executes the training pipeline blocks sequentially.
    Each block follows the Kedro node pattern: func, inputs, outputs, name, description.

    Pipeline blocks:
        1. setup_training_context    - Initialize environment
        2. load_datasets              - Load tokenized datasets
        3. initialize_tokenizer       - Configure tokenizer
        4. initialize_model           - Load base model
        5. apply_lora_to_model        - Apply LoRA optimization
        6. build_weighted_trainer     - Create WeightedTrainer
        7. patch_mlflow_callback      - Prevent subprocess hang
        8. execute_training_loop      - Run training
        9. evaluate_and_save_results  - Evaluate and save
       10. cleanup_distributed        - Clean up resources
    """
    logging.basicConfig(level=logging.INFO)

    # Parse configuration
    args = parse_args()
    with open(args.config_yml, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # Define pipeline blocks (Kedro-style declarative format)
    pipeline = [
        {
            "func": setup_training_context,
            "inputs": ["args", "cfg"],
            "outputs": "context",
            "name": "setup_context",
            "description": "Initialize training environment",
        },
        {
            "func": load_datasets,
            "inputs": ["context", "logger"],
            "outputs": "artifacts",
            "name": "load_datasets",
            "description": "Load tokenized datasets",
        },
        {
            "func": initialize_tokenizer,
            "inputs": ["context", "artifacts", "logger"],
            "outputs": "artifacts",
            "name": "initialize_tokenizer",
            "description": "Configure tokenizer and infer num_labels",
        },
        {
            "func": initialize_model,
            "inputs": ["context", "artifacts", "logger"],
            "outputs": "artifacts",
            "name": "initialize_model",
            "description": "Load base model without optimization",
        },
        {
            "func": apply_lora_to_model,
            "inputs": ["context", "artifacts", "logger"],
            "outputs": "artifacts",
            "name": "apply_lora",
            "description": "Apply LoRA optimization if configured",
        },
        {
            "func": build_weighted_trainer,
            "inputs": ["context", "artifacts", "logger", "logger_zero"],
            "outputs": "artifacts",
            "name": "build_trainer",
            "description": "Create WeightedTrainer with all components",
        },
        {
            "func": patch_mlflow_callback,
            "inputs": ["artifacts", "logger_zero"],
            "outputs": "artifacts",
            "name": "patch_mlflow",
            "description": "Override MLflow callback to prevent hang",
        },
        {
            "func": execute_training_loop,
            "inputs": ["context", "artifacts", "logger", "logger_zero"],
            "outputs": "artifacts",
            "name": "execute_training",
            "description": "Run training loop with checkpoint support",
        },
        {
            "func": evaluate_and_save_results,
            "inputs": ["context", "artifacts", "logger_zero"],
            "outputs": None,
            "name": "evaluate_save",
            "description": "Evaluate on test set and save model",
        },
        {
            "func": cleanup_distributed_process_group,
            "inputs": ["logger_zero"],
            "outputs": None,
            "name": "cleanup",
            "description": "Clean up distributed resources",
        },
    ]

    # Execute pipeline
    state = {
        "args": args,
        "cfg": cfg,
        "logger": LOGGER,
        "logger_zero": LOGGER_ZERO,
    }

    for block in pipeline:
        LOGGER.info(f"Executing block [{block['name']}]: {block['description']}")

        # Resolve inputs from state
        inputs = [state[inp] for inp in block["inputs"]]

        # Execute block function
        result = block["func"](*inputs)

        # Store outputs in state
        if block["outputs"]:
            state[block["outputs"]] = result


if __name__ == "__main__":
    main()
