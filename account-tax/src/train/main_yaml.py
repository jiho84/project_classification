"""Standalone training script executed via DeepSpeed launcher.

Usage (invoked by Kedro node):

    deepspeed --num_gpus <N> src/train/main_yaml.py --config_yml path/to/config.yml

The configuration YAML is generated by the ``launch_training`` node and contains:

- model:        HuggingFace model parameters
- data:         tokenized dataset path and split names
- training_args:Arguments passed to ``TrainingArguments``
- lora:         LoRA enable flag and configuration
- deepspeed:    DeepSpeed config dict (optional)
- metrics:      Optional output path for evaluation metrics
- seed:         Random seed
"""

from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict

import torch
import yaml
from datasets import load_from_disk
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.trainer_utils import get_last_checkpoint

try:
    from peft import LoraConfig, get_peft_model
    from peft import TaskType  # type: ignore
except ImportError:  # pragma: no cover - optional dependency guard
    LoraConfig = None
    get_peft_model = None
    TaskType = None


LOGGER = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="DeepSpeed Trainer driver")
    parser.add_argument("--config_yml", type=str, required=True, help="Path to training YAML config")
    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--rank", type=int, default=0)
    parser.add_argument("--world_size", type=int, default=1)
    return parser.parse_args()


def load_config(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError("Training configuration must be a YAML mapping")
    return cfg


def build_training_arguments(args_cfg: Dict[str, Any], deepspeed_cfg: Dict[str, Any] | None) -> TrainingArguments:
    cfg = dict(args_cfg)
    if deepspeed_cfg:
        cfg["deepspeed"] = deepspeed_cfg
    # Trainer expects lists for report_to, etc.
    if "report_to" in cfg and isinstance(cfg["report_to"], str):
        cfg["report_to"] = [cfg["report_to"]]
    output_dir = Path(cfg.get("output_dir", "./outputs"))
    output_dir.mkdir(parents=True, exist_ok=True)
    return TrainingArguments(**cfg)


def infer_num_labels(dataset) -> int:
    if "labels" in dataset.features:
        feature = dataset.features["labels"]
        # SequenceClassification: ClassLabel
        if hasattr(feature, "num_classes"):
            return feature.num_classes
    return len(set(dataset["labels"]))


def maybe_apply_lora(model, lora_section: Dict[str, Any]) -> torch.nn.Module:
    if not lora_section.get("enable", False):
        return model
    if LoraConfig is None or get_peft_model is None:
        raise ImportError("peft package is required for LoRA but is not installed")

    cfg = dict(lora_section.get("config", {}))
    task_type = cfg.pop("task_type", "SEQ_CLS")
    if TaskType is not None and isinstance(task_type, str):
        task_type_enum = TaskType[task_type]
        cfg["task_type"] = task_type_enum

    lora_cfg = LoraConfig(**cfg)
    LOGGER.info("Applying LoRA with config: %s", cfg)
    return get_peft_model(model, lora_cfg)


def save_metrics(metrics: Dict[str, Any], path: str | None) -> None:
    if not path:
        return
    destination = Path(path)
    destination.parent.mkdir(parents=True, exist_ok=True)
    with open(destination, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)


def compute_accuracy(preds, labels) -> Dict[str, float]:
    if preds.ndim > 1:
        preds = preds.argmax(axis=-1)
    accuracy = (preds == labels).mean().item() if hasattr(preds, "mean") else float((preds == labels).mean())
    return {"accuracy": float(accuracy)}


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    args = parse_args()
    cfg = load_config(args.config_yml)

    seed = int(cfg.get("seed", 42))
    set_seed(seed)

    model_cfg = cfg.get("model", {})
    data_cfg = cfg.get("data", {})
    training_args_cfg = cfg.get("training_args", {})
    deepspeed_cfg = cfg.get("deepspeed")
    lora_cfg = cfg.get("lora", {})
    metrics_cfg = cfg.get("metrics", {})

    tokenized_path = data_cfg["tokenized_path"]
    LOGGER.info("Loading tokenized datasets from %s", tokenized_path)
    dataset_dict = load_from_disk(tokenized_path)

    train_split = data_cfg.get("train_split", "train")
    eval_split = data_cfg.get("eval_split")

    train_dataset = dataset_dict[train_split]
    eval_dataset = dataset_dict[eval_split] if eval_split and eval_split in dataset_dict else None

    model_name = model_cfg["name_or_path"]
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=model_cfg.get("trust_remote_code", False))
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    num_labels = model_cfg.get("num_labels")
    if num_labels is None:
        num_labels = infer_num_labels(train_dataset)
        LOGGER.info("Inferred num_labels=%s", num_labels)

    torch_dtype_str = model_cfg.get("torch_dtype", "bfloat16")
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    torch_dtype = dtype_map.get(torch_dtype_str, torch.bfloat16)

    checkpoint = get_last_checkpoint(training_args_cfg.get("output_dir", ""))
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        torch_dtype=torch_dtype,
        trust_remote_code=model_cfg.get("trust_remote_code", False),
        resume_download=bool(checkpoint),
    )

    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    if model_cfg.get("gradient_checkpointing", False):
        model.gradient_checkpointing_enable()

    model = maybe_apply_lora(model, lora_cfg)

    training_args = build_training_arguments(training_args_cfg, deepspeed_cfg)

    collator = DataCollatorWithPadding(
        tokenizer=tokenizer,
        pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
    )

    def compute_metrics_fn(eval_pred):
        predictions, labels = eval_pred
        return compute_accuracy(predictions, labels)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=collator,
        compute_metrics=compute_metrics_fn if eval_dataset is not None else None,
    )

    trainer.train()

    trainer.save_model()
    if trainer.state.is_world_process_zero():
        metrics: Dict[str, Any] = {"global_step": trainer.state.global_step}
        if eval_dataset is not None:
            metrics.update(trainer.evaluate())
        save_metrics(metrics, metrics_cfg.get("path"))


if __name__ == "__main__":
    main()
