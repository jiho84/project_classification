"""Standalone training script executed via DeepSpeed launcher.

Usage (invoked by Kedro node):

    deepspeed --num_gpus <N> src/train/main_yaml.py --config_yml path/to/config.yml

The configuration YAML is generated by the ``launch_training`` node and contains:

- model:        HuggingFace model parameters
- data:         tokenized dataset path and split names
- training_args:Arguments passed to ``TrainingArguments``
- lora:         LoRA enable flag and configuration
- deepspeed:    DeepSpeed config dict (optional)
- metrics:      Optional output path for evaluation metrics
- seed:         Random seed
"""

from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict

import torch
import yaml
import numpy as np
from datasets import load_from_disk
from sklearn.metrics import f1_score
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    TrainingArguments,
    set_seed,
)

from account_tax.utils import RankZeroLogger, build_class_weight_tensor
from account_tax.utils.common import (
    GPUMemoryCallback,
    WeightedTrainer,
    build_training_arguments,
    compute_classification_metrics,
    infer_num_labels,
    maybe_apply_lora,
    save_metrics,
)

try:
    import mlflow
except ImportError:
    mlflow = None

try:
    from transformers.integrations import MLflowCallback
except ImportError:
    MLflowCallback = None
from transformers.trainer_utils import get_last_checkpoint


def is_rank_zero() -> bool:
    return os.environ.get("LOCAL_RANK", "0") == "0"


LOGGER = logging.getLogger(__name__)
LOGGER_ZERO = RankZeroLogger(LOGGER, is_rank_zero)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="DeepSpeed Trainer driver")
    parser.add_argument("--config_yml", type=str, required=True, help="Path to training YAML config")
    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--rank", type=int, default=0)
    parser.add_argument("--world_size", type=int, default=1)
    return parser.parse_args()


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    args = parse_args()
    with open(args.config_yml, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError("Training configuration must be a YAML mapping")

    seed = int(cfg.get("seed", 42))
    set_seed(seed)

    model_cfg = cfg.get("model", {})
    data_cfg = cfg.get("data", {})
    training_args_cfg = cfg.get("training_args", {})
    deepspeed_cfg = cfg.get("deepspeed")
    lora_cfg = cfg.get("lora", {})
    metrics_cfg = cfg.get("metrics", {})
    resume_cfg = cfg.get("resume", {})

    tokenized_path = data_cfg["tokenized_path"]
    LOGGER.info("Loading tokenized datasets from %s", tokenized_path)
    dataset_dict = load_from_disk(tokenized_path)

    train_split = data_cfg.get("train_split", "train")
    eval_split = data_cfg.get("eval_split")

    train_dataset = dataset_dict[train_split]
    eval_dataset = dataset_dict[eval_split] if eval_split and eval_split in dataset_dict else None
    test_split = data_cfg.get("test_split")
    test_dataset = dataset_dict[test_split] if test_split and test_split in dataset_dict else None

    model_name = model_cfg["name_or_path"]
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=model_cfg.get("trust_remote_code", False))
    if tokenizer.pad_token_id is None:
        eos_token = tokenizer.eos_token
        if eos_token is None:
            raise ValueError("Tokenizer is missing both pad and eos tokens; cannot proceed.")
        pad_token_id = tokenizer.convert_tokens_to_ids(eos_token)
        if pad_token_id is None:
            raise ValueError("Unable to determine token id for eos token; cannot reuse for padding.")
        tokenizer.pad_token = eos_token
        tokenizer.pad_token_id = pad_token_id

    num_labels = model_cfg.get("num_labels")
    if num_labels is None:
        num_labels = infer_num_labels(train_dataset)
        LOGGER.info("Inferred num_labels=%s", num_labels)

    torch_dtype_str = model_cfg.get("torch_dtype", "bfloat16")
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    torch_dtype = dtype_map.get(torch_dtype_str, torch.bfloat16)

    checkpoint = get_last_checkpoint(training_args_cfg.get("output_dir", ""))
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        dtype=torch_dtype,
        trust_remote_code=model_cfg.get("trust_remote_code", False),
    )

    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    if model_cfg.get("gradient_checkpointing", False):
        model.gradient_checkpointing_enable()

    model = maybe_apply_lora(model, lora_cfg, logger=LOGGER, rank_zero_fn=is_rank_zero)

    training_args = build_training_arguments(
        training_args_cfg,
        deepspeed_cfg,
        num_gpus=deepspeed_cfg.get("num_gpus", 4),
        rank_zero_fn=is_rank_zero
    )

    collator = DataCollatorWithPadding(
        tokenizer=tokenizer,
        pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
    )

    loss_cfg = cfg.get("loss", {})
    use_class_weights = bool(loss_cfg.get("use_class_weights", False))
    class_weight_alpha = float(loss_cfg.get("class_weight_alpha", 1.0))

    train_labels = np.array(train_dataset["labels"])
    class_weight_min = float(loss_cfg.get("class_weight_min", 1.0))
    class_weight_max = float(loss_cfg.get("class_weight_max", 1.0))
    class_weights_tensor = build_class_weight_tensor(
        labels=train_labels,
        num_labels=num_labels,
        alpha=class_weight_alpha,
        weight_min=class_weight_min,
        weight_max=class_weight_max,
        enabled=use_class_weights,
    )

    LOGGER_ZERO.info(
        "Class weights configured (alpha=%.3f, min=%.3f, max=%.3f, mean=%.4f)",
        class_weight_alpha,
        class_weight_min,
        class_weight_max,
        float(class_weights_tensor.mean().item()),
    )

    # Create callbacks
    callbacks = []
    if torch.cuda.is_available():
        callbacks.append(GPUMemoryCallback(rank_zero_fn=is_rank_zero))
    # Note: Regular step-based checkpoints (save_steps=100) work fine with DeepSpeed
    # Epoch-end saves caused NCCL hangs, so we rely on step-based checkpoints + final PEFT save

    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer,
        data_collator=collator,
        class_weights=class_weights_tensor,
        compute_metrics=compute_classification_metrics if eval_dataset is not None else None,
        callbacks=callbacks,
    )

    # Override MLflowCallback.on_train_end() to prevent subprocess hang
    # The hang occurs when nested MLflow run tries to finalize and sync with parent process
    if MLflowCallback is not None:
        for callback in trainer.callback_handler.callbacks:
            if isinstance(callback, MLflowCallback):
                def safe_on_train_end(self, args, state, control, **kwargs):  # noqa: ARG001
                    """
                    Skip mlflow.end_run() that causes hang in subprocess.

                    Root cause: mlflow.end_run() tries to:
                    1. Flush metrics to tracking server
                    2. Sync artifacts to storage
                    3. Update run status to FINISHED
                    4. Notify parent run (for nested runs)

                    Step 4 causes hang because subprocess cannot communicate
                    with parent process's ThreadLocal MLflow context.

                    Solution: Skip end_run() here. Parent Kedro process will
                    handle run finalization after subprocess completes.
                    """
                    LOGGER_ZERO.info("MLflow metrics logged successfully during training")
                    LOGGER_ZERO.info("Skipping mlflow.end_run() in subprocess to prevent hang")
                    LOGGER_ZERO.info("Parent Kedro process will finalize MLflow run")
                    return control

                # Replace the method (binding to instance)
                callback.on_train_end = safe_on_train_end.__get__(callback, MLflowCallback)

                LOGGER_ZERO.info("MLflowCallback.on_train_end() overridden for hang prevention")
                break

    # CRITICAL: trainer.train() may hang during DeepSpeed cleanup after training completes
    # We wrap it in try-except to ensure PEFT adapter gets saved even if hang occurs
    training_completed = False
    try:
        # Resume from checkpoint if configured
        resume_enabled = resume_cfg.get("enabled", False)
        checkpoint_path = resume_cfg.get("checkpoint_path") if resume_enabled else None
        if checkpoint_path:
            LOGGER_ZERO.info("Resuming training from checkpoint: %s", checkpoint_path)
        else:
            LOGGER_ZERO.info("Starting training from scratch (no checkpoint resume)")

        trainer.train(resume_from_checkpoint=checkpoint_path)
        training_completed = True
        LOGGER.info("trainer.train() returned successfully")
    except KeyboardInterrupt:
        LOGGER.warning("Training interrupted by user")
    except Exception as e:
        LOGGER.error("Training failed with error: %s", e)
        raise

    # Save final PEFT adapter for inference (avoiding DeepSpeed ZeRO hang)
    # Training resume will use checkpoint-{last_step}/ which contains full DeepSpeed state
    output_dir = Path(training_args.output_dir)

    eval_metrics: Dict[str, Any] | None = None
    test_metrics: Dict[str, Any] | None = None
    if test_dataset is not None and training_completed:
        try:
            test_metrics = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix="test")
            LOGGER_ZERO.info(
                "Test evaluation completed: %s",
                {k: float(v) if isinstance(v, (int, float)) else v for k, v in test_metrics.items()}
            )
        except Exception as e:
            LOGGER.warning("Test evaluation failed: %s", e)

    # CRITICAL FIX: DeepSpeed ZeRO requires all ranks to participate in save operations
    # Rank0-only save causes deadlock because:
    # 1. save_pretrained() may trigger implicit collective operations (all_gather for sharded weights)
    # 2. Other ranks proceed to barrier while rank0 is stuck in save
    # 3. Result: NCCL timeout / hang
    if not torch.distributed.is_initialized():
        raise RuntimeError("DeepSpeed expected but torch.distributed is not initialized")

    LOGGER_ZERO.info("DeepSpeed detected: saving with all-rank participation")

    torch.distributed.barrier()

    unwrapped_model = trainer.model
    if hasattr(unwrapped_model, 'module'):
        unwrapped_model = unwrapped_model.module

    unwrapped_model.save_pretrained(
        output_dir,
        safe_serialization=True,
    )

    tokenizer.save_pretrained(output_dir)

    torch.distributed.barrier()

    LOGGER_ZERO.info("PEFT adapter saved to %s for inference (all-rank save)", output_dir)

    if trainer.is_world_process_zero():
        metrics: Dict[str, Any] = {"global_step": trainer.state.global_step}
        if eval_metrics is not None:
            metrics.update(eval_metrics)
            save_metrics(metrics, metrics_cfg.get("path"))
        if test_metrics is not None:
            test_payload: Dict[str, Any] = {"global_step": trainer.state.global_step}
            test_payload.update(test_metrics)
            save_metrics(test_payload, metrics_cfg.get("test_path"))
        LOGGER_ZERO.info("Training can be resumed from checkpoint-%d/", trainer.state.global_step)
        LOGGER_ZERO.info("MLflow artifacts will be logged by Kedro node.")

    try:
        torch.distributed.barrier()
        torch.distributed.destroy_process_group()
        LOGGER_ZERO.info("DeepSpeed process group destroyed successfully")
    except Exception as e:
        LOGGER_ZERO.warning("Failed to destroy process group: %s", e)

    # Force exit to avoid any remaining cleanup that might hang
    LOGGER_ZERO.info("Training script exiting normally")


if __name__ == "__main__":
    main()
