"""Standalone training script executed via DeepSpeed launcher.

Usage (invoked by Kedro node):

    deepspeed --num_gpus <N> src/train/main_yaml.py --config_yml path/to/config.yml

The configuration YAML is generated by the ``launch_training`` node and contains:

- model:        HuggingFace model parameters
- data:         tokenized dataset path and split names
- training_args:Arguments passed to ``TrainingArguments``
- lora:         LoRA enable flag and configuration
- deepspeed:    DeepSpeed config dict (optional)
- metrics:      Optional output path for evaluation metrics
- seed:         Random seed
"""

from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict

import torch
import torch.nn as nn
import yaml
import numpy as np
from datasets import load_from_disk
from sklearn.metrics import f1_score
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    set_seed,
)

from account_tax.utils import build_class_weight_tensor

try:
    import mlflow
except ImportError:
    mlflow = None

try:
    from transformers.integrations import MLflowCallback
except ImportError:
    MLflowCallback = None
from transformers.trainer_utils import get_last_checkpoint

try:
    from peft import LoraConfig, get_peft_model
    from peft import TaskType  # type: ignore
except ImportError:  # pragma: no cover - optional dependency guard
    LoraConfig = None
    get_peft_model = None
    TaskType = None


LOGGER = logging.getLogger(__name__)


def is_rank_zero() -> bool:
    return os.environ.get("LOCAL_RANK", "0") == "0"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="DeepSpeed Trainer driver")
    parser.add_argument("--config_yml", type=str, required=True, help="Path to training YAML config")
    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--rank", type=int, default=0)
    parser.add_argument("--world_size", type=int, default=1)
    return parser.parse_args()


def load_config(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError("Training configuration must be a YAML mapping")
    return cfg


def build_training_arguments(args_cfg: Dict[str, Any], deepspeed_cfg: Dict[str, Any] | None) -> TrainingArguments:
    cfg = dict(args_cfg)
    if deepspeed_cfg:
        cfg["deepspeed"] = deepspeed_cfg
        # Override TrainingArguments with DeepSpeed config values to avoid mismatch errors
        if "train_micro_batch_size_per_gpu" in deepspeed_cfg:
            cfg["per_device_train_batch_size"] = deepspeed_cfg["train_micro_batch_size_per_gpu"]
        if is_rank_zero():
            LOGGER.info("DeepSpeed override: per_device_train_batch_size = %s", deepspeed_cfg["train_micro_batch_size_per_gpu"])
        if "gradient_accumulation_steps" in deepspeed_cfg:
            cfg["gradient_accumulation_steps"] = deepspeed_cfg["gradient_accumulation_steps"]
            if is_rank_zero():
                LOGGER.info("DeepSpeed override: gradient_accumulation_steps = %s", deepspeed_cfg["gradient_accumulation_steps"])
    # Trainer expects lists for report_to, etc.
    if "report_to" in cfg and isinstance(cfg["report_to"], str):
        cfg["report_to"] = [cfg["report_to"]]
    output_dir = Path(cfg.get("output_dir", "./outputs"))
    output_dir.mkdir(parents=True, exist_ok=True)
    return TrainingArguments(**cfg)


def infer_num_labels(dataset) -> int:
    if "labels" in dataset.features:
        feature = dataset.features["labels"]
        # SequenceClassification: ClassLabel
        if hasattr(feature, "num_classes"):
            return feature.num_classes
    return len(set(dataset["labels"]))


def maybe_apply_lora(model, lora_section: Dict[str, Any]) -> torch.nn.Module:
    if not lora_section.get("enable", False):
        return model
    if LoraConfig is None or get_peft_model is None:
        raise ImportError("peft package is required for LoRA but is not installed")

    cfg = dict(lora_section.get("config", {}))
    task_type = cfg.pop("task_type", "SEQ_CLS")
    if TaskType is not None and isinstance(task_type, str):
        task_type_enum = TaskType[task_type]
        cfg["task_type"] = task_type_enum

    lora_cfg = LoraConfig(**cfg)
    LOGGER.info("Applying LoRA with config: %s", cfg)
    return get_peft_model(model, lora_cfg)


def save_metrics(metrics: Dict[str, Any], path: str | None) -> None:
    if not path:
        return
    destination = Path(path)
    destination.parent.mkdir(parents=True, exist_ok=True)
    with open(destination, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    args = parse_args()
    cfg = load_config(args.config_yml)

    seed = int(cfg.get("seed", 42))
    set_seed(seed)

    model_cfg = cfg.get("model", {})
    data_cfg = cfg.get("data", {})
    training_args_cfg = cfg.get("training_args", {})
    deepspeed_cfg = cfg.get("deepspeed")
    lora_cfg = cfg.get("lora", {})
    metrics_cfg = cfg.get("metrics", {})
    resume_cfg = cfg.get("resume", {})

    tokenized_path = data_cfg["tokenized_path"]
    LOGGER.info("Loading tokenized datasets from %s", tokenized_path)
    dataset_dict = load_from_disk(tokenized_path)

    train_split = data_cfg.get("train_split", "train")
    eval_split = data_cfg.get("eval_split")

    train_dataset = dataset_dict[train_split]
    eval_dataset = dataset_dict[eval_split] if eval_split and eval_split in dataset_dict else None

    model_name = model_cfg["name_or_path"]
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=model_cfg.get("trust_remote_code", False))
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    num_labels = model_cfg.get("num_labels")
    if num_labels is None:
        num_labels = infer_num_labels(train_dataset)
        LOGGER.info("Inferred num_labels=%s", num_labels)

    torch_dtype_str = model_cfg.get("torch_dtype", "bfloat16")
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    torch_dtype = dtype_map.get(torch_dtype_str, torch.bfloat16)

    checkpoint = get_last_checkpoint(training_args_cfg.get("output_dir", ""))
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        dtype=torch_dtype,
        trust_remote_code=model_cfg.get("trust_remote_code", False),
    )

    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    if model_cfg.get("gradient_checkpointing", False):
        model.gradient_checkpointing_enable()

    model = maybe_apply_lora(model, lora_cfg)

    training_args = build_training_arguments(training_args_cfg, deepspeed_cfg)

    collator = DataCollatorWithPadding(
        tokenizer=tokenizer,
        pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
    )

    def compute_metrics_fn(eval_pred):
        predictions, labels = eval_pred
        preds = predictions.argmax(axis=-1) if predictions.ndim > 1 else predictions

        accuracy = float((preds == labels).mean())
        f1_weighted = float(f1_score(labels, preds, average='weighted', zero_division=0))
        f1_macro = float(f1_score(labels, preds, average='macro', zero_division=0))

        return {
            "accuracy": accuracy,
            "f1_weighted": f1_weighted,
            "f1_macro": f1_macro,
        }

    loss_cfg = cfg.get("loss", {})
    use_class_weights = bool(loss_cfg.get("use_class_weights", False))
    class_weight_alpha = float(loss_cfg.get("class_weight_alpha", 1.0))

    class_weights_tensor: torch.Tensor | None = None
    if use_class_weights:
        train_labels = np.array(train_dataset["labels"])
        class_weight_min = float(loss_cfg["class_weight_min"])
        class_weight_max = float(loss_cfg["class_weight_max"])
        class_weights_tensor = build_class_weight_tensor(
            labels=train_labels,
            num_labels=num_labels,
            alpha=class_weight_alpha,
            weight_min=class_weight_min,
            weight_max=class_weight_max,
        )

        if is_rank_zero():
            LOGGER.info(
                "Class weights enabled (alpha=%.3f, min=%.3f, max=%.3f, mean=%.4f)",
                class_weight_alpha,
                class_weight_min,
                class_weight_max,
                float(class_weights_tensor.mean().item()),
            )

    else:
        if is_rank_zero():
            LOGGER.info("Class weights disabled; using uniform loss.")

    class GPUMemoryCallback(TrainerCallback):
        """Callback to monitor and log GPU memory usage during training."""

        def __init__(self):
            self.enabled = is_rank_zero() and torch.cuda.is_available()
            self.gpu_count = torch.cuda.device_count() if self.enabled else 0
            self.device_props = []
            if self.enabled and self.gpu_count > 0:
                props = torch.cuda.get_device_properties(0)
                self.device_props.append(props)

        def on_log(self, args, state, control, logs=None, **kwargs):
            """Add GPU memory usage to logs."""
            if not self.enabled or logs is None or self.gpu_count == 0:
                return

            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            total = self.device_props[0].total_memory / 1024**3
            percent = (allocated / total) * 100 if total > 0 else 0

            logs["gpu_mem"] = f"{allocated:.1f}GB/{total:.0f}GB ({percent:.0f}%)"
            logs["gpu_reserved"] = f"{reserved:.1f}GB"

    class WeightedTrainer(Trainer):
        def __init__(self, *args, class_weights: torch.Tensor | None = None, **kwargs):
            super().__init__(*args, **kwargs)
            self.class_weights = class_weights
            # Fix gradient accumulation loss scaling: Let Trainer handle loss averaging
            self.model_accepts_loss_kwargs = False

        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.get("labels")

            if labels is None or self.class_weights is None:
                # Fallback to default loss
                return super().compute_loss(model, inputs, return_outputs=return_outputs, num_items_in_batch=num_items_in_batch)

            # Remove labels to prevent model from computing loss internally
            inputs = dict(inputs)
            labels = inputs.pop("labels")

            # Forward pass without labels
            outputs = model(**inputs)
            logits = outputs.logits

            # Custom weighted loss with ignore_index=-100
            loss_fct = nn.CrossEntropyLoss(
                weight=self.class_weights.to(logits.device, dtype=logits.dtype),
                ignore_index=-100  # Important: ignore padding tokens!
            )
            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))

            return (loss, outputs) if return_outputs else loss

    # Create callbacks
    callbacks = []
    if torch.cuda.is_available():
        callbacks.append(GPUMemoryCallback())
    # Note: Regular step-based checkpoints (save_steps=100) work fine with DeepSpeed
    # Epoch-end saves caused NCCL hangs, so we rely on step-based checkpoints + final PEFT save

    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=collator,
        class_weights=class_weights_tensor,
        compute_metrics=compute_metrics_fn if eval_dataset is not None else None,
        callbacks=callbacks,
    )

    # Override MLflowCallback.on_train_end() to prevent subprocess hang
    # The hang occurs when nested MLflow run tries to finalize and sync with parent process
    if MLflowCallback is not None:
        for callback in trainer.callback_handler.callbacks:
            if isinstance(callback, MLflowCallback):
                def safe_on_train_end(self, args, state, control, **kwargs):  # noqa: ARG001
                    """
                    Skip mlflow.end_run() that causes hang in subprocess.

                    Root cause: mlflow.end_run() tries to:
                    1. Flush metrics to tracking server
                    2. Sync artifacts to storage
                    3. Update run status to FINISHED
                    4. Notify parent run (for nested runs)

                    Step 4 causes hang because subprocess cannot communicate
                    with parent process's ThreadLocal MLflow context.

                    Solution: Skip end_run() here. Parent Kedro process will
                    handle run finalization after subprocess completes.
                    """
                    if is_rank_zero():
                        LOGGER.info("MLflow metrics logged successfully during training")
                        LOGGER.info("Skipping mlflow.end_run() in subprocess to prevent hang")
                        LOGGER.info("Parent Kedro process will finalize MLflow run")
                    return control

                # Replace the method (binding to instance)
                callback.on_train_end = safe_on_train_end.__get__(callback, MLflowCallback)

                if is_rank_zero():
                    LOGGER.info("MLflowCallback.on_train_end() overridden for hang prevention")
                break

    # CRITICAL: trainer.train() may hang during DeepSpeed cleanup after training completes
    # We wrap it in try-except to ensure PEFT adapter gets saved even if hang occurs
    training_completed = False
    try:
        # Resume from checkpoint if configured
        resume_enabled = resume_cfg.get("enabled", False)
        checkpoint_path = resume_cfg.get("checkpoint_path") if resume_enabled else None
        if checkpoint_path:
            if is_rank_zero():
                LOGGER.info("Resuming training from checkpoint: %s", checkpoint_path)
        else:
            if is_rank_zero():
                LOGGER.info("Starting training from scratch (no checkpoint resume)")

        trainer.train(resume_from_checkpoint=checkpoint_path)
        training_completed = True
        LOGGER.info("trainer.train() returned successfully")
    except KeyboardInterrupt:
        LOGGER.warning("Training interrupted by user")
    except Exception as e:
        LOGGER.error("Training failed with error: %s", e)
        raise

    # Save final PEFT adapter for inference (avoiding DeepSpeed ZeRO hang)
    # Training resume will use checkpoint-{last_step}/ which contains full DeepSpeed state
    output_dir = Path(training_args.output_dir)

    eval_metrics: Dict[str, Any] | None = None
    if eval_dataset is not None and training_completed:
        try:
            eval_metrics = trainer.evaluate()
            if is_rank_zero():
                LOGGER.info(
                    "Evaluation completed: %s",
                    {k: float(v) if isinstance(v, (int, float)) else v for k, v in eval_metrics.items()}
                )
        except Exception as e:
            LOGGER.warning("Evaluation failed: %s", e)

    # CRITICAL FIX: DeepSpeed ZeRO requires all ranks to participate in save operations
    # Rank0-only save causes deadlock because:
    # 1. save_pretrained() may trigger implicit collective operations (all_gather for sharded weights)
    # 2. Other ranks proceed to barrier while rank0 is stuck in save
    # 3. Result: NCCL timeout / hang
    if deepspeed_cfg and torch.distributed.is_initialized():
        # DeepSpeed path: ALL ranks must call save_pretrained()
        if is_rank_zero():
            LOGGER.info("DeepSpeed detected: saving with all-rank participation")

        # Pre-save barrier to ensure all ranks finish evaluation
        torch.distributed.barrier()

        # Unwrap model (all ranks need same model reference)
        unwrapped_model = trainer.model
        if hasattr(unwrapped_model, 'module'):
            unwrapped_model = unwrapped_model.module

        # ALL ranks call save_pretrained (PEFT internally handles rank0 I/O)
        unwrapped_model.save_pretrained(
            output_dir,
            safe_serialization=True,
            # PEFT checks this internally, but doesn't cause issues if all ranks call it
        )

        # Tokenizer save is lightweight, all ranks can call it safely
        tokenizer.save_pretrained(output_dir)

        # Post-save barrier to ensure save completed before cleanup
        torch.distributed.barrier()

        if is_rank_zero():
            LOGGER.info("PEFT adapter saved to %s for inference (all-rank save)", output_dir)

        # Metrics and logging: rank0 only
        if trainer.is_world_process_zero():
            metrics: Dict[str, Any] = {"global_step": trainer.state.global_step}
            if eval_metrics is not None:
                metrics.update(eval_metrics)
            save_metrics(metrics, metrics_cfg.get("path"))
            LOGGER.info("Training can be resumed from checkpoint-%d/", trainer.state.global_step)
            LOGGER.info("MLflow artifacts will be logged by Kedro node.")
    else:
        # Non-DeepSpeed path: rank0-only save is safe
        if trainer.is_world_process_zero():
            # Evaluate and save metrics
            metrics: Dict[str, Any] = {"global_step": trainer.state.global_step}
            if eval_metrics is not None:
                metrics.update(eval_metrics)
            save_metrics(metrics, metrics_cfg.get("path"))

            # Save PEFT adapter only (lightweight, no DeepSpeed sharding issues)
            unwrapped_model = trainer.model
            if hasattr(unwrapped_model, 'module'):
                unwrapped_model = unwrapped_model.module

            unwrapped_model.save_pretrained(
                output_dir,
                safe_serialization=True  # Use safetensors format
            )
            tokenizer.save_pretrained(output_dir)

            LOGGER.info("PEFT adapter saved to %s for inference", output_dir)
            LOGGER.info("Training can be resumed from checkpoint-%d/", trainer.state.global_step)
            LOGGER.info("MLflow artifacts will be logged by Kedro node.")

    # Clean up DeepSpeed resources
    if deepspeed_cfg and torch.distributed.is_initialized():
        try:
            torch.distributed.barrier()  # Sync all processes
            torch.distributed.destroy_process_group()
            if is_rank_zero():
                LOGGER.info("DeepSpeed process group destroyed successfully")
        except Exception as e:
            if is_rank_zero():
                LOGGER.warning("Failed to destroy process group: %s", e)

    # Force exit to avoid any remaining cleanup that might hang
    if is_rank_zero():
        LOGGER.info("Training script exiting normally")


if __name__ == "__main__":
    main()
