"""Standalone training script executed via DeepSpeed launcher.

Usage (invoked by Kedro node):

    deepspeed --num_gpus <N> src/train/main_yaml.py --config_yml path/to/config.yml

The configuration YAML is generated by the ``launch_training`` node and contains:

- model:        HuggingFace model parameters
- data:         tokenized dataset path and split names
- training_args:Arguments passed to ``TrainingArguments``
- lora:         LoRA enable flag and configuration
- deepspeed:    DeepSpeed config dict (optional)
- metrics:      Optional output path for evaluation metrics
- seed:         Random seed
"""

from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict

import torch
import torch.nn as nn
import yaml
import numpy as np
from datasets import load_from_disk
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
    set_seed,
)
from transformers.trainer_utils import get_last_checkpoint

try:
    from peft import LoraConfig, get_peft_model
    from peft import TaskType  # type: ignore
except ImportError:  # pragma: no cover - optional dependency guard
    LoraConfig = None
    get_peft_model = None
    TaskType = None


LOGGER = logging.getLogger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="DeepSpeed Trainer driver")
    parser.add_argument("--config_yml", type=str, required=True, help="Path to training YAML config")
    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--rank", type=int, default=0)
    parser.add_argument("--world_size", type=int, default=1)
    return parser.parse_args()


def load_config(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if not isinstance(cfg, dict):
        raise ValueError("Training configuration must be a YAML mapping")
    return cfg


def build_training_arguments(args_cfg: Dict[str, Any], deepspeed_cfg: Dict[str, Any] | None) -> TrainingArguments:
    cfg = dict(args_cfg)
    if deepspeed_cfg:
        cfg["deepspeed"] = deepspeed_cfg
        # Override TrainingArguments with DeepSpeed config values to avoid mismatch errors
        if "train_micro_batch_size_per_gpu" in deepspeed_cfg:
            cfg["per_device_train_batch_size"] = deepspeed_cfg["train_micro_batch_size_per_gpu"]
            LOGGER.info("DeepSpeed override: per_device_train_batch_size = %s", deepspeed_cfg["train_micro_batch_size_per_gpu"])
        if "gradient_accumulation_steps" in deepspeed_cfg:
            cfg["gradient_accumulation_steps"] = deepspeed_cfg["gradient_accumulation_steps"]
            LOGGER.info("DeepSpeed override: gradient_accumulation_steps = %s", deepspeed_cfg["gradient_accumulation_steps"])
    # Trainer expects lists for report_to, etc.
    if "report_to" in cfg and isinstance(cfg["report_to"], str):
        cfg["report_to"] = [cfg["report_to"]]
    output_dir = Path(cfg.get("output_dir", "./outputs"))
    output_dir.mkdir(parents=True, exist_ok=True)
    return TrainingArguments(**cfg)


def infer_num_labels(dataset) -> int:
    if "labels" in dataset.features:
        feature = dataset.features["labels"]
        # SequenceClassification: ClassLabel
        if hasattr(feature, "num_classes"):
            return feature.num_classes
    return len(set(dataset["labels"]))


def maybe_apply_lora(model, lora_section: Dict[str, Any]) -> torch.nn.Module:
    if not lora_section.get("enable", False):
        return model
    if LoraConfig is None or get_peft_model is None:
        raise ImportError("peft package is required for LoRA but is not installed")

    cfg = dict(lora_section.get("config", {}))
    task_type = cfg.pop("task_type", "SEQ_CLS")
    if TaskType is not None and isinstance(task_type, str):
        task_type_enum = TaskType[task_type]
        cfg["task_type"] = task_type_enum

    lora_cfg = LoraConfig(**cfg)
    LOGGER.info("Applying LoRA with config: %s", cfg)
    return get_peft_model(model, lora_cfg)


def save_metrics(metrics: Dict[str, Any], path: str | None) -> None:
    if not path:
        return
    destination = Path(path)
    destination.parent.mkdir(parents=True, exist_ok=True)
    with open(destination, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)


def compute_accuracy(preds, labels) -> Dict[str, float]:
    if preds.ndim > 1:
        preds = preds.argmax(axis=-1)
    accuracy = (preds == labels).mean().item() if hasattr(preds, "mean") else float((preds == labels).mean())
    return {"accuracy": float(accuracy)}


def main() -> None:
    logging.basicConfig(level=logging.INFO)
    args = parse_args()
    cfg = load_config(args.config_yml)

    seed = int(cfg.get("seed", 42))
    set_seed(seed)

    model_cfg = cfg.get("model", {})
    data_cfg = cfg.get("data", {})
    training_args_cfg = cfg.get("training_args", {})
    deepspeed_cfg = cfg.get("deepspeed")
    lora_cfg = cfg.get("lora", {})
    metrics_cfg = cfg.get("metrics", {})

    tokenized_path = data_cfg["tokenized_path"]
    LOGGER.info("Loading tokenized datasets from %s", tokenized_path)
    dataset_dict = load_from_disk(tokenized_path)

    train_split = data_cfg.get("train_split", "train")
    eval_split = data_cfg.get("eval_split")

    train_dataset = dataset_dict[train_split]
    eval_dataset = dataset_dict[eval_split] if eval_split and eval_split in dataset_dict else None

    model_name = model_cfg["name_or_path"]
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=model_cfg.get("trust_remote_code", False))
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    num_labels = model_cfg.get("num_labels")
    if num_labels is None:
        num_labels = infer_num_labels(train_dataset)
        LOGGER.info("Inferred num_labels=%s", num_labels)

    torch_dtype_str = model_cfg.get("torch_dtype", "bfloat16")
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32,
    }
    torch_dtype = dtype_map.get(torch_dtype_str, torch.bfloat16)

    checkpoint = get_last_checkpoint(training_args_cfg.get("output_dir", ""))
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels,
        torch_dtype=torch_dtype,
        trust_remote_code=model_cfg.get("trust_remote_code", False),
        resume_download=bool(checkpoint),
    )

    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    if model_cfg.get("gradient_checkpointing", False):
        model.gradient_checkpointing_enable()

    model = maybe_apply_lora(model, lora_cfg)

    training_args = build_training_arguments(training_args_cfg, deepspeed_cfg)

    collator = DataCollatorWithPadding(
        tokenizer=tokenizer,
        pad_to_multiple_of=8 if training_args.bf16 or training_args.fp16 else None,
    )

    def compute_metrics_fn(eval_pred):
        predictions, labels = eval_pred
        return compute_accuracy(predictions, labels)

    loss_cfg = cfg.get("loss", {})
    use_class_weights = bool(loss_cfg.get("use_class_weights", False))
    class_weight_alpha = float(loss_cfg.get("class_weight_alpha", 1.0))

    class_weights_tensor: torch.Tensor | None = None
    if use_class_weights:
        train_labels = np.array(train_dataset["labels"])
        class_counts = np.bincount(train_labels, minlength=num_labels)
        # For zero-sample classes, set small default value to avoid division by zero
        zero_mask = class_counts == 0
        num_zero_classes = zero_mask.sum()
        if num_zero_classes > 0:
            # Set default count as 1% of mean count for non-zero classes, minimum 1
            non_zero_counts = class_counts[~zero_mask]
            default_count = max(1.0, non_zero_counts.mean() * 0.01)
            class_counts = class_counts.astype(np.float32)
            class_counts[zero_mask] = default_count
            LOGGER.info("Zero-sample classes: %d/%d (default count: %.2f)", num_zero_classes, num_labels, default_count)

        class_weights = (len(train_labels) / (num_labels * class_counts)).astype(np.float32)
        if class_weight_alpha != 1.0:
            class_weights = class_weights ** class_weight_alpha
        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)
        LOGGER.info(
            "Class weights applied (alpha=%s): non-zero classes=%d, weight range=[%.4f, %.4f]",
            class_weight_alpha,
            num_labels - num_zero_classes,
            class_weights.min(),
            class_weights.max(),
        )
    else:
        LOGGER.info("Class weights disabled; using uniform loss.")

    class WeightedTrainer(Trainer):
        def __init__(self, *args, class_weights: torch.Tensor | None = None, **kwargs):
            super().__init__(*args, **kwargs)
            self.class_weights = class_weights

        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
            labels = inputs.get("labels")
            if labels is None:
                return super().compute_loss(model, inputs, return_outputs=return_outputs, num_items_in_batch=num_items_in_batch)

            # Remove labels to avoid default loss computation
            inputs = dict(inputs)
            labels = inputs.pop("labels")

            outputs = model(**inputs)
            logits = outputs.logits if hasattr(outputs, "logits") else outputs[0]

            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device) if self.class_weights is not None else None)
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1).to(logits.device))

            return (loss, outputs) if return_outputs else loss

    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        data_collator=collator,
        class_weights=class_weights_tensor,
        compute_metrics=compute_metrics_fn if eval_dataset is not None else None,
    )

    trainer.train()

    trainer.save_model()
    if trainer.state.is_world_process_zero():
        metrics: Dict[str, Any] = {"global_step": trainer.state.global_step}
        if eval_dataset is not None:
            metrics.update(trainer.evaluate())
        save_metrics(metrics, metrics_cfg.get("path"))


if __name__ == "__main__":
    main()
